{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "from nltk.tokenize import word_tokenize\r\n",
                "from nltk.corpus import stopwords\r\n",
                "from keras.preprocessing.text import Tokenizer\r\n",
                "from keras.preprocessing.sequence import pad_sequences\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "def text_preprocession(Corpus):\r\n",
                "    TEXT = Corpus.iloc[:,1]\r\n",
                "    # Step - a : Remove blank rows if any.\r\n",
                "    TEXT.dropna(inplace=True)\r\n",
                "    # Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\r\n",
                "    TEXT = [entry.lower() for entry in TEXT]\r\n",
                "    # Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\r\n",
                "    TEXT = [word_tokenize(entry) for entry in TEXT]\r\n",
                "    CorpusList = []\r\n",
                "    # for entry in enumerate(TEXT):\r\n",
                "    for entry in TEXT:\r\n",
                "        # Declaring Empty List to store the words that follow the rules for this step\r\n",
                "        Final_words = ''\r\n",
                "        for word in entry:\r\n",
                "            if word not in stopwords.words('english'):\r\n",
                "                Final_words = Final_words + word + ' '\r\n",
                "        CorpusList.append(Final_words)\r\n",
                "    return CorpusList"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "dataset = pd.read_csv('data/Bai_news_headlines.csv')\r\n",
                "news_list = text_preprocession(dataset)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "news_list[0]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'precious-gold regains footing equities , libya ; cenbanks eyed '"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "MAX_NUM_WORDS = 2000\r\n",
                "MAX_SEQUENCE_LENGTH = 50\r\n",
                "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\r\n",
                "tokenizer.fit_on_texts(news_list)\r\n",
                "sequences = tokenizer.texts_to_sequences(news_list)\r\n",
                "\r\n",
                "word_index = tokenizer.word_index\r\n",
                "print('Found %s unique tokens.' % len(word_index))\r\n",
                "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
                "\r\n",
                "new_data = []\r\n",
                "for l in data:\r\n",
                "    l = list(l)\r\n",
                "    while True:\r\n",
                "        l.remove(0)\r\n",
                "        if 0 not in l:\r\n",
                "            break\r\n",
                "    new_data.append(l)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Found 8856 unique tokens.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "with open('word_embedding_data.txt','w',encoding='utf-8') as f:\r\n",
                "    for i in range(len(news_list)):\r\n",
                "        f.write(news_list[i]+'\\n')\r\n",
                "        \r\n",
                "with open('word_embedding_doc_term_mat.txt','w',encoding='utf-8') as f:\r\n",
                "    for i in range(len(new_data)):\r\n",
                "        l = str(new_data[i]).strip('[')\r\n",
                "        r = l.strip(']')\r\n",
                "        f.write(r+'\\n')\r\n",
                "\r\n",
                "\r\n",
                "with open('word_embedding_vocab.txt','w',encoding='utf-8') as f:\r\n",
                "    for key in word_index:       \r\n",
                "        f.write(f\"{key} {word_index[key]}\\n\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "d1={\"1\":1,\"2\":2,\"3\":3}\r\n",
                "d2={\"2\":2,\"3\":1}\r\n",
                "{**d1,**d2}"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "{'1': 1, '2': 2, '3': 1}"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 17
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "embeddings_index = {}\r\n",
                "f = open('data/glove.6B.50d.txt',encoding = 'utf-8')\r\n",
                "for line in f:\r\n",
                "    values = line.split(' ')\r\n",
                "    word = values[0] ## The first entry is the word\r\n",
                "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\r\n",
                "    embeddings_index[word] = coefs\r\n",
                "f.close()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "embeddings_index[\"the\"]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
                            "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
                            "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
                            "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
                            "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
                            "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
                            "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
                            "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
                            "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
                            "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
                            "      dtype=float32)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 28
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "word_index[\"the\"]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4023"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 25
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "print('create vocab')\r\n",
                "vocab = {}\r\n",
                "fp = open(\"word_embedding_data.txt\", 'r',encoding='utf-8')\r\n",
                "for line in fp:\r\n",
                "    arr = re.split('\\s', line[:-1])\r\n",
                "    for wd in arr:\r\n",
                "        try:\r\n",
                "            vocab[wd] += 1\r\n",
                "        except:\r\n",
                "            vocab[wd] = 1\r\n",
                "fp.close()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "create vocab\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "vocab_arr = [[wd, vocab[wd]] for wd in vocab if vocab[wd] >3]\r\n",
                "vocab_arr = sorted(vocab_arr, key=lambda k: k[1])[::-1]\r\n",
                "vocab_arr = vocab_arr[:2000]\r\n",
                "vocab_arr = sorted(vocab_arr)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "fout = open(\"vocab.txt\", 'w')\r\n",
                "for itm in vocab_arr:\r\n",
                "    itm[1] = str(itm[1])\r\n",
                "    fout.write(' '.join(itm)+'\\n')\r\n",
                "fout.close()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "vocab_arr[0]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['', '28220']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 51
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "# vocabulary to id\r\n",
                "vocab2id = {itm[1][0]: itm[0] for itm in enumerate(vocab_arr)}\r\n",
                "print('create document term matrix')\r\n",
                "data_arr = []\r\n",
                "fp = open(\"word_embedding_data.txt\", 'r',encoding='utf-8')\r\n",
                "fout = open(\"wedoc_term_mat.txt\", 'w')\r\n",
                "for line in fp:\r\n",
                "    arr = re.split('\\s', line[:-1])\r\n",
                "    arr = [str(vocab2id[wd]) for wd in arr if wd in vocab2id]\r\n",
                "    sen = ' '.join(arr)\r\n",
                "    fout.write(sen+'\\n')\r\n",
                "fp.close()\r\n",
                "fout.close()\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "create document term matrix\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.11",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.11 64-bit ('ds-env': conda)"
        },
        "interpreter": {
            "hash": "6ea1ff613e4d4108378a98b641e44fcddf5add35145041c20507c4a55380beef"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}