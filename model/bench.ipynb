{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, MaxPool3D, Bidirectional, ConvLSTM2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import pymongo\n",
    "import random\n",
    "import string\n",
    "import fasttext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "HOME = os.environ['LIMA_HOME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> list:\n",
    "    \"\"\"\n",
    "    clean text with tokenization; stemming; removing stop word, punctuation, number, and empty string.\n",
    "\n",
    "    Args:\n",
    "        text (str): text\n",
    "\n",
    "    Returns:\n",
    "        list: cleaned text as list of tokenized str\n",
    "    \"\"\"\n",
    "\n",
    "    # to list of token\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    # stemming and convert to lower case if not proper noun: punctuation and stop word seem to help POS tagging, remove them after stemming\n",
    "    word_tag = pos_tag(text)\n",
    "    porter = PorterStemmer()\n",
    "    text = [\n",
    "        porter.stem(each[0])\n",
    "        if each[1] != \"NNP\" and each[1] != \"NNPS\" else each[0]\n",
    "        for each in word_tag\n",
    "    ]\n",
    "\n",
    "    # remove stop word: it seems stemming skip stop word; OK to remove stop word after stemming;\n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    text = [each for each in text if not each in stop_word]\n",
    "\n",
    "    # remove punctuation\n",
    "    text = [\n",
    "        each.translate(str.maketrans('', '', string.punctuation))\n",
    "        for each in text\n",
    "    ]\n",
    "    # text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", text) # if using re\n",
    "\n",
    "    # convert number to <NUM>\n",
    "    text = [\"<NUM>\" if each.isdigit() else each for each in text]\n",
    "\n",
    "    # remove empty string\n",
    "    text = [each for each in text if each != \"\"]\n",
    "\n",
    "    return text\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = data.copy()\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "        \n",
    "\t\tnames += [f'{data.columns[j]}(t-{i})' for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [f'{data.columns[j]}(t)' for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [f'{data.columns[j]}(t+{i})' for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def get_TS_cv(k=5, test_size=None):\n",
    "    \"\"\"\n",
    "    ML models do not need to care about forecast horizon when splitting training and test set. Forecast horizon should be handled by feature preparation ([X_t-1,X_t-2...]). Actually repeated K-fold can also be used, but stick to TS split to align with TS_evaluate().\n",
    "    \"\"\"\n",
    "    return TimeSeriesSplit(\n",
    "        n_splits=k,\n",
    "        gap=0,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "\n",
    "def evaluate_series(y_true, y_pred, horizon):\n",
    "    \"\"\"\n",
    "    Some models (like ARIMA) may not support cross_validate(), compare the forecasting result directly\n",
    "    Args:\n",
    "        y_true: y of test set\n",
    "        y_pred: y of prediction\n",
    "        horizon: forecast horizon\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: single row DF with 3 metrics wrt horizon\n",
    "    \"\"\"\n",
    "    # RMSE\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2=r2_score(y_true, y_pred)\n",
    "    forecast_error = {\n",
    "        'h': horizon,\n",
    "        'mae': [mae],\n",
    "        'rmse': [rmse],\n",
    "        'mape': [mape],\n",
    "        'r2':[r2],\n",
    "        'descriptions': \"\"\n",
    "    }\n",
    "    return forecast_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "h = 1\n",
    "past = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient\n",
    "client= InfluxDBClient.from_config_file(f\"{HOME}/dev/DB/influxdb_config.ini\")\n",
    "query_api = client.query_api()\n",
    "df_WTI = query_api.query_data_frame(\"\"\"\n",
    "from(bucket: \"dummy\")\n",
    "  |> range(start: 2011-04-01, stop: 2019-04-01)\n",
    "  |> filter(fn: (r) => r[\"_measurement\"] == \"WTI\") \n",
    "  |> filter(fn: (r) => r[\"type\"] == \"closing_price\") \n",
    "  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "  |> drop(columns: [\"_start\", \"_stop\"])\n",
    "\"\"\")\n",
    "df_WTI=df_WTI[[\"_time\",\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]]\n",
    "df_WTI.columns=[\"Date\",\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]\n",
    "df_WTI.set_index(\"Date\",inplace=True)\n",
    "df_WTI.index=df_WTI.index.map(lambda each: each.date())\n",
    "df_WTI.index=pd.to_datetime(df_WTI.index)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=[each.month for each in df_WTI.index]\n",
    "day=[each.day for each in df_WTI.index]\n",
    "day_in_week=[each.weekday() for each in df_WTI.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt=pd.DataFrame()\n",
    "df_dt[\"month\"]=month\n",
    "df_dt[\"day\"]=day\n",
    "df_dt[\"day_in_week\"]=day_in_week\n",
    "df_dt.index=df_WTI.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "ARMA_pred=[]\n",
    "for i in range(past,len(df_WTI)):\n",
    "    train_data=df_WTI.iloc[i-past:i][\"CLC1\"].to_numpy()\n",
    "    model = ARIMA(train_data,order=(1, 0,1)).fit()\n",
    "    forecasts = model.forecast(1)\n",
    "    ARMA_pred.append(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ARMA_pred)==len(df_WTI.index[past:])\n",
    "# ARMA_CLC1=pd.concat(ARMA_pred)\n",
    "# ARMA_CLC1=ARMA_CLC1[:-1]\n",
    "df_ARMA=pd.DataFrame(ARMA_pred)\n",
    "df_ARMA.columns=[\"ARMA_CLC1\"]\n",
    "df_ARMA.index=df_WTI.index[past:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mongo_db = mongo_client[\"lima\"]\n",
    "mongo_collection = mongo_db[\"investing_news\"]\n",
    "cursor = mongo_collection.find({\"News\":{\"$ne\":\"NEURONswap: First Dex To Implement Governance 2.0\"}})\n",
    "df_news_com =  pd.DataFrame(list(cursor))[[\"Date\",\"News\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_collection = mongo_db[\"investing_news_world\"]\n",
    "cursor = mongo_collection.find({\"News\":{\"$ne\":\"NEURONswap: First Dex To Implement Governance 2.0\"}})\n",
    "df_news_world=  pd.DataFrame(list(cursor))[[\"Date\",\"News\"]]\n",
    "\n",
    "# mongo_collection = mongo_db[\"investing_news_econ\"]\n",
    "# cursor = mongo_collection.find({\"News\":{\"$ne\":\"NEURONswap: First Dex To Implement Governance 2.0\"}})\n",
    "# df_news_econ=  pd.DataFrame(list(cursor))[[\"Date\",\"News\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news=pd.concat([df_news_com,df_news_world])\n",
    "df_news=df_news.sort_values(by='Date',ignore_index=True)\n",
    "df_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.load_model(f\"{HOME}/data/big/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news=df_news[df_news.Date.isin(df_WTI.index)]\n",
    "df_news.News = df_news.News.apply(lambda r: \" \".join(clean(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_aggregated(df):\n",
    "    df=df.copy()\n",
    "    df[\"Polarity\"] = df.apply(\n",
    "        lambda row: TextBlob(row['News']).sentiment.polarity, axis=1)\n",
    "    df[\"Subjectivity\"] = df.apply(\n",
    "        lambda row: TextBlob(row['News']).sentiment.subjectivity, axis=1)\n",
    "    df_daily_averaged_sentiment_score = df.groupby(['Date']).mean()\n",
    "    return df_daily_averaged_sentiment_score\n",
    "\n",
    "df_sentiment = get_sentiment_aggregated(df_news)\n",
    "df_sentiment[\"Combined_Sentiment\"]=df_sentiment.Polarity*(1+df_sentiment.Subjectivity)\n",
    "def decay_features(df):\n",
    "    window_size=5\n",
    "    feature_list=[]\n",
    "    for each in df.columns:\n",
    "        feature_list.append(df[each].iloc[:window_size].to_list())\n",
    "    feature_num=len(feature_list)\n",
    "    for idx in range(window_size,len(df_sentiment)):\n",
    "        feature_tmp=np.zeros(feature_num)\n",
    "        for t in range(window_size):\n",
    "            for feature_idx in range(feature_num):\n",
    "                feature_tmp[feature_idx]+=df.iloc[idx-t][df.columns[feature_idx]]*((window_size-t)/window_size)\n",
    "        for feature_idx in range(feature_num):\n",
    "            feature_list[feature_idx].append(feature_tmp[feature_idx])\n",
    "    df_result=pd.DataFrame(feature_list).transpose()\n",
    "    df_result.index=df.index\n",
    "    df_result.columns=[f\"Decay_{each}\" for each in df.columns]\n",
    "    return df_result\n",
    "\n",
    "df_res=decay_features(df_sentiment)\n",
    "df_sentiment=pd.concat([df_sentiment,df_res],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sentiment.to_pickle(\"df_sentiment_2type.pkl\")\n",
    "df_sentiment=pd.read_pickle(\"df_sentiment_2type.pkl\")\n",
    "# df_sentiment=pd.read_pickle(\"df_sentiment.pkl\")\n",
    "df_sentiment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic modeling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_aggregated(df):\n",
    "    df=df.copy()\n",
    "    news_emb = df.News.apply(lambda x: fasttext_model.get_sentence_vector(\n",
    "        (x))).to_numpy().tolist()\n",
    "    news_emb = np.array(news_emb)\n",
    "    emb_scaler=MinMaxScaler()\n",
    "    news_emb=emb_scaler.fit_transform(news_emb)\n",
    "    lda_model=LatentDirichletAllocation(n_components=5,n_jobs=-1)\n",
    "    topic= lda_model.fit_transform(news_emb)\n",
    "    for i in range(5):\n",
    "        df[f\"Topic{i+1}\"] = topic[:, i]\n",
    "    df_daily_averaged_topic = df.groupby(['Date']).mean()\n",
    "    return df_daily_averaged_topic, emb_scaler, lda_model\n",
    "\n",
    "df_topic, emb_scaler, lda_model = get_topic_aggregated(df_news)\n",
    "df_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res=decay_features(df_topic)\n",
    "df_topic=pd.concat([df_topic,df_res],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topic.to_pickle(\"df_topic_2type.pkl\")\n",
    "df_topic=pd.read_pickle(\"df_topic_2type.pkl\")\n",
    "# df_topic=pd.read_pickle(\"df_topic.pkl\")\n",
    "df_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "geo_pattern={\n",
    "\"Geopolitical_Threats\":\"Geopolitical risk concern tension uncertainty United States tensions military war geopolitical coup guerrilla warfare Latin America Central America South America Europe Africa Middle East Far East Asia\",\n",
    "\"Nuclear_Threats\":\"nuclear war atomic war nuclear conflict atomic conflict nuclear missile fear threat risk peril menace\",\n",
    "\"War_Threats\":\"war risk risk of war fear of war war fear military threat war threat threat of war military action military operation military fce risk threat\",\n",
    "\"Terrorist_Threats\":\"terrorist threat threat of terrorism terrorism menace menace of terrorism terrorist risk terr risk risk of terrorism terr threat\",\n",
    "\"War_Acts\":\"beginning of the war outbreak of the war onset of the war escalation of the war start of the war war military air strike war battle heavy casualties\",\n",
    "\"Terrorist_Acts\":\"terrorist act terrorist acts\"}\n",
    "\n",
    "def get_geoidx_aggregated(df):\n",
    "    df=df.copy()\n",
    "    df[\"news_emb\"] = df.News.apply(lambda x: fasttext_model.get_sentence_vector(\n",
    "        (x))).to_numpy().tolist()\n",
    "    for each in geo_pattern:\n",
    "        pattern_emb=fasttext_model.get_sentence_vector(each)\n",
    "        df[each]=df.news_emb.apply(lambda x:1-spatial.distance.cosine(pattern_emb, x))\n",
    "    df=df.drop([\"News\",\"news_emb\"],axis=1)\n",
    "    df_daily_averaged_geoidx = df.groupby(['Date']).max()\n",
    "    return df_daily_averaged_geoidx\n",
    "\n",
    "df_geoidx = get_geoidx_aggregated(df_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res=decay_features(df_geoidx)\n",
    "df_geoidx=pd.concat([df_geoidx,df_res],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_geoidx.to_pickle(\"df_geoidx_2type.pkl\")\n",
    "df_geoidx=pd.read_pickle(\"df_geoidx_2type.pkl\")\n",
    "df_geoidx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy = pd.concat([df_sentiment,df_topic, df_geoidx,df_WTI[\"CLC1\"]], axis=1, join=\"inner\")\n",
    "print(df_Xy.shape)\n",
    "df_Xy.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_emb_maxmin(emb):\n",
    "    data=np.array(emb.to_numpy().tolist())\n",
    "    return np.concatenate([data.max(axis=0),data.min(axis=0)])\n",
    "def aggregate_emb_maxmeanmin(emb):\n",
    "    data=np.array(emb.to_numpy().tolist())\n",
    "    return np.concatenate([data.max(axis=0),data.mean(axis=0),data.min(axis=0)])\n",
    "\n",
    "news_emb = df_news.News.apply(lambda x: fasttext_model.get_sentence_vector(\n",
    "        (x)))\n",
    "# news_emb = np.array(news_emb)\n",
    "# emb_scaler=MinMaxScaler()\n",
    "# news_emb=emb_scaler.fit_transform(news_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb=pd.concat([df_news.Date,news_emb],axis=1)\n",
    "df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily mean of mean\n",
    "# df_emb[\"News\"]=df_emb.News.apply(lambda r:np.array(r.tolist()).mean())\n",
    "\n",
    "# daily mean\n",
    "# df_emb=df_emb.groupby(\"Date\").mean()\n",
    "\n",
    "# max + min\n",
    "# df_emb=df_emb.groupby(\"Date\")['News'].agg(aggregate_emb_maxmin)\n",
    "\n",
    "# max + mean + min\n",
    "df_emb=df_emb.groupby(\"Date\")['News'].agg(aggregate_emb_maxmeanmin)\n",
    "\n",
    "df_emb=pd.DataFrame(df_emb)\n",
    "# df_emb.set_index(\"Date\",inplace=True)\n",
    "df_emb.index=pd.to_datetime(df_emb.index)\n",
    "df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to align\n",
    "df_emb=df_emb[df_emb.index.isin(df_Xy[\"CLC1\"].index)]\n",
    "# emb=df_emb.to_numpy()\n",
    "# emb =np.array([each.tolist() for each in emb])\n",
    "# emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=df_WTI[df_WTI.index.isin(df_emb.index)][\"CLC1\"].to_numpy()\n",
    "# y.shape\n",
    "# feature_name=\"EMB_maxmeanmin_single\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CORENLP_HOME\"]=\"~/.stanfordnlp_resources/stanford-corenlp-4.1.0/\"\n",
    "# from openie import StanfordOpenIE\n",
    "# openie_client= StanfordOpenIE()\n",
    "# openie_client.client.ensure_alive()\n",
    "# # df_news.to_pickle(\"df_news_3type.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_tuples = []\n",
    "# for idx, row in df_news.iterrows():\n",
    "#     text = row['News']\n",
    "#     for triple in openie_client.annotate(text):\n",
    "#         triple['Date'] = row['Date']\n",
    "#         event_tuples.append(triple)\n",
    "# openie_client.client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_events = pd.DataFrame(event_tuples)\n",
    "# df_events.to_pickle(\"df_events_2type.pkl\")\n",
    "# df_events=pd.read_pickle(\"df_events.pkl\")\n",
    "df_events=pd.read_pickle(\"df_events_2type.pkl\")\n",
    "# df_events=pd.read_pickle(\"df_events_3type.pkl\")\n",
    "df_events.subject = df_events.subject.apply(\n",
    "    lambda x: fasttext_model.get_sentence_vector((x)))\n",
    "df_events.relation = df_events.relation.apply(\n",
    "    lambda x: fasttext_model.get_sentence_vector((x)))\n",
    "df_events.object = df_events.object.apply(lambda x: fasttext_model.get_sentence_vector(\n",
    "    (x)))\n",
    "df_events.dropna(inplace=True)\n",
    "df_events.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_emb_maxmeanmin(emb):\n",
    "    data=np.array(emb.to_numpy().tolist())\n",
    "    return np.concatenate([data.max(axis=0),data.mean(axis=0),data.min(axis=0)])\n",
    "\n",
    "# max + mean + min\n",
    "series_subject=df_events.groupby(\"Date\")['subject'].agg(aggregate_emb_maxmeanmin)\n",
    "series_relation=df_events.groupby(\"Date\")['relation'].agg(aggregate_emb_maxmeanmin)\n",
    "series_object=df_events.groupby(\"Date\")['object'].agg(aggregate_emb_maxmeanmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_emb=pd.concat([series_subject,series_relation,series_object],axis=1)\n",
    "df_event_emb.index=pd.to_datetime(df_event_emb.index)\n",
    "df_event_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to align\n",
    "# df_event_emb=df_event_emb[df_event_emb.index.isin(df_WTI[\"CLC1\"].index)]\n",
    "# join 3 : (-1,2700)\n",
    "# df_event_emb[\"event\"]=df_event_emb.apply(np.concatenate,axis=1)\n",
    "# join 3 : (-1,3,900)\n",
    "df_event_emb[\"event\"]=df_event_emb.apply(np.array,axis=1)\n",
    "# event_emb=df_event_emb[\"event\"].to_numpy()\n",
    "# event_emb=np.array([each.tolist() for each in event_emb])\n",
    "# X=event_emb\n",
    "# event_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationary test before diff\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "for i in range(df_Xy.shape[1]):\n",
    "    test_result=adfuller(df_Xy[df_Xy.columns[i]].to_numpy())\n",
    "    if test_result[1]>0.05:\n",
    "        print(f\"{df_Xy.columns[i]}: {test_result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller(df_Xy['CLC1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy.columns[:]\n",
    "# df_Xy.columns[7:]#\n",
    "df_Xy.columns[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aic': 25, 'bic': 5, 'hqic': 5, 'fpe': 25}\n",
      "{'aic': 10, 'bic': 5, 'hqic': 5, 'fpe': 10}\n",
      "{'aic': 5, 'bic': 1, 'hqic': 1, 'fpe': 5}\n",
      "{'aic': 23, 'bic': 2, 'hqic': 2, 'fpe': 23}\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "# endo=df_Xy[df_Xy.columns[-4:]].to_numpy()\n",
    "# exo=df_Xy[df_Xy.columns[:-4]].to_numpy()\n",
    "endo=df_Xy[[\"Polarity\",\"Subjectivity\",'Combined_Sentiment',\"CLC1\"]].to_numpy()\n",
    "exo=None\n",
    "model = VAR(endog=endo,exog=exo)\n",
    "var_result=model.select_order(50)\n",
    "print(var_result.selected_orders)\n",
    "\n",
    "endo=df_Xy[[\"Topic1\",\"Topic2\",\"Topic3\",\"Topic4\",\"CLC1\"]].to_numpy()\n",
    "model = VAR(endog=endo,exog=exo)\n",
    "var_result=model.select_order(50)\n",
    "print(var_result.selected_orders)\n",
    "\n",
    "endo=df_Xy[[\"Geopolitical_Threats\",\"Nuclear_Threats\",\"War_Threats\",\"Terrorist_Threats\",'War_Acts','Terrorist_Acts ',\"CLC1\"]].to_numpy()\n",
    "model = VAR(endog=endo,exog=exo)\n",
    "var_result=model.select_order(50)\n",
    "print(var_result.selected_orders)\n",
    "\n",
    "endo=df_WTI[[\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]].to_numpy()\n",
    "model = VAR(endog=endo,exog=exo)\n",
    "var_result=model.select_order(50)\n",
    "print(var_result.selected_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endo=df_Xy[['Polarity', 'Subjectivity', 'Combined_Sentiment',\n",
    "         'Topic1', 'Topic2',\n",
    "       'Topic3', 'Topic4',\n",
    "         'Geopolitical_Threats',\n",
    "       'Nuclear_Threats', 'War_Threats', 'Terrorist_Threats', 'War_Acts',\n",
    "       'Terrorist_Acts ',\"CLC1\"]].to_numpy()\n",
    "model = VAR(endog=endo,exog=exo)\n",
    "var_result=model.select_order(50)\n",
    "print(var_result.selected_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endo=df_Xy[df_Xy.columns[-1:]].to_numpy()\n",
    "# exo=df_Xy[df_Xy.columns[:-1]].to_numpy()\n",
    "# model = VAR(endog=endo,exog=exo)\n",
    "# var_result=model.select_order(30)\n",
    "# var_result.selected_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in df_Xy.columns[:-1]:\n",
    "    try:\n",
    "        endo=df_Xy[[\"CLC1\",each]].to_numpy()\n",
    "        model = VAR(endo)\n",
    "        var_result=model.select_order(120)\n",
    "        print(f\"{each}: {var_result.selected_orders}\")\n",
    "    except:\n",
    "        print(f\"ERROR: {each}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(df_Xy.CLC3,lags=9, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "all_results=[]\n",
    "for each in df_Xy.columns:\n",
    "    # if each ==\"CLC1\":\n",
    "    #     continue\n",
    "    rest_results=grangercausalitytests(df_Xy[[\"CLC1\",each]],maxlag=30,verbose=False)\n",
    "    for lag in range(1,30):\n",
    "        all_results.append({\"type\":each,\"lag\":lag, \"p\":rest_results[lag][0]['ssr_ftest'][1]})\n",
    "df_test=pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"p\")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_name=\"day\"\n",
    "# df_selected=df_Xy #[[feature_name,'CLC1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grangercausalitytests(df_Xy[[\"CLC1\",\"CLC1\"]],maxlag=20,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection with Granger\n",
    "# df_g_test=df.sort_values(\"p\")\n",
    "# selected_features_series=[]\n",
    "# for idx,each in df_g_test.iterrows():\n",
    "#     series=df_Xy[each[\"type\"]].shift(each[\"lag\"])\n",
    "#     series.name=f\"{each['type']}(t-{each['lag']})\"\n",
    "#     selected_features_series.append(series)\n",
    "# df_selected=pd.concat(selected_features_series,axis=1).dropna()\n",
    "# df_selected.columns[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_selected[df_selected.columns[:1]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 311)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# individual lag\n",
    "# df_shifted_sentiment=series_to_supervised(df_sentiment,25,0,False)\n",
    "# df_shifted_topic=series_to_supervised(df_topic,10,0,False)\n",
    "# df_shifted_geoidx=series_to_supervised(df_geoidx,5,0,False)\n",
    "# df_shifted_features=pd.concat([df_shifted_sentiment,df_shifted_topic,df_shifted_geoidx,df_WTI[\"CLC1\"]],axis=1).dropna()\n",
    "# df_shifted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve original price for inverting prediction\n",
    "df_original_price = df_Xy[[\"CLC1\"]].shift(h).dropna()\n",
    "# 1st order DIFF\n",
    "df_selected = df_Xy.diff().dropna()\n",
    "df_nodiff = df_Xy.dropna()\n",
    "# shift back $past days\n",
    "# past=5\n",
    "df_selected = series_to_supervised(df_selected, past, h)\n",
    "df_nodiff = series_to_supervised(df_nodiff, past, h)\n",
    "# df_selected = series_to_supervised(df_emb, past, h)\n",
    "\n",
    "df_original_price = df_original_price[df_original_price.index.isin(df_selected.index)]\n",
    "# remove current day features for forecast\n",
    "for each in df_selected.columns[:-1]:\n",
    "    if \"(t)\" in each:\n",
    "        df_selected.drop(each, axis=1, inplace=True)\n",
    "        df_nodiff.drop(each, axis=1, inplace=True)\n",
    "        \n",
    "\n",
    "# add time feature without shift \n",
    "df_selected=pd.concat([df_dt,df_selected],axis=1).dropna()\n",
    "df_nodiff=pd.concat([df_dt,df_nodiff],axis=1).dropna()\n",
    "df_nodiff=df_nodiff[df_nodiff.index.isin(df_selected.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stationary test after diff\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# for i in range(df_selected.shape[1]):\n",
    "#     test_result=adfuller(df_selected[df_selected.columns[i]].to_numpy())\n",
    "#     if test_result[1]>0.05:\n",
    "#         print(f\"{df_selected.columns[i]}: {test_result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EMB\n",
    "# df_selected_emb = series_to_supervised(df_emb, past, h)\n",
    "# for each in df_selected_emb.columns:\n",
    "#     if \"(t)\" in each:\n",
    "#         df_selected_emb.drop(each, axis=1, inplace=True)\n",
    "# df_selected_emb = df_selected_emb[df_selected_emb.index.isin(df_selected.index)]\n",
    "# df_selected_emb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event-EMB\n",
    "df_selected_emb = series_to_supervised(df_event_emb[[\"event\"]], past, h)\n",
    "for each in df_selected_emb.columns:\n",
    "    if \"(t)\" in each:\n",
    "        df_selected_emb.drop(each, axis=1, inplace=True)\n",
    "df_selected_emb = df_selected_emb[df_selected_emb.index.isin(df_selected.index)]\n",
    "df_selected_emb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected=df_selected[df_selected.index.isin(df_selected_emb.index)]\n",
    "df_nodiff=df_nodiff[df_nodiff.index.isin(df_selected_emb.index)]\n",
    "df_original_price=df_original_price[df_original_price.index.isin(df_selected_emb.index)]\n",
    "print(f\"{df_selected.shape} | {df_original_price.shape}\")\n",
    "print(f\"{df_selected.shape} | {df_nodiff.shape}\")\n",
    "print(f\"{df_selected.shape} | {df_selected_emb.shape}\")\n",
    "(df_selected_emb.index==df_selected.index).all()\n",
    "# df_selected.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb =np.array([each.tolist() for each in df_emb.to_numpy()])\n",
    "emb= df_selected_emb.to_numpy()\n",
    "emb =np.array([each.tolist() for each in emb])\n",
    "emb =np.array([each.tolist() for each in emb])\n",
    "print(f\"{emb.shape} | {emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = df_selected.to_numpy()[:, :-1]\n",
    "no_diff_raw_X = df_nodiff.to_numpy()[:, :-1]\n",
    "y =  df_selected.to_numpy()[:, -1].reshape(-1, 1) \n",
    "no_diff_y =  df_nodiff.to_numpy()[:, -1].reshape(-1, 1) \n",
    "# y = df_Xy[df_Xy.index.isin(df_selected.index)].to_numpy()[:, -1].reshape(-1, 1)\n",
    "# y=df_WTI[df_WTI.index.isin(df_selected.index)][\"CLC1\"].to_numpy().reshape(-1, 1)\n",
    "f\"{raw_X.shape} |{no_diff_raw_X.shape} |{y.shape} | {no_diff_y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding\n",
    "# raw_X=np.array([each.tolist() for each in raw_X])\n",
    "# raw_X=np.array([each.tolist() for each in raw_X])\n",
    "# raw_X.shape\n",
    "# X=raw_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump for ARIMA\n",
    "# df_dump=df_Xy[df_Xy.index.isin(df_selected.index)]\n",
    "# df_dump.to_pickle(\"df_dump.pkl\")\n",
    "# df_Xy=pd.read_pickle(\"df_dump.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression,RFE,RFECV,SelectFromModel,SequentialFeatureSelector,chi2,SelectKBest,f_regression,VarianceThreshold,r_regression\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor\n",
    "from sklearn.svm import LinearSVR,SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{raw_X.shape} | {y.shape}\")\n",
    "print(f\"{no_diff_raw_X.shape} | {no_diff_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tsfel.feature_extraction.features import entropy,abs_energy,autocorr,neighbourhood_peaks\n",
    "# # tsfel.feature_extraction.features.abs_energy()\n",
    "# def tswrapper(a,b):\n",
    "#     # a=MinMaxScaler().fit_transform(raw_X)\n",
    "#     result=pd.DataFrame(a).apply(autocorr).to_numpy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector based\n",
    "estimator = Lasso(random_state=42)\n",
    "# estimator = DecisionTreeRegressor(random_state=42)\n",
    "# estimator = Ridge(random_state=42)\n",
    "# estimator = LinearRegression()\n",
    "# estimator = LinearSVR(tol=0.001,random_state=42)\n",
    "selector = RFE(estimator,n_features_to_select=10,step=1)\n",
    "# selector = RFECV(estimator, min_features_to_select=20, cv=get_TS_cv(),step=1,n_jobs=-1)\n",
    "# selector=SelectFromModel(estimator,max_features=20)\n",
    "# selector=SequentialFeatureSelector(estimator,n_features_to_select=10,direction='forward',n_jobs=-1,cv=get_TS_cv())\n",
    "# selector=SelectKBest(r_regression,k=20)\n",
    "# selector=SelectKBest(tswrapper,k=20)\n",
    "# selector=VarianceThreshold(3.21)\n",
    "scaled_raw_X=MinMaxScaler().fit_transform(no_diff_raw_X)\n",
    "# scaled_y=MinMaxScaler().fit_transform(y)\n",
    "# selector = selector.fit(scaled_raw_X,scaled_y.ravel())\n",
    "selector = selector.fit(scaled_raw_X,no_diff_y.ravel())\n",
    "# selector = selector.fit(no_diff_raw_X,no_diff_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_X[:, selector.get_support()]\n",
    "no_diff_X = no_diff_raw_X[:, selector.get_support()]\n",
    "# X = raw_X\n",
    "feature_name=f\"_{X.shape[-1]}\"\n",
    "print(f\"{X.shape} | {y.shape}\")\n",
    "print(f\"{no_diff_X.shape} | {no_diff_y.shape}\")\n",
    "df_selected.columns[:-1][selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA,FastICA,FactorAnalysis,NMF\n",
    "# scaled_raw_X=MinMaxScaler((1,100)).fit_transform(raw_X)\n",
    "# # pca = PCA(n_components=150,svd_solver='full')\n",
    "# pca = NMF(n_components=150,max_iter=1000)\n",
    "# # decomposer = FactorAnalysis(n_components=7)\n",
    "# X=pca.fit_transform(scaled_raw_X)\n",
    "# feature_name=f\"NMF_{X.shape[-1]}\"\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=raw_X\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=df_Xy['CLC1'].to_numpy().ravel()\n",
    "# # X=df_Xy[['CLC2','CLC3','CLC4']].to_numpy()\n",
    "# X=df_geoidx.to_numpy()\n",
    "# # X=df_Xy[['CLC2','CLC3','CLC4']].to_numpy()\n",
    "# f\"{X.shape} | {y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X=np.random.random((2014, 12))\n",
    "# X=np.zeros((2014, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, ARDRegression, SGDRegressor, ElasticNet, Lars, Lasso, GammaRegressor, TweedieRegressor, PoissonRegressor, Ridge, BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor\n",
    "\n",
    "lin_model=Ridge(random_state =42)    #LinearRegression,Ridge,LinearSVR\n",
    "# lin_model=LinearRegression()  \n",
    "model_name=lin_model.__class__.__name__\n",
    "\n",
    "# msg=f\"{model_name}_{feature_name}\"\n",
    "msg=f\"{model_name}\"\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "# scaled_X=X_scaler.fit_transform(X)\n",
    "scaled_X=X_scaler.fit_transform(no_diff_X)\n",
    "# scaled_X=X\n",
    "\n",
    "cv_results = cross_validate(lin_model,\n",
    "                                scaled_X,\n",
    "                                no_diff_y,\n",
    "                                scoring=[\n",
    "                                    'neg_mean_absolute_error',\n",
    "                                    'neg_root_mean_squared_error',\n",
    "                                    'neg_mean_absolute_percentage_error'\n",
    "                                ],\n",
    "                                cv=get_TS_cv(),\n",
    "                                n_jobs=-1)\n",
    "mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "mape = -cv_results[\"test_neg_mean_absolute_percentage_error\"]\n",
    "k = 5\n",
    "print(f\"\"\"\n",
    "Forecast Error ({k}-fold cross-validated performance):\n",
    "{lin_model.__class__.__name__}:\n",
    "MAE = {mae.mean():.3f} +/- {mae.std():.3f}\n",
    "RMSE = {rmse.mean():.3f} +/- {rmse.std():.3f}\n",
    "MAPE = {mape.mean():.3f} +/- {mape.std():.3f}\n",
    "\"\"\")\n",
    "print(f\"{msg},{mae.mean():.6f},{rmse.mean():.6f},{mape.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape,MaxPooling2D,Bidirectional,ConvLSTM2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN \n",
    "from keras.layers import Conv2D,Conv3D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.keras.backend.clear_session()\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_y=df_ARMA[df_ARMA.index.isin(df_selected.index)].to_numpy()\n",
    "t_y=no_diff_y-t_y\n",
    "# y=df_WTI[df_WTI.index.isin(df_ARMA.index)][\"CLC1\"].to_numpy()\n",
    "# X=X.reshape(-1,10,900)\n",
    "# X=X.reshape(-1,10,3,900)\n",
    "# emb=event_emb\n",
    "f\"{X.shape} | {y.shape} |{t_y.shape}| {emb.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=X.shape[0]\n",
    "# train_size=int(length*2/3)\n",
    "train_size=int(length*0.7)\n",
    "# val_size=int(train_size*0.1)\n",
    "val_size=5\n",
    "step_size=1\n",
    "\n",
    "# train_X=no_diff_X[:train_size]\n",
    "train_X=X[:train_size]\n",
    "train_emb=emb[:train_size,:,:]\n",
    "train_t_y=t_y[:train_size,:]\n",
    "train_y=y[:train_size,:]\n",
    "\n",
    "# test_X=no_diff_X[train_size:]\n",
    "test_X=X[train_size:]\n",
    "test_emb=emb[train_size:,:,:]\n",
    "test_t_y=t_y[train_size:,:]\n",
    "test_y=y[train_size:,:]\n",
    "\n",
    "val_X=train_X[-val_size:]\n",
    "val_emb=train_emb[-val_size:,:,:]\n",
    "val_t_y=train_y[-val_size:,:]\n",
    "val_y=train_y[-val_size:,:]\n",
    "\n",
    "train_X=train_X[:-val_size]\n",
    "train_emb=train_emb[:-val_size,:,:]\n",
    "train_t_y=train_y[:-val_size,:]\n",
    "train_y=train_y[:-val_size,:]\n",
    "\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X_scaler.fit(train_X)\n",
    "t_y_scaler = MinMaxScaler()\n",
    "t_y_scaler.fit(train_t_y)\n",
    "y_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "# y_scaler = MinMaxScaler()\n",
    "y_scaler.fit(train_y)\n",
    "\n",
    "train_X=X_scaler.transform(train_X)\n",
    "test_X=X_scaler.transform(test_X)\n",
    "val_X=X_scaler.transform(val_X)\n",
    "train_y=y_scaler.transform(train_y)\n",
    "test_y=y_scaler.transform(test_y)\n",
    "val_y=y_scaler.transform(val_y)\n",
    "train_t_y=t_y_scaler.transform(train_t_y)\n",
    "test_t_y=t_y_scaler.transform(test_t_y)\n",
    "val_t_y=t_y_scaler.transform(val_t_y)\n",
    "\n",
    "train_X=train_X.reshape(train_X.shape[0],step_size,train_X.shape[-1])\n",
    "test_X=test_X.reshape(test_X.shape[0],step_size,test_X.shape[-1])\n",
    "val_X=val_X.reshape(val_X.shape[0],step_size,val_X.shape[-1])\n",
    "print(f\"train_X: {train_X.shape}\\t val_X: {val_X.shape}\\t test_X:{test_X.shape}\")\n",
    "print(f\"train_emb: {train_emb.shape}\\t val_emb: {val_emb.shape}\\t test_emb:{test_emb.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\\t val_y: {val_y.shape} test_y:{test_y.shape}\")\n",
    "print(f\"train_t_y: {train_t_y.shape}\\t val_t_y: {val_t_y.shape} test_t_y:{test_t_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbeats_keras.model import NBeatsNet\n",
    "# model = NBeatsNet(\n",
    "#     backcast_length=1, forecast_length=1,\n",
    "#     stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n",
    "#     nb_blocks_per_stack=2, thetas_dim=(4, 4), share_weights_in_stack=True,\n",
    "#     hidden_layer_units=128\n",
    "# )\n",
    "# model.compile(loss='mse', optimizer='adam',run_eagerly=True)\n",
    "# past=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size=train_emb.shape[-1]\n",
    "emb_step=3\n",
    "# emb_inputs = Input(shape=(past,emb_size))     # emb\n",
    "emb_inputs = Input(shape=(past,emb_step,emb_size))       # event\n",
    "emb_model = Reshape((1,past,emb_step,3,300))(emb_inputs)\n",
    "emb_model = Conv3D(10,(2,1,2), activation=\"relu\",padding='valid')(emb_model)\n",
    "emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = Conv3D(50,(3,1,2), activation=\"relu\",padding='valid')(emb_model)\n",
    "# emb_model = Dropout(0.3)(emb_model)\n",
    "# emb_model = Reshape((2,2,-1))(emb_model)\n",
    "# emb_model = Conv2D(10,(2,2), activation=\"relu\",padding='valid')(emb_model)\n",
    "# emb_model = Dropout(0.3)(emb_model)\n",
    "emb_model = Flatten()(emb_model)\n",
    "emb_model = Dense(10)(emb_model)\n",
    "emb_model = Model(inputs=emb_inputs, outputs=emb_model)\n",
    "# opt=Adam(0.0007)\n",
    "# model.compile(loss='mae', optimizer=opt)\n",
    "# emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "ts_inputs = Input(shape=(step_size,train_X.shape[-1]))\n",
    "ts_model=Reshape((step_size,train_X.shape[-1]))(ts_inputs)\n",
    "# ts_model=Bidirectional(GRU(300,dropout=0.33,return_sequences=True))(ts_model)\n",
    "# ts_model= Dropout(0.3)(ts_model)\n",
    "ts_model=Bidirectional(GRU(10,dropout=0 ,return_sequences=False))(ts_model)\n",
    "ts_model= Dropout(0.33)(ts_model)\n",
    "ts_model =Dense(10)(ts_model)\n",
    "# ts_model =Dense(1)(ts_model)\n",
    "ts_model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "# ts_model.compile(loss='mae', optimizer=Adam(0.0005))\n",
    "# ts_model.compile(loss='mae', optimizer=Adam(0.0005))\n",
    "# ts_model.compile(loss='log_cosh', optimizer=Adam(0.0002))\n",
    "# ts_model = Bidirectional(GRU(500,dropout=0.1,return_sequences=False))(ts_inputs)\n",
    "# ts_model= Dropout(0.2)(ts_model)\n",
    "# model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "# opt = Adam(learning_rate=0.001)\n",
    "# ts_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arma_inputs = Input(shape=(train_t_y.shape[-1]))\n",
    "arma_model=Reshape((step_size,train_t_y.shape[-1]))(arma_inputs)\n",
    "# arma_model=Bidirectional(GRU(50,dropout=0.33,return_sequences=True))(arma_model)\n",
    "# arma_model =Dropout(0.3)(arma_model)\n",
    "arma_model=Bidirectional(GRU(10,dropout=0,return_sequences=False))(arma_model)\n",
    "arma_model =Dropout(0.33)(arma_model)\n",
    "arma_model =Dense(20)(arma_model)\n",
    "arma_model = Model(inputs=arma_inputs, outputs=arma_model)\n",
    "# arma_model.compile(loss='mae', optimizer=Adam(0.0007))\n",
    "# arma_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = concatenate([arma_model.output,ts_model.output,emb_model.output])\n",
    "# combined_model= Reshape((1,-1),name='alpha')(combined_model)\n",
    "# combined_model= Bidirectional(GRU(100,dropout=0.33,return_sequences=False))(combined_model)\n",
    "# combined_model= Bidirectional(GRU(100,dropout=0.33,return_sequences=False))(combined_model)\n",
    "# combined_model = Dense(100)(combined_model)\n",
    "# combined_model = Dropout(0.3)(combined_model)\n",
    "combined_model = Dense(1)(combined_model)\n",
    "model = Model(inputs=[arma_model.input,ts_model.input,emb_model.input], outputs=combined_model)\n",
    "model.compile(loss='log_cosh', optimizer=Adam(0.0002))\n",
    "# model.compile(loss='msle', optimizer=Adam(0.0002))\n",
    "# model.compile(loss='mae', optimizer=Adam(0.0007))\n",
    "# model.compile(loss='mape', optimizer=Adam(0.0005))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_error = pd.DataFrame(\n",
    "        columns=['h', 'mae', 'rmse', 'mape', 'descriptions'])\n",
    "history = model.fit([train_t_y,train_X,train_emb], train_y, epochs=100, batch_size=40, validation_data=([val_t_y,val_X,val_emb], val_y), verbose=1, shuffle=False)\n",
    "# history = model.fit([train_X,train_emb], train_y, epochs=100, batch_size=40, verbose=1, shuffle=False)\n",
    "# history = ts_model.fit(train_X, train_y, epochs=50, batch_size=100, validation_data=(test_X, test_y), verbose=1, shuffle=False)\n",
    "# history = arma_model.fit(train_t_y, train_y, epochs=200, batch_size=40, validation_data=(test_t_y, test_y), verbose=1, shuffle=False)\n",
    "# # plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict([test_t_y,test_X,test_emb])\n",
    "# pred_y = model.predict([test_X,test_emb])\n",
    "# pred_y = ts_model.predict(test_X)\n",
    "# pred_y = arma_model.predict(test_t_y)\n",
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_y=pred_y[:,0,0,0]\n",
    "inverted_pred_y = y_scaler.inverse_transform(pred_y.reshape(test_y.shape))\n",
    "inverted_test_y = y_scaler.inverse_transform(test_y)\n",
    "# original_test_price=df_original_price.to_numpy()[train_size:]\n",
    "# inverted_pred_y=inverted_pred_y+original_test_price\n",
    "# inverted_test_y=inverted_test_y+original_test_price\n",
    "forecast_error=evaluate_series(inverted_test_y, inverted_pred_y, h)\n",
    "\n",
    "print(f\"GRU_,{forecast_error['mae'][0]},{forecast_error['rmse'][0]},{forecast_error['mape'][0]}\")\n",
    "forecast_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_series(no_diff_y[train_size:],df_ARMA[df_ARMA.index.isin(df_selected.index)][train_size:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast_error)\n",
    "df_forecast_error = df_forecast_error.append(\n",
    "    pd.DataFrame(forecast_error), ignore_index=True)\n",
    "mae = df_forecast_error[\"mae\"]\n",
    "rmse = df_forecast_error[\"rmse\"]\n",
    "mape = df_forecast_error[\"mape\"]\n",
    "k = 1\n",
    "msg = f\"\"\"\n",
    "Forecast Error ({k}-fold cross-validation)\n",
    "X: {X.shape}\n",
    "y: {y.shape}\n",
    "h= {h}\n",
    "Model: {model.__class__.__name__}\n",
    "MAE = {mae.mean():.6f} +/- {mae.std():.3f}\n",
    "RMSE = {rmse.mean():.6f} +/- {rmse.std():.3f}\n",
    "MAPE = {mape.mean():.6f} +/- {mape.std():.3f}\n",
    "\"\"\"\n",
    "print(msg)\n",
    "logging.info(msg)\n",
    "evaluation_result = {\n",
    "'h': h,\n",
    "'mae': [mae.mean()],\n",
    "'rmse': [rmse.mean()],\n",
    "'mape': [mape.mean()],\n",
    "'descriptions': [msg]\n",
    "}\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_forecast_error\n",
    "# model.save_weights(\"weights.h5\")\n",
    "# model.sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_series(inverted_test_y, inverted_pred_y, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import gzip\n",
    "\n",
    "# with gzip.GzipFile('./trained_models/model.pgz', 'w') as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gzip.open('./trained_models/model.pgz', 'r') as f:\n",
    "#     trained_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_y = trained_model.predict([test_X,test_emb])\n",
    "# inverted_pred_y = y_scaler.inverse_transform(pred_y.reshape(-1, 1))\n",
    "# inverted_test_y = y_scaler.inverse_transform(test_y)  # should be same as testXy\n",
    "# # inverted_pred_y=inverted_pred_y.ravel()+df_original_price.Price.to_numpy()[train_size:]\n",
    "# # inverted_test_y=inverted_test_y.ravel()+df_original_price.Price.to_numpy()[train_size:]\n",
    "# forecast_error = evaluate_series(inverted_test_y, inverted_pred_y, h)\n",
    "# forecast_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export architecture as json \n",
    "# import json\n",
    "# model_json=model.to_json()\n",
    "# model_name=\"hybrid_multimodal-03\"\n",
    "# with open(f'{model_name}.json', 'w') as f:\n",
    "#     json.dump(model_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare models\n",
    "# step_size=1\n",
    "# ts_size=X.shape[-1]\n",
    "\n",
    "# ts_inputs = Input(shape=(step_size,ts_size),name='ts_input')\n",
    "# # ts_model = Bidirectional(GRU(300,dropout=0.3,return_sequences=False))(ts_inputs)\n",
    "# # ts_model = LSTM(300,dropout=0.3,return_sequences=False)(ts_inputs)\n",
    "# # ts_model = Bidirectional(GRU(300,dropout=0.4,return_sequences=False))(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# ts_model=Reshape((step_size,ts_size))(ts_inputs)\n",
    "# ts_model=GRU(300,dropout=0.33,return_sequences=True)(ts_model)\n",
    "# ts_model= Dropout(0.3)(ts_model)\n",
    "# ts_model=GRU(300,dropout=0.33,return_sequences=False)(ts_model)\n",
    "# ts_model= Dropout(0.3)(ts_model)\n",
    "# # ts_model=Bidirectional(ConvLSTM2D(300,(1,5),dropout=0.3,return_sequences=False))(ts_model)\n",
    "# # ts_model=ConvLSTM2D(300,(1,5),dropout=0.3,return_sequences=True)(ts_model)\n",
    "# # ts_model=ConvLSTM2D(300,(1,5),dropout=0.3,return_sequences=False)(ts_model)\n",
    "# # ts_model=Reshape((10,100))(ts_model)\n",
    "# # ts_model = Bidirectional(GRU(200,dropout=0.1,return_sequences=True))(ts_model)\n",
    "# # ts_model = Bidirectional(GRU(100,dropout=0.1,return_sequences=False))(ts_model)\n",
    "# # ts_model = Dense(1)(ts_model)\n",
    "# # ts_model=Reshape((1,-1))(ts_model)\n",
    "# # ts_model = Dense(1)(ts_model)\n",
    "# ts_model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "# # model.compile(loss='mape', optimizer='adam')\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_size=emb.shape[-1]\n",
    "# emb_inputs = Input(shape=(past,3,emb_size),name='emb_input')\n",
    "# emb_model = Reshape((1,past,3,3,300),name='emb_reshape')(emb_inputs)\n",
    "# emb_model = Conv3D(50,(3,2,1), activation=\"relu\",padding='valid')(emb_model)\n",
    "# emb_model = Dropout(0.3)(emb_model)\n",
    "# emb_model = Conv3D(50,(5,1,2), activation=\"relu\",padding='valid')(emb_model)\n",
    "# emb_model = Dropout(0.3)(emb_model)\n",
    "# emb_model = Reshape((4,4,-1))(emb_model)\n",
    "# emb_model = Conv2D(10,(2,2), activation=\"relu\",padding='valid')(emb_model)\n",
    "# emb_model = Dropout(0.3)(emb_model)\n",
    "# # emb_model = Reshape((1,18,-1))(emb_model)\n",
    "# # emb_model = Dropout(0.4)(emb_model) \n",
    "# # emb_model = Dropout(0.4)(emb_model) \n",
    "# emb_model = Flatten()(emb_model)\n",
    "# # emb_model = Dense(1)(emb_model)\n",
    "# # opt=Adam(0.0007)\n",
    "# emb_model = Model(inputs=emb_inputs, outputs=emb_model)\n",
    "# # emb_model.compile(loss='mae', optimizer=opt)\n",
    "# # emb_model = Dense(10)(emb_model)\n",
    "# # emb_model.compile(loss='mape', optimizer='adam')\n",
    "# # emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_model = concatenate([ts_model.output,emb_model.output])\n",
    "# combined_model= Reshape((1,-1),name='alpha')(combined_model)\n",
    "# combined_model= Bidirectional(GRU(100,dropout=0.33,return_sequences=False))(combined_model)\n",
    "# # combined_model= Bidirectional(GRU(200,dropout=0.33,return_sequences=True))(combined_model)\n",
    "# # combined_model= Bidirectional(SimpleRNN(100,dropout=0.3))(combined_model)\n",
    "# combined_model = Dropout(0.3)(combined_model)\n",
    "# combined_model = Dense(1)(combined_model)\n",
    "# model = Model(inputs=[ts_model.input,emb_model.input], outputs=combined_model)\n",
    "# # model.compile(loss='mae', optimizer='adam')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# data_col=[t_y,X,emb,y]\n",
    "# with open('data_col.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_col, f)\n",
    "with open('data_col.pkl', 'rb') as f:\n",
    "    data_col=pickle.load(f)\n",
    "t_y,X,emb,y=data_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from keras.models import model_from_json\n",
    "with open(\"hybrid_multimodal-03.json\", 'r') as f:\n",
    "    model_json=json.loads(f.read())\n",
    "# loaded_model=model_from_json()\n",
    "# loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cv = get_TS_cv()\n",
    "    df_forecast_error = pd.DataFrame(\n",
    "        columns=['h', 'mae', 'rmse', 'mape', 'descriptions'])\n",
    "    for train_idx, test_idx in cv.split(y):\n",
    "        \n",
    "        # split train/test set for emb,X,y      \n",
    "        train_X = X[train_idx,:]\n",
    "        test_X = X[test_idx,:]\n",
    "        train_emb=emb[train_idx,:,:]\n",
    "        test_emb=emb[test_idx,:,:]\n",
    "        train_y = y[train_idx]\n",
    "        test_y = y[test_idx]\n",
    "        train_t_y = t_y[train_idx]\n",
    "        test_t_y = t_y[test_idx]\n",
    "\n",
    "        # normalize features\n",
    "        X_scaler = MinMaxScaler()\n",
    "        X_scaler.fit(train_X)\n",
    "        t_y_scaler = MinMaxScaler()\n",
    "        t_y_scaler.fit(train_t_y)\n",
    "        y_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "        y_scaler.fit(train_y)\n",
    "\n",
    "        train_X=X_scaler.transform(train_X)\n",
    "        test_X=X_scaler.transform(test_X)\n",
    "        train_t_y=y_scaler.transform(train_t_y)\n",
    "        test_t_y=y_scaler.transform(test_t_y)\n",
    "        train_y=y_scaler.transform(train_y)\n",
    "        test_y=y_scaler.transform(test_y)\n",
    "\n",
    "        # reshape to 3D for RNN/LSTM/GRU       \n",
    "        train_X=train_X.reshape(train_X.shape[0],step_size,train_X.shape[-1])\n",
    "        test_X=test_X.reshape(test_X.shape[0],step_size,test_X.shape[-1])\n",
    "        print(f\"train_X: {train_X.shape} test_X:{test_X.shape}\")\n",
    "        print(f\"train_t_y: {train_t_y.shape} test_t_y:{test_t_y.shape}\")\n",
    "        print(f\"train_y: {train_y.shape} test_y:{test_y.shape}\")\n",
    "\n",
    "        # ts_inputs = Input(shape=(step_size,train_X.shape[-1]))\n",
    "        # ts_model=Reshape((step_size,train_X.shape[-1]))(ts_inputs)\n",
    "        # ts_model=SimpleRNN(1,return_sequences=False)(ts_model)\n",
    "        # ts_model =Dense(1)(ts_model)\n",
    "        \n",
    "        \n",
    "        loaded_model=model_from_json(model_json)\n",
    "\n",
    "        # model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "        # model = Model(inputs=emb_inputs, outputs=emb_model)\n",
    "        # model = Model(inputs=[ts_model.input,emb_model.input], outputs=combined_model)\n",
    "        # loaded_model.compile(loss='mae', optimizer='adam')\n",
    "        # model.compile(loss='msle', optimizer=Adam(0.0005))\n",
    "        loaded_model.compile(loss='log_cosh', optimizer=Adam(0.0002))\n",
    "        history = loaded_model.fit(x=[train_t_y,train_X,train_emb], y=train_y, epochs=100, batch_size=40, verbose=0, shuffle=False)\n",
    "        # history = model.fit(x=train_emb, y=train_y, epochs=80, batch_size=100, validation_data=(test_emb, test_y), verbose=0, shuffle=False)\n",
    "        # history = model.fit(x=[train_X,train_emb], y=train_y, epochs=80, batch_size=40, validation_data=([test_X,test_emb], test_y), verbose=0, shuffle=False)\n",
    "        \n",
    "        # plot history\n",
    "        pyplot.plot(history.history['loss'], label='train')\n",
    "        # pyplot.plot(history.history['val_loss'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n",
    "        # pred_y = model.predict(test_X)\n",
    "        # pred_y = model.predict(test_emb)\n",
    "        pred_y = loaded_model.predict([test_t_y,test_X,test_emb])\n",
    "        print(f\"pred_y: {pred_y.shape}\")\n",
    "        \n",
    "        # scale\n",
    "        inverted_pred_y = y_scaler.inverse_transform(pred_y.reshape(test_y.shape))\n",
    "        inverted_test_y = y_scaler.inverse_transform(test_y)\n",
    "        # original_test_price=df_original_price.to_numpy()[test_idx]\n",
    "        # inverted_pred_y=inverted_pred_y+original_test_price\n",
    "        # inverted_test_y=inverted_test_y+original_test_price\n",
    "\n",
    "        forecast_error = evaluate_series(inverted_test_y, inverted_pred_y, h)\n",
    "        # forecast_error = evaluate_series(test_y.reshape(-1, 1), pred_y.reshape(-1, 1), h)\n",
    "        print(forecast_error)\n",
    "        df_forecast_error = df_forecast_error.append(\n",
    "            pd.DataFrame(forecast_error), ignore_index=True)\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        tf.random.set_seed(seed_value)\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    mae = df_forecast_error[\"mae\"]\n",
    "    rmse = df_forecast_error[\"rmse\"]\n",
    "    mape = df_forecast_error[\"mape\"]\n",
    "    k = cv.get_n_splits()\n",
    "    msg = f\"\"\"\n",
    "    Forecast Error ({k}-fold cross-validation)\n",
    "    X: {X.shape}\n",
    "    y: {y.shape}\n",
    "    h= {h}\n",
    "    MAE = {mae.mean():.6f} +/- {mae.std():.3f}\n",
    "    RMSE = {rmse.mean():.6f} +/- {rmse.std():.3f}\n",
    "    MAPE = {mape.mean():.6f} +/- {mape.std():.3f}\n",
    "    \"\"\"\n",
    "    print(msg)\n",
    "    logging.info(msg)\n",
    "    evaluation_result = {\n",
    "        'h': h,\n",
    "        'mae': [mae.mean()],\n",
    "        'rmse': [rmse.mean()],\n",
    "        'mape': [mape.mean()],\n",
    "        'descriptions': [msg]\n",
    "    }    \n",
    "except Exception as e:\n",
    "    logging.exception(\"EXCEPTION: %s\", e, exc_info=True)\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-fold results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"multi,{df_forecast_error['mae'].to_numpy().mean():.6f}, {df_forecast_error['rmse'].to_numpy().mean():.6f}, {df_forecast_error['mape'].to_numpy().mean():.6f}, {df_forecast_error['r2'].to_numpy().mean():.6f}\")\n",
    "df_forecast_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_log=\"BiGRU;\"\n",
    "# FE_log=\"CLC1(h=1); 1stDIFF(xy)+ RFE(Ridge,60); scale y;\"\n",
    "# msg=f\"{model_log} {FE_log} mse;\"\n",
    "# evaluation_result[\"descriptions\"]=msg\n",
    "# df_result = pd.DataFrame(evaluation_result)\n",
    "# df_result[\"time\"] = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# df_result = df_result[['time', 'descriptions', 'h', 'mae', 'rmse', 'mape']]\n",
    "# df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result.to_csv(f\"{HOME}/results/experiment_results.csv\",mode=\"a+\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f3e05a59671f1eb5b3f5f0e003aaa5a39f5d3316373e39c3606e56079185283"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('OPP-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
