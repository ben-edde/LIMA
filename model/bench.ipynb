{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, MaxPool3D, Bidirectional, ConvLSTM2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import pymongo\n",
    "import random\n",
    "import string\n",
    "import fasttext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "HOME = os.environ['LIMA_HOME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> list:\n",
    "    \"\"\"\n",
    "    clean text with tokenization; stemming; removing stop word, punctuation, number, and empty string.\n",
    "\n",
    "    Args:\n",
    "        text (str): text\n",
    "\n",
    "    Returns:\n",
    "        list: cleaned text as list of tokenized str\n",
    "    \"\"\"\n",
    "\n",
    "    # to list of token\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    # stemming and convert to lower case if not proper noun: punctuation and stop word seem to help POS tagging, remove them after stemming\n",
    "    word_tag = pos_tag(text)\n",
    "    porter = PorterStemmer()\n",
    "    text = [\n",
    "        porter.stem(each[0])\n",
    "        if each[1] != \"NNP\" and each[1] != \"NNPS\" else each[0]\n",
    "        for each in word_tag\n",
    "    ]\n",
    "\n",
    "    # remove stop word: it seems stemming skip stop word; OK to remove stop word after stemming;\n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    text = [each for each in text if not each in stop_word]\n",
    "\n",
    "    # remove punctuation\n",
    "    text = [\n",
    "        each.translate(str.maketrans('', '', string.punctuation))\n",
    "        for each in text\n",
    "    ]\n",
    "    # text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", text) # if using re\n",
    "\n",
    "    # convert number to <NUM>\n",
    "    text = [\"<NUM>\" if each.isdigit() else each for each in text]\n",
    "\n",
    "    # remove empty string\n",
    "    text = [each for each in text if each != \"\"]\n",
    "\n",
    "    return text\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = data.copy()\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "        \n",
    "\t\tnames += [f'{data.columns[j]}(t-{i})' for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [f'{data.columns[j]}(t)' for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [f'{data.columns[j]}(t+{i})' for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def get_TS_cv(k=10, test_size=None):\n",
    "    \"\"\"\n",
    "    ML models do not need to care about forecast horizon when splitting training and test set. Forecast horizon should be handled by feature preparation ([X_t-1,X_t-2...]). Actually repeated K-fold can also be used, but stick to TS split to align with TS_evaluate().\n",
    "    \"\"\"\n",
    "    return TimeSeriesSplit(\n",
    "        n_splits=k,\n",
    "        gap=0,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "\n",
    "def evaluate_series(y_true, y_pred, horizon):\n",
    "    \"\"\"\n",
    "    Some models (like ARIMA) may not support cross_validate(), compare the forecasting result directly\n",
    "    Args:\n",
    "        y_true: y of test set\n",
    "        y_pred: y of prediction\n",
    "        horizon: forecast horizon\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: single row DF with 3 metrics wrt horizon\n",
    "    \"\"\"\n",
    "    # RMSE\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2=r2_score(y_true, y_pred)\n",
    "    forecast_error = {\n",
    "        'h': horizon,\n",
    "        'mae': [mae],\n",
    "        'rmse': [rmse],\n",
    "        'mape': [mape],\n",
    "        'r2':[r2],\n",
    "        'descriptions': \"\"\n",
    "    }\n",
    "    return forecast_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "h = 1\n",
    "past = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient\n",
    "client= InfluxDBClient.from_config_file(f\"{HOME}/dev/DB/influxdb_config.ini\")\n",
    "query_api = client.query_api()\n",
    "df_WTI = query_api.query_data_frame(\"\"\"\n",
    "from(bucket: \"dummy\")\n",
    "  |> range(start: 2011-04-01, stop: 2019-04-01)\n",
    "  |> filter(fn: (r) => r[\"_measurement\"] == \"WTI\") \n",
    "  |> filter(fn: (r) => r[\"type\"] == \"closing_price\") \n",
    "  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "  |> drop(columns: [\"_start\", \"_stop\"])\n",
    "\"\"\")\n",
    "df_WTI=df_WTI[[\"_time\",\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]]\n",
    "df_WTI.columns=[\"Date\",\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]\n",
    "df_WTI.set_index(\"Date\",inplace=True)\n",
    "df_WTI.index=df_WTI.index.map(lambda each: each.date())\n",
    "df_WTI.index=pd.to_datetime(df_WTI.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=[each.month for each in df_WTI.index]\n",
    "day=[each.day for each in df_WTI.index]\n",
    "day_in_week=[each.weekday() for each in df_WTI.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt=pd.DataFrame()\n",
    "df_dt[\"month\"]=month\n",
    "df_dt[\"day\"]=day\n",
    "df_dt[\"day_in_week\"]=day_in_week\n",
    "df_dt.index=df_WTI.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mongo_db = mongo_client[\"lima\"]\n",
    "mongo_collection = mongo_db[\"investing_news\"]\n",
    "cursor = mongo_collection.find({\"News\":{\"$ne\":\"NEURONswap: First Dex To Implement Governance 2.0\"}})\n",
    "df_news =  pd.DataFrame(list(cursor))[[\"Date\",\"News\"]]\n",
    "df_news=df_news[df_news.Date.isin(df_WTI.index)]\n",
    "fasttext_model = fasttext.load_model(f\"{HOME}/data/big/cc.en.300.bin\")\n",
    "df_news.News = df_news.News.apply(lambda r: \" \".join(clean(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_aggregated(df):\n",
    "    df=df.copy()\n",
    "    df[\"Polarity\"] = df.apply(\n",
    "        lambda row: TextBlob(row['News']).sentiment.polarity, axis=1)\n",
    "    df[\"Subjectivity\"] = df.apply(\n",
    "        lambda row: TextBlob(row['News']).sentiment.subjectivity, axis=1)\n",
    "    df_daily_averaged_sentiment_score = df.groupby(['Date']).mean()\n",
    "    return df_daily_averaged_sentiment_score\n",
    "\n",
    "df_sentiment = get_sentiment_aggregated(df_news)\n",
    "df_sentiment[\"Combined_Sentiment\"]=df_sentiment.Polarity*(1+df_sentiment.Subjectivity)\n",
    "def decay_features(df):\n",
    "    window_size=5\n",
    "    feature_list=[]\n",
    "    for each in df.columns:\n",
    "        feature_list.append(df[each].iloc[:window_size].to_list())\n",
    "    feature_num=len(feature_list)\n",
    "    for idx in range(window_size,len(df_sentiment)):\n",
    "        feature_tmp=np.zeros(feature_num)\n",
    "        for t in range(window_size):\n",
    "            for feature_idx in range(feature_num):\n",
    "                feature_tmp[feature_idx]+=df.iloc[idx-t][df.columns[feature_idx]]*((window_size-t)/window_size)\n",
    "        for feature_idx in range(feature_num):\n",
    "            feature_list[feature_idx].append(feature_tmp[feature_idx])\n",
    "    df_result=pd.DataFrame(feature_list).transpose()\n",
    "    df_result.index=df.index\n",
    "    df_result.columns=[f\"Decay_{each}\" for each in df.columns]\n",
    "    return df_result\n",
    "\n",
    "df_res=decay_features(df_sentiment)\n",
    "df_sentiment=pd.concat([df_sentiment,df_res],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic modeling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_aggregated(df):\n",
    "    df=df.copy()\n",
    "    news_emb = df.News.apply(lambda x: fasttext_model.get_sentence_vector(\n",
    "        (x))).to_numpy().tolist()\n",
    "    news_emb = np.array(news_emb)\n",
    "    emb_scaler=MinMaxScaler()\n",
    "    news_emb=emb_scaler.fit_transform(news_emb)\n",
    "    lda_model=LatentDirichletAllocation(n_components=5,n_jobs=-1)\n",
    "    topic= lda_model.fit_transform(news_emb)\n",
    "    for i in range(5):\n",
    "        df[f\"Topic{i+1}\"] = topic[:, i]\n",
    "    df_daily_averaged_topic = df.groupby(['Date']).mean()\n",
    "    return df_daily_averaged_topic, emb_scaler, lda_model\n",
    "\n",
    "df_topic, emb_scaler, lda_model = get_topic_aggregated(df_news)\n",
    "df_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res=decay_features(df_topic)\n",
    "df_topic=pd.concat([df_topic,df_res],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy = pd.concat([df_sentiment,df_topic, df_WTI], axis=1, join=\"inner\")\n",
    "df_Xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_emb_maxmin(emb):\n",
    "    data=np.array(emb.to_numpy().tolist())\n",
    "    return np.concatenate([data.max(axis=0),data.min(axis=0)])\n",
    "def aggregate_emb_maxmeanmin(emb):\n",
    "    data=np.array(emb.to_numpy().tolist())\n",
    "    return np.concatenate([data.max(axis=0),data.mean(axis=0),data.min(axis=0)])\n",
    "\n",
    "news_emb = df_news.News.apply(lambda x: fasttext_model.get_sentence_vector(\n",
    "        (x)))\n",
    "# news_emb = np.array(news_emb)\n",
    "# emb_scaler=MinMaxScaler()\n",
    "# news_emb=emb_scaler.fit_transform(news_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb=pd.concat([df_news.Date,news_emb],axis=1)\n",
    "df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily mean of mean\n",
    "# df_emb[\"News\"]=df_emb.News.apply(lambda r:np.array(r.tolist()).mean())\n",
    "\n",
    "# daily mean\n",
    "# df_emb=df_emb.groupby(\"Date\").mean()\n",
    "\n",
    "# max + min\n",
    "# df_emb=df_emb.groupby(\"Date\")['News'].agg(aggregate_emb_maxmin)\n",
    "\n",
    "# max + mean + min\n",
    "df_emb=df_emb.groupby(\"Date\")['News'].agg(aggregate_emb_maxmeanmin)\n",
    "\n",
    "df_emb=pd.DataFrame(df_emb)\n",
    "# df_emb.set_index(\"Date\",inplace=True)\n",
    "df_emb.index=pd.to_datetime(df_emb.index)\n",
    "df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to align\n",
    "df_emb=df_emb[df_emb.index.isin(df_WTI[\"CLC1\"].index)]\n",
    "emb=df_emb.to_numpy()\n",
    "emb =np.array([each.tolist() for each in emb])\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_WTI[df_WTI.index.isin(df_emb.index)][\"CLC1\"].to_numpy()\n",
    "y.shape\n",
    "feature_name=\"EMB_maxmeanmin_single\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openie import StanfordOpenIE\n",
    "openie_client = StanfordOpenIE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_tuples = []\n",
    "for idx, row in df_news.iterrows():\n",
    "    text = row['News']\n",
    "    for triple in openie_client.annotate(text):\n",
    "        triple['Date'] = row['Date']\n",
    "        event_tuples.append(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.DataFrame(event_tuples)\n",
    "df_events.subject = df_events.subject.apply(\n",
    "    lambda x: fasttext_model.get_sentence_vector((x)))\n",
    "df_events.relation = df_events.relation.apply(\n",
    "    lambda x: fasttext_model.get_sentence_vector((x)))\n",
    "df_events.object = df_events.object.apply(lambda x: fasttext_model.get_sentence_vector(\n",
    "    (x)))\n",
    "df_events.dropna(inplace=True)\n",
    "df_events.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_emb_maxmeanmin(emb):\n",
    "    data=np.array(emb.to_numpy().tolist())\n",
    "    return np.concatenate([data.max(axis=0),data.mean(axis=0),data.min(axis=0)])\n",
    "\n",
    "# max + mean + min\n",
    "series_subject=df_events.groupby(\"Date\")['subject'].agg(aggregate_emb_maxmeanmin)\n",
    "series_relation=df_events.groupby(\"Date\")['relation'].agg(aggregate_emb_maxmeanmin)\n",
    "series_object=df_events.groupby(\"Date\")['object'].agg(aggregate_emb_maxmeanmin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_emb=pd.concat([series_subject,series_relation,series_object],axis=1)\n",
    "df_event_emb.index=pd.to_datetime(df_event_emb.index)\n",
    "df_event_emb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to align\n",
    "df_event_emb=df_event_emb[df_event_emb.index.isin(df_WTI[\"CLC1\"].index)]\n",
    "# join 3 : (-1,2700)\n",
    "# df_event_emb[\"event\"]=df_event_emb.apply(np.concatenate,axis=1)\n",
    "# join 3 : (-1,3,900)\n",
    "# df_event_emb[\"event\"]=df_event_emb.apply(np.array,axis=1)\n",
    "# event_emb=df_event_emb[\"event\"].to_numpy()\n",
    "# event_emb=np.array([each.tolist() for each in event_emb])\n",
    "# X=event_emb\n",
    "# event_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name=\"eventEMB_maxmeanmin_10step\"\n",
    "y=df_WTI[df_WTI.index.isin(df_event_emb.index)][\"CLC1\"].to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationary test before diff\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "for i in range(df_Xy.shape[1]):\n",
    "    test_result=adfuller(df_Xy[df_Xy.columns[i]].to_numpy())\n",
    "    if test_result[1]>0.05:\n",
    "        print(f\"{df_Xy.columns[i]}: {test_result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy.columns[:]\n",
    "# df_Xy.columns[7:]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "model = VAR(df_Xy[['Subjectivity','CLC1']].diff().dropna().to_numpy())\n",
    "var_result=model.select_order(40)\n",
    "var_result.selected_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(df_Xy.CLC1,lags=9, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "all_results=[]\n",
    "for each in df_Xy.columns:\n",
    "    if each ==\"CLC1\":\n",
    "        continue\n",
    "    rest_results=grangercausalitytests(df_Xy[[\"CLC1\",each]],maxlag=9,verbose=False)\n",
    "    for lag in range(1,10):\n",
    "        all_results.append({\"type\":each,\"lag\":lag, \"p\":rest_results[lag][0]['ssr_ftest'][1]})\n",
    "df_test=pd.DataFrame(all_results)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name=\"day\"\n",
    "df_selected=df_Xy#[[feature_name,'CLC1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection with Granger\n",
    "# df_g_test=df.sort_values(\"p\")\n",
    "# selected_features_series=[]\n",
    "# for idx,each in df_g_test.iterrows():\n",
    "#     series=df_Xy[each[\"type\"]].shift(each[\"lag\"])\n",
    "#     series.name=f\"{each['type']}(t-{each['lag']})\"\n",
    "#     selected_features_series.append(series)\n",
    "# df_selected=pd.concat(selected_features_series,axis=1).dropna()\n",
    "# df_selected.columns[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected[df_selected.columns[:1]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve original price for inverting prediction\n",
    "# df_original_price = df_selected[[\"CLC1\"]].shift(h).dropna()\n",
    "# 1st order DIFF\n",
    "# df_selected = df_selected.diff().dropna()\n",
    "# shift back $past days\n",
    "past=30\n",
    "df_selected = series_to_supervised(df_Xy[df_Xy.columns], past, h)\n",
    "# df_selected = series_to_supervised(df_emb, past, h)\n",
    "\n",
    "# df_original_price = df_original_price[df_original_price.index.isin(df_selected.index)]\n",
    "# remove current day features for forecast\n",
    "for each in df_selected.columns[:-1]:\n",
    "    if \"(t)\" in each:\n",
    "        df_selected.drop(each, axis=1, inplace=True)\n",
    "\n",
    "# add time feature without shift \n",
    "df_selected=pd.concat([df_dt,df_selected],axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_in_week</th>\n",
       "      <th>Polarity(t-30)</th>\n",
       "      <th>Subjectivity(t-30)</th>\n",
       "      <th>Combined_Sentiment(t-30)</th>\n",
       "      <th>Decay_Polarity(t-30)</th>\n",
       "      <th>Decay_Subjectivity(t-30)</th>\n",
       "      <th>Decay_Combined_Sentiment(t-30)</th>\n",
       "      <th>Topic1(t-30)</th>\n",
       "      <th>...</th>\n",
       "      <th>Decay_Topic1(t-1)</th>\n",
       "      <th>Decay_Topic2(t-1)</th>\n",
       "      <th>Decay_Topic3(t-1)</th>\n",
       "      <th>Decay_Topic4(t-1)</th>\n",
       "      <th>Decay_Topic5(t-1)</th>\n",
       "      <th>CLC4(t-1)</th>\n",
       "      <th>CLC3(t-1)</th>\n",
       "      <th>CLC2(t-1)</th>\n",
       "      <th>CLC1(t-1)</th>\n",
       "      <th>CLC1(t)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-05-16</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.11566</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.11566</td>\n",
       "      <td>0.106026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547563</td>\n",
       "      <td>0.628016</td>\n",
       "      <td>0.90359</td>\n",
       "      <td>0.630677</td>\n",
       "      <td>0.290154</td>\n",
       "      <td>100.79</td>\n",
       "      <td>100.48</td>\n",
       "      <td>100.12</td>\n",
       "      <td>99.65</td>\n",
       "      <td>97.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 604 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            month  day  day_in_week  Polarity(t-30)  Subjectivity(t-30)  \\\n",
       "Date                                                                      \n",
       "2011-05-16      5   16            0        0.085833              0.3475   \n",
       "\n",
       "            Combined_Sentiment(t-30)  Decay_Polarity(t-30)  \\\n",
       "Date                                                         \n",
       "2011-05-16                   0.11566              0.085833   \n",
       "\n",
       "            Decay_Subjectivity(t-30)  Decay_Combined_Sentiment(t-30)  \\\n",
       "Date                                                                   \n",
       "2011-05-16                    0.3475                         0.11566   \n",
       "\n",
       "            Topic1(t-30)  ...  Decay_Topic1(t-1)  Decay_Topic2(t-1)  \\\n",
       "Date                      ...                                         \n",
       "2011-05-16      0.106026  ...           0.547563           0.628016   \n",
       "\n",
       "            Decay_Topic3(t-1)  Decay_Topic4(t-1)  Decay_Topic5(t-1)  \\\n",
       "Date                                                                  \n",
       "2011-05-16            0.90359           0.630677           0.290154   \n",
       "\n",
       "            CLC4(t-1)  CLC3(t-1)  CLC2(t-1)  CLC1(t-1)  CLC1(t)  \n",
       "Date                                                             \n",
       "2011-05-16     100.79     100.48     100.12      99.65    97.37  \n",
       "\n",
       "[1 rows x 604 columns]"
      ]
     },
     "execution_count": 815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(1982, 603) | (1982, 1)'"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_X = df_selected.to_numpy()[:, :-1]\n",
    "y =  df_selected.to_numpy()[:, -1].reshape(-1, 1) \n",
    "# y = df_Xy[df_Xy.index.isin(df_selected.index)].to_numpy()[:, -1].reshape(-1, 1)\n",
    "# y=df_WTI[df_WTI.index.isin(df_selected.index)][\"CLC1\"].to_numpy().reshape(-1, 1)\n",
    "f\"{raw_X.shape} | {y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding\n",
    "# raw_X=np.array([each.tolist() for each in raw_X])\n",
    "# raw_X=np.array([each.tolist() for each in raw_X])\n",
    "# raw_X.shape\n",
    "# X=raw_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression,RFE,RFECV,SelectFromModel,SequentialFeatureSelector,chi2,SelectKBest,f_regression,VarianceThreshold,r_regression\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor\n",
    "from sklearn.svm import LinearSVR,SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1982, 603) | (1982, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{raw_X.shape} | {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsfel.feature_extraction.features.abs_energy()\n",
    "def tswrapper(a,b):\n",
    "    a=MinMaxScaler((1,100)).fit_transform(raw_X)\n",
    "    result=pd.DataFrame(a).apply(tsfel.feature_extraction.features.autocorr).to_numpy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.conda/envs/OPP-env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.412e+02, tolerance: 1.067e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# selector based\n",
    "estimator = Lasso(random_state=42)\n",
    "# estimator = DecisionTreeRegressor(criterion='poisson',random_state=42)\n",
    "# estimator = Ridge(0.5,random_state=42)\n",
    "# selector = RFE(estimator,step=1)\n",
    "# selector = RFECV(estimator, min_features_to_select=30, cv=get_TS_cv(),step=1,n_jobs=-1)\n",
    "selector=SelectFromModel(estimator,max_features=30)\n",
    "# selector=SequentialFeatureSelector(estimator,n_features_to_select=100,direction='forward',n_jobs=-1)\n",
    "# selector=SelectKBest(tswrapper,k=100)\n",
    "# selector=VarianceThreshold(3.21)\n",
    "scaled_raw_X=MinMaxScaler((1,100)).fit_transform(raw_X)\n",
    "# scaled_y=MinMaxScaler((1,100)).fit_transform(y)\n",
    "selector = selector.fit(scaled_raw_X,y)\n",
    "# selector = selector.fit(raw_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1982, 16) | (1982, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['day', 'Decay_Topic2(t-30)', 'Decay_Topic4(t-30)', 'Topic5(t-18)',\n",
       "       'Decay_Topic2(t-17)', 'Decay_Topic4(t-12)', 'Decay_Topic2(t-10)',\n",
       "       'Decay_Topic4(t-5)', 'Decay_Topic2(t-4)', 'Decay_Topic2(t-2)',\n",
       "       'Decay_Topic4(t-2)', 'CLC1(t-2)', 'Decay_Topic4(t-1)', 'CLC3(t-1)',\n",
       "       'CLC2(t-1)', 'CLC1(t-1)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = raw_X[:, selector.get_support()]\n",
    "feature_name=f\"scale_RFE_Lasso_{X.shape[-1]}\"\n",
    "print(f\"{X.shape} | {y.shape}\")\n",
    "df_selected.columns[:-1][selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA,FastICA,FactorAnalysis,NMF\n",
    "# scaled_raw_X=MinMaxScaler((1,100)).fit_transform(raw_X)\n",
    "# # pca = PCA(n_components=150,svd_solver='full')\n",
    "# pca = NMF(n_components=150,max_iter=1000)\n",
    "# # decomposer = FactorAnalysis(n_components=7)\n",
    "# X=pca.fit_transform(scaled_raw_X)\n",
    "# feature_name=f\"NMF_{X.shape[-1]}\"\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=raw_X\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forecast Error (10-fold cross-validated performance):\n",
      "Ridge:\n",
      "MAE = 1.049 +/- 0.331\n",
      "RMSE = 1.347 +/- 0.411\n",
      "MAPE = 0.016 +/- 0.005\n",
      "\n",
      "Ridge_scale_RFE_Lasso_50,1.048722,1.346966,0.016429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, ARDRegression, SGDRegressor, ElasticNet, Lars, Lasso, GammaRegressor, TweedieRegressor, PoissonRegressor, Lasso, Ridge, BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor\n",
    "\n",
    "lin_model=Ridge(random_state =42)    #LinearRegression,Ridge,LinearSVR\n",
    "model_name=\"Ridge\"\n",
    "\n",
    "msg=f\"{model_name}_{feature_name}\"\n",
    "\n",
    "X_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "scaled_X=X_scaler.fit_transform(X)\n",
    "\n",
    "cv_results = cross_validate(lin_model,\n",
    "                                scaled_X,\n",
    "                                y.ravel(),\n",
    "                                scoring=[\n",
    "                                    'neg_mean_absolute_error',\n",
    "                                    'neg_root_mean_squared_error',\n",
    "                                    'neg_mean_absolute_percentage_error'\n",
    "                                ],\n",
    "                                cv=get_TS_cv(),\n",
    "                                n_jobs=-1)\n",
    "mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "mape = -cv_results[\"test_neg_mean_absolute_percentage_error\"]\n",
    "k = 10\n",
    "print(f\"\"\"\n",
    "Forecast Error ({k}-fold cross-validated performance):\n",
    "{lin_model.__class__.__name__}:\n",
    "MAE = {mae.mean():.3f} +/- {mae.std():.3f}\n",
    "RMSE = {rmse.mean():.3f} +/- {rmse.std():.3f}\n",
    "MAPE = {mape.mean():.3f} +/- {mape.std():.3f}\n",
    "\"\"\")\n",
    "print(f\"{msg},{mae.mean():.6f},{rmse.mean():.6f},{mape.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape,MaxPooling2D,Bidirectional,ConvLSTM2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN \n",
    "from keras.layers import Conv2D \n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.random.set_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(1982, 50) | (1982, 1)'"
      ]
     },
     "execution_count": 1049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X=event_emb.reshape(-1,3,emb.shape[-1])\n",
    "# y=y.reshape(-1,1)\n",
    "# X=X.reshape(-1,10,900)\n",
    "# X=X.reshape(-1,10,3,900)\n",
    "f\"{X.shape} | {y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X: (1189, 1, 50)\n",
      "test_X:(793, 1, 50)\n",
      "train_y: (1189, 1)\n",
      "test_y:(793, 1)\n"
     ]
    }
   ],
   "source": [
    "length=X.shape[0]\n",
    "train_size=int(length*0.6)\n",
    "step_size=1\n",
    "\n",
    "train_X=X[:train_size]\n",
    "train_y=y[:train_size,:]\n",
    "test_X=X[train_size:]\n",
    "test_y=y[train_size:,:]\n",
    "\n",
    "X_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "X_scaler.fit(train_X)\n",
    "y_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "y_scaler.fit(train_y)\n",
    "\n",
    "train_X=X_scaler.transform(train_X)\n",
    "test_X=X_scaler.transform(test_X)\n",
    "train_y=y_scaler.transform(train_y)\n",
    "test_y=y_scaler.transform(test_y)\n",
    "\n",
    "train_X=train_X.reshape(train_X.shape[0],step_size,train_X.shape[-1])\n",
    "test_X=test_X.reshape(test_X.shape[0],step_size,test_X.shape[-1])\n",
    "print(f\"train_X: {train_X.shape}\\ntest_X:{test_X.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\\ntest_y:{test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_inputs = Input(shape=(3,step_size,train_X.shape[-1]))\n",
    "# emb_model = Reshape((1,10,1,train_X.shape[-1]))(emb_inputs)\n",
    "# emb_model = ConvLSTM2D(300,(5,1),padding='same')(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = Reshape((1,-1,1,train_X.shape[-1]))(emb_inputs)\n",
    "# emb_model = ConvLSTM2D(300,(5,1),padding='same')(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = Reshape((1,-1,1,train_X.shape[-1]))(emb_inputs)\n",
    "# emb_model = ConvLSTM2D(300,(5,1),padding='same')(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = Reshape((1,-1,1,train_X.shape[-1]))(emb_inputs)\n",
    "# emb_model = ConvLSTM2D(300,(5,1),padding='same')(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = Reshape((1,-1,1,train_X.shape[-1]))(emb_inputs)\n",
    "# emb_model = ConvLSTM2D(300,(10,1))(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "\n",
    "# emb_inputs = Input(shape=(step_size,train_X.shape[-1]))\n",
    "# emb_model = Reshape((1,step_size,1,train_X.shape[-1]))(emb_inputs)\n",
    "# emb_model = Conv2D(100,(5,1), activation=\"relu\",padding='valid',data_format=\"channels_last\")(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = Conv2D(1,(6, 1), activation=\"relu\",padding='valid',data_format=\"channels_last\")(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = Conv2D(300,(3,10), activation=\"relu\")(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = Reshape((1,step_size,900,300))(emb_model)\n",
    "# emb_model = ConvLSTM2D(100,(1,50), activation=\"relu\",data_format=\"channels_first\",return_sequences=True)(emb_model)\n",
    "# emb_model = Dropout(0.4)(emb_model)\n",
    "# emb_model = ConvLSTM2D(100,(1,50), activation=\"relu\",data_format=\"channels_first\")(emb_model)\n",
    "# emb_model = Reshape((-1,100))(emb_model)\n",
    "# emb_model = GRU(100)(emb_model)\n",
    "# emb_model = Dense(1)(emb_model)\n",
    "\n",
    "# model = Model(inputs=emb_inputs, outputs=emb_model)\n",
    "# model.compile(loss='mse', optimizer='adam')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 50)]           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 1000)             1656000   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,657,001\n",
      "Trainable params: 1,657,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "ts_inputs = Input(shape=(step_size,train_X.shape[-1]))\n",
    "ts_model = Bidirectional(GRU(500,dropout=0.1,return_sequences=False))(ts_inputs)\n",
    "ts_model= Dropout(0.2)(ts_model)\n",
    "ts_model =Dense(1)(ts_model)\n",
    "model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxK0lEQVR4nO3deXxU1f3/8ddJZjKTzGTfIUBCwIVFdoqCrTugVtygarH0W1v6ba3F/mpb7e63m938Umvdqla+bsW64YKKIigqssoqgbAnJGQleybJzJzfH2dCEraEbMOd+TwfDx7J3Jm5c+4wed9zP+fcO0prjRBCCOuJCHYDhBBCdI8EuBBCWJQEuBBCWJQEuBBCWJQEuBBCWJStP18sJSVFZ2dn9+dLCiGE5W3YsKFca5167PJ+DfDs7GzWr1/fny8phBCWp5Q6cKLlUkIRQgiLkgAXQgiLkgAXQgiL6tcauBBCnK6WlhYKCwvxeDzBbkqfczqdZGVlYbfbu/R4CXAhxBmtsLCQ2NhYsrOzUUoFuzl9RmtNRUUFhYWF5OTkdOk5UkIRQpzRPB4PycnJIR3eAEopkpOTT+tIQwJcCHHGC/XwbnW622mJAF++o4SHVu4OdjOEEOKMYokA/3BXGY+s3BPsZgghwlRVVRUPPfTQaT/vyiuvpKqqqvcbFGCJAI912qlr8iJfPiGECIaTBbjP5zvl85YuXUpCQkIftcois1BinTb8GhqafbgclmiyECKE3H333ezZs4exY8dit9txu91kZmayadMmPv/8c6699loKCgrweDwsWLCA+fPnA22XD6mrq2PmzJlMmzaNTz75hIEDB7JkyRKio6N71K4upaFSaj9QC/gAr9Z6olIqCVgMZAP7gTla6yM9as1JuJ2mmbUerwS4EGHs3te383lRTa+uc8SAOH715ZGnfMx9993Htm3b2LRpEytXruSqq65i27ZtR6f7PfnkkyQlJdHY2MikSZO44YYbSE5O7rCO/Px8nn/+ef75z38yZ84cXnrpJebOndujtp9OCeVirfVYrfXEwO27geVa6+HA8sDtPhHrNJPaaz0tffUSQgjRZZMnT+4wV/uBBx5gzJgxTJkyhYKCAvLz8497Tk5ODmPHjgVgwoQJ7N+/v8ft6El3dhZwUeD3RcBK4Cc9bM8JxQZ63bVN3r5YvRDCIjrrKfcXl8t19PeVK1fy3nvvsXr1amJiYrjoootOOJfb4XAc/T0yMpLGxsYet6OrPXANLFNKbVBKzQ8sS9daFwMEfqad6IlKqflKqfVKqfVlZWXdamRsuxKKEEL0t9jYWGpra094X3V1NYmJicTExJCXl8enn37ab+3qag98qta6SCmVBryrlMrr6gtorR8DHgOYOHFit6aRtJZQ6iTAhRBBkJyczNSpUxk1ahTR0dGkp6cfvW/GjBk88sgjnHfeeZx99tlMmTKl39rVpQDXWhcFfpYqpV4BJgMlSqlMrXWxUioTKO2rRrYNYkoNXAgRHM8999wJlzscDt56660T3tda505JSWHbtm1Hl99111290qZOSyhKKZdSKrb1d+AKYBvwGjAv8LB5wJJeadEJSAlFCCGO15UeeDrwSuAcfRvwnNb6baXUOuAFpdRtwEFgdl810h0lg5hCCHGsTgNca70XGHOC5RXApX3RqGNFRCjcDpuUUIQQoh1LnEoPpowiJRQhhGhjqQCXWShCCNHGMgHudtiobZISihBCtLJMgMc67dIDF0IERXcvJwuwcOFCGhoaerlFhoUCXGrgQojgOFMD3DKX9ot12qiRABdCBEH7y8lefvnlpKWl8cILL9DU1MR1113HvffeS319PXPmzKGwsBCfz8cvfvELSkpKKCoq4uKLLyYlJYUVK1b0arssFOB26qQGLkR4e+tuOLy1d9eZMRpm3nfKh7S/nOyyZct48cUXWbt2LVprrrnmGj788EPKysoYMGAAb775JmCukRIfH8/999/PihUrSElJ6d12Y6ESitthw9Pip8XnD3ZThBBhbNmyZSxbtoxx48Yxfvx48vLyyM/PZ/To0bz33nv85Cc/YdWqVcTHx/d5WyzUA287nT7JFRXk1gghgqKTnnJ/0Fpzzz338O1vf/u4+zZs2MDSpUu55557uOKKK/jlL3/Zp22xTA9crkgohAiW9peTnT59Ok8++SR1dXUAHDp0iNLSUoqKioiJiWHu3LncddddbNy48bjn9jbL9MDdgS91qJHT6YUQ/az95WRnzpzJLbfcwvnnnw+A2+3mmWeeYffu3fzoRz8iIiICu93Oww8/DMD8+fOZOXMmmZmZ4TuIGRcoodTJBa2EEEFw7OVkFyxY0OF2bm4u06dPP+55d9xxB3fccUeftMlyJRSZCy6EEIZlAly+1EEIITqyTIDHSglFiLCldbe+jdFyTnc7LRfgUkIRIrw4nU4qKipCPsS11lRUVOB0Orv8HMsMYjpskURFRkiACxFmsrKyKCwspKysLNhN6XNOp5OsrKwuP94yAQ6tF7SSGrgQ4cRut5OTkxPsZpyRLFNCAbkioRBCtGepAHc7bTKIKYQQAZYK8FiHXUooQggRYKkAd0sJRQghjrJUgEsNXAgh2lgqwOOcUkIRQohWlgpwt8MMYob6hH4hhOgKSwV4rNOGX0NDsy/YTRFCiKCzWIDLFQmFEKKVpQLcffSCVlIHF0IISwV46wWtaqQHLoQQ1grwOLkioRBCHGWpAHc75IuNhRCilaUCPFa+lUcIIY6yZIDLBa2EEMJiAe6KsqGUDGIKIQScRoArpSKVUp8ppd4I3E5SSr2rlMoP/Ezsu2YaEREKd5R8qYMQQsDp9cAXADva3b4bWK61Hg4sD9zuc26nTQYxhRCCLga4UioLuAp4vN3iWcCiwO+LgGt7tWUnIVckFEIIo6s98IXAjwF/u2XpWutigMDPtBM9USk1Xym1Xim1vje+lDTWaZdBTCGEoAsBrpS6GijVWm/ozgtorR/TWk/UWk9MTU3tzio6cDukBi6EENC1b6WfClyjlLoScAJxSqlngBKlVKbWulgplQmU9mVDW8U6bRRUNvTHSwkhxBmt0x641voerXWW1jobuAl4X2s9F3gNmBd42DxgSZ+1sp1Yp51aKaEIIUSP5oHfB1yulMoHLg/c7nNmEFNKKEII0ZUSylFa65XAysDvFcClvd+kU0tyReFp8VPX5MXtOK3mCyFESLHUmZgAAxOiATh0pDHILRFCiOCyXoAnBgK8SgYyhRDhzXIBniU9cCGEACwY4CluB1GRERRWSYALIcKb5QI8IkIxIMEpPXAhRNizXICDqYMfkh64ECLMWTPAE6KlBy6ECHsWDfAYSmubaPL6gt0UIYQIGmsGeGAqYXGVJ8gtEUKI4LFmgLdOJZQ6uBAijFkywLMSZS64EEJYMsAz4p1EKGQuuBAirFkywO2REaTHyVxwIUR4s2SAQ2AqoVwPRQgRxqwb4HIyjxAizFk3wBOiKa7y4PPrYDdFCCGCwroBnhiN168prZW54EKI8GTdAJfLygohwpxlA/zoXHCpgwshwpRlA3xAoAdeKD1wIUSYsmyAx0TZSHJFSQ9cCBG2LBvgIJeVFUKEN+sHuPTAhRBhytoBnmh64FrLXHAhRPixdIBnJUbT2OKjor452E0RQoh+Z+kAz0lxAbC3rD7ILRFCiP5n6QDPTXUDsLesLsgtEUKI/mfpAB+QEI3DFsHecumBCyHCj6UDPDJCkZPiYk+p9MCFEOHH0gEOMDTVJT1wIURYsnyA56a6OVjZQLPXH+ymCCFEv7J8gA9NdeHzaw5WSi9cCBFeLB/grTNRdpdKgAshwkunAa6Uciql1iqlNiultiul7g0sT1JKvauUyg/8TOz75h7v6FzwchnIFEKEl670wJuAS7TWY4CxwAyl1BTgbmC51no4sDxwu9/FOu2kxTrkZB4hRNjpNMC10dq9tQf+aWAWsCiwfBFwbV80sCtyU93skZN5hBBhpks1cKVUpFJqE1AKvKu1XgOka62LAQI/007y3PlKqfVKqfVlZWW91OyOhqa62FtWLxe1EkKElS4FuNbap7UeC2QBk5VSo7r6Alrrx7TWE7XWE1NTU7vZzFPLTXVT3dgiF7USQoSV05qForWuAlYCM4ASpVQmQOBnaW83rquGpspFrYQQ4acrs1BSlVIJgd+jgcuAPOA1YF7gYfOAJX3Uxk61TiWUOrgQIpzYuvCYTGCRUioSE/gvaK3fUEqtBl5QSt0GHARm92E7T+noRa0kwIUQYaTTANdabwHGnWB5BXBpXzTqdLVe1EpKKEKIcGL5MzFbDU11SQlFCBFWQibAc1PdFBxppMnrC3ZThBCiX4RMgB+9qFVFQ7CbIoQQ/SJkAjwnxcxE2SfXBhdChInQCfBkMxd8f4UEuBAiPIRMgMfH2EmMsbOvXEooQojwEDIBDpCd4mK/lFCEEGEipAI8J8UlNXAhRNgIrQBPdnG4xkNjs0wlFEKEvpAK8OwUGcgUQoSPkArw1q9XkzKKECIchFSAZ0uACyHCSEgFuNthIzXWITNRhBBhIaQCHGQmihAifIRegCe7ZBBTCBEWQi7As1NclNc1U+NpCXZThBCiT4VcgLfORJE6uBAi1IVsgEsdXAgR6kIuwIckxwCwXy5qJYQIcSEX4E57JAPinTKQKYQIeSEX4AA5qS72SglFCBHiQjLAs5PlsrJCiNAXkgGek+KiurGFI/XNwW6KEEL0mZANcEDKKEKIkBaSAT401XzB8d6yuiC3RAgh+k5IBvigxGjskYo9ZdIDF0KErpAMcFtkBEOSXeyRHrgQIoSFZIAD5Ka6pIQihAhpIRzgbg5UNNDi8we7KUII0SdCNsCHprrx+jUFlXJKvRAiNIVsgOemmqmEMpAphAhVIRvgrVMJZSBTCBGqQjbA46PtpLgdMpAphAhZnQa4UmqQUmqFUmqHUmq7UmpBYHmSUupdpVR+4Gdi3zf39OSmuqSEIoQIWV3pgXuBH2qtzwWmALcrpUYAdwPLtdbDgeWB22eUoalu6YELIUJWpwGutS7WWm8M/F4L7AAGArOARYGHLQKu7aM2dltuqosjDS1UykWthBAh6LRq4EqpbGAcsAZI11oXgwl5IK3XW9dDuWlyTRQhROjqcoArpdzAS8CdWuua03jefKXUeqXU+rKysu60sdtyU2QmihAidHUpwJVSdkx4P6u1fjmwuEQplRm4PxMoPdFztdaPaa0naq0npqam9kabu2xgYjRRtggZyBRChKSuzEJRwBPADq31/e3ueg2YF/h9HrCk95vXM5ERiqEpck0UIURosnXhMVOBW4GtSqlNgWU/Be4DXlBK3QYcBGb3SQt7aGiqix3FtcFuhhBC9LpOA1xr/RGgTnL3pb3bnN6Xm+rmne0lNHv9RNlC9rwlIUQYCvlEy0114/NrDlZKHVwIEVpCPsCHBi5qtbtU6uBCiNASBgHeOpVQeuBCiNAS8gHudtjIjHfKXHAhRMgJ+QAHUweXHrgQItSESYC72Ftah9Y62E0RQoheEx4BnuamtslLaW1TsJsihBC9JjwCvHUgU2aiCCFCSFgE+LA0uaiVECL0hEWAp8U6cDtsMpAphAgpYRHgSqnA16tJD1wIETrCIsDB1MHlbEwhRCgJnwBPc1Nc7aGuyRvspgghRK8InwAPzETZJ3VwIUSICJsAH5ZmLmoldXAhRKgImwAfnOQiMkJJgAshQkbYBHiULYIhSTEykCmECBlhE+BgLi0rPXAhRKgIqwDPTXOxv7wBr88f7KYIIUSPhVWAD0t10+zzU3ikMdhNEUKIHgurAM8NXBNF6uBCiFAQVgE+PBDgeYdrgtwSIYToubAK8FinnZwUF9sOSYALIawvrAIcYOSAOLYVVQe7GUII0WNhGODxFB5ppLqhJdhNEUKIHgm7AB81MA6A7dILF0JYXNgF+MgB8QBSRhFCWF7YBXiSK4qBCdEykCmEsLywC3CQgUwhRGgI0wCPZ195PfXy5Q5CCAuzBbsBfa40Dz75O9QcgoYKaGnggrG/53817CiuYWJ2UrBbKIQQ3RK6PfDmBnjvXnhkKux4DZpqITYD6koZXfgsANsOSRlFCGFdodMDP7QB3vwh+Lxgc0BNEdQWwZhb4IrfgCvFPG7pj3FseIqhruvYViQDmUII6wqNHrjfB699H6oLIWEwOOMhYzTMewOue7gtvAHGfRXla+K/4tdLD1wIYWmd9sCVUk8CVwOlWutRgWVJwGIgG9gPzNFaH+m7ZnZi4yIo2Qazn4KR1536sZljIGM0l9e+x6+PTMXT4sNpj+yXZgohRG/qSg/8KWDGMcvuBpZrrYcDywO3g6PxCCz/DQyZCiOu7dpzxs4loz6P4foAu0pq+7R5QgjRVzoNcK31h0DlMYtnAYsCvy8Cru3dZp2GD/4EniqYcR8o1bXnjJ6NjrAzO/IDthRKGUUIYU3drYGna62LAQI/0072QKXUfKXUeqXU+rKysm6+3EmU7YS1j8H4eZB5Xtef50qGc67ketvH7N6xGT5aCE9Mh/X/6t329YfyfKjr5fdVCGEJfT6IqbV+TGs9UWs9MTU1tXdX/t6vwR4Dl/z8tJ+qxs4lkRp+feBWeO9XcGQfvHEnbHy64wNLd8CR/V1f8YHV8PY94G3u2uOrDsIL86C2pONybzO8fie8/zs48An4TnD1RG8zPHEF/Gde19snhAgZ3Q3wEqVUJkDgZ2nvNekEGiph/8cdlx1YDTuXwrQ7O84y6arcS9ibcwt/avkKeXNWwZ1bIfcSeO0O2PoiVOyBxbfCQ1PggfGw5HY4cqDz9a76K3z6ELy+ALQ+9WN9Xnjpm/D5q7D5uY737V0BG/4FH/4J/jUT/pgDu97p+Jjd70JjJRz4+Pj3p3QHFH3WeXuFEJbV3QB/DWjt9s0DlvROc07irR/DMzdA4XpzW2t495fgzoAvfKd764y0EXfDQh7yzWJ5SYyZO/6VZ81g6Mvz4R+TYfdyuOge+MK3Yct/4O8T4LGL4R9fgP8dZaYuttfigf0fQewAE8ir/nrqNqz6CxSsAWcCbH+1433bXwVHPNy1G77yDLhTYfn/dNwpbFkMMSngSjVB36q2BJ66Ch6/DLa/0r33R1ifp/r0jh6F5XRlGuHzwEVAilKqEPgVcB/wglLqNuAgMLsvG8n0P5jwfnY23LbM1L4L18KX/wZRMd1ebYrbwbmZcazKL+P2i4eZdd3yb3jlv8GdBl+6G2LTzYPP/x58/DeoyIf4LHNq/mdPwyW/MOEKcHA1eBvh6qdg20vw/m8gMgqaamDP+6ZcMuHrMOW7pnb9wR/hvJsgfYTZIVXug6QcUxrJexPOucqs+9wvm6OQ179vetvZ06CxCna+bdYXP9A8v2AdZE00RwvN9ZBxHrz4DWiqg/G3mvVvehbKd0HycEg7F1KGQ/wgiE7s+iCw6LqaInMph0t+0aPPare8eZc5artzs/n/FSGn0wDXWt98krsu7eW2nJw7FW592dR7n77OhGLycBg7t8ervnB4Cv/6eB8NzV5iomzgiIWbnj3+gfED4cp2vdzDW+GRaZD3Okz8hlm2Z7lpW86FkHsxVBfAu78AFWmCdeAE+PDP8OkjYI82wXnln00Z5N1fwudLTElo7wpoqoaR17a93nlzTM3/04dNgO94DXxNcN5XIPVsMxD74Z/grOmmtHLlX2DsV2HxXHjte7D+SSjaCCrCnOy04w3Qvrb122MgaxLc/DxEudqWV+6Ft+6GGX+A5Nwev99B4ffD+ifMe5MwuON9zfVgc0JEH50LsGGRKaklDIYp3Txa7I6WRlNibK4zn5mLf9p/ry36jXXOxEwaCl990cz7rtwDl/0KInt+JYBpw1Jo8WnW7Dt2pmQn0kdBUm7HEsXu92HwFBOANgfc8gJ89SX4yT5z5HDLYvjOJzDsUvMHdsPj4IyDxGwYMM7UwqGtfDL04rZ126NNb3vnUlOL3/KCef2B48HhhvO/C/nL4O2fwrDLYNI3TY/v5udh1I3mcPrin8Od22DBZvhZMXxnNcz5P3OEc94c2PeBWW97H/8N8t+Bl79lavZ94dAGc4RyrIJ1sPKPnY8ltNLaHJ0d+/hNz8LSu2DRNR1n7BR9Bv870pTD3v+dOULqbfnLzM9PHuz6wHZv2PO+Ce+EwabD0FjVdt/BNfDOz8wZzMLSrBPgAAPGwtyXzayTc67ulVVOzkkiyhbBR/nlp/dEpcxZn/s/MqFQUwSl2yG33YGJMw6GX2ZO7W+VPhLmLIJ7CmDQ5LblI641gVKe31Y+sUV1fM1JtwHK9MT3f2R6361lj8nzTehHuWDWP9qW2xxw4xPw/Y3wpR+ZI4nW5ekjYMQsE/5XLzSXH1jzaFsANlTC5sWQeo4J2Y/ub2tL8ZbAuMSGjm3U2gRG6Y7j37Nd7xz/+OYG+PdcWPw18BxzbZp3fgorfw+b/338uk5kxe/N2MWqv7Qt81TD8nsh9VyoPQzPzTYlpYJ1sGgWRMWa9+HDP8PC82DJ98zOtTfUlZqjniFToaYQtv6n8+f4/bDhKTMIvujL8MA4eOMHpuPSqvYwLP0x/Ofr5rOw4Snz+Wvv8yWmbDJ7kTma+/Rhs7xsl3kPVj9oxnjaqz1sZlA1nGZnRgSNtQIcYPAX4Is/6rV6rdMeyeTsJFbld2Mu9chrQftNGaW1Bzmsi5WlY9vfWi554wfHl09axWfBiGtg+8uAhtE3tt3njIdbX4H/Wmquuni6lILJ34ayHbB/lVn22dOmpn/DE6YX/8EfzU5m82J44nLY/Z4JmvY9uc+XwNs/gVe/27EnXFNkZvU8e4MJtlZrHjEXHWuuNT3lVkWfmXEOuwveuadjz7lku+lBtu8xb1hkSkiuNBPkez8wyz/8M9SXm2vizH4KijfDM9ebUlxMknm/5r5kjkqmfMds8xNXdG3GUXstHqg+1HHZ7vfMz+m/g/TR8PFCE9An422Cl75h3tPPXzM7kpSzzbb9fSJ89ow5InlgvCmJFW82PfvXF5jzGFo8bevZ+RacfZU5QjvnahPglXvhuTmmzBeTYoK/vQ/+ZMo9y35xetseKvx+Uy5852fHH8VtfdEs7/a6++Zox3oB3gemDU9hV0kdJTWe03ti+ihIHmbKKLuXgzvdLOuOxGzIHGvC89jySXuts26yJh1fk86aYAYmu2v0jRCdZHrhfh+sfRyGTIOMUXDVX0w4/t8seGU+DJwIM/8MJVtN6IGpJ7/zMxO6RRs79vA+Wmhq7s31ppwBpqf30UI4ayZkTW57XYA1j5n1fG2Jec7bgas17FsFT84wPcgHJ8OHfzH1/Dd+YEpH31tn/k9eus0cpXz6CIyba0pUZ88wRxoFa8xO7r+WQsKgwPs/xNT5b15swvuxL5ne6EvfMqWX//yXeR1v0/Hv28E18PAF8PfxHXcqu94xM6Uyx5qxjfJdsPNNc1/JdjOraPNiM2uo8Qg8fb35LF3+G/jxXvjme2ZQ/dsfmBLiktvNEcnwy+D2NfD9z+DnJabN1Qfh03+Yde9daQbOR8wyt7/0Y9MpePRLZkd603PmPdn1tul1g9nJbXrW9No3PWPe52CpL4cdr7ftkE6kucEcmZ3qMadDa/MZW/Ow+Wy1P1oq3gKvfscs787U3Io9ZuZawbreaWs7oXM52R6YNszMI/9wVxmzJw7q+hOVMqWPj+6HKLfp6fTkyGDktVC86cTlk1aDJsMFd5w84HvCHg0T5pm699p/mlCY/ltzX3QiXPsPeO4mmHI7XP4/ZuBv+8vmWjQjrzNhXFMI8143PfAP/miOSGoPm97emJsgMcfMzvn8NROkzbVw2a9N+enFb5iacdYkM4tn/K0waBJceJcJLncarHvcrOOrL5o/qPd/Y9qXcZ7pYTtiTV3/n5eYEoTdBZf+sm0bJ8wzAZ92rumBH+vsGTB/hWnLuifMLCR3OuzbbrbVEW+2KWMUpI0wJ1mtfhDiBpqjsVX3w5cXmhOv9qwwR0ytn5P3fwsr/mBOFss/Zk6/Ix5aGszRTvsjKzClrW+8YwLXnWYGxFtFRJo2n32Vee2xc81RkCMehn7JPCZzjLl/55tw45PmMxSTbI4IPnsGvniXeV+9HvjG26Y088adZrzG5ujWRwkwO+OKPVD6udlBDhjX+XMaq8wOs3S76UyM/5qZJJA4pN16/fDKt81A/v5VpmTYUx/+GdY+aj7brZemHjzFHKm89E3TlqYa85mY9WDb88p2mYvpnTfHvM/H8lTD8zebL5Npna3Wi5Tu6gBRL5g4caJev359v71eV/n9mkvv/4CmFh9vLfgi8TH2rj/58DbzpRFw4j++01FVYOZuz34Khpzf/fX0RFUB/G0MoM189gWbOw4We5s77lyKPjNz40dcA3lLzfZf94gJhDd/aHrQO982lzy4Y4MpA/3zYhPqnmpTx5/1oAm8heeZaY1Dv2R6p99dA2nnmNd89IumvJM12QwGt4bv7vdg60tmULt96WjzYnOkcMXv4ILvde+90Lpth+zzwr6V5lB6/0dmhlGrCV83vebl95pyx/c3mp74U1fBnKfNewPmUg1v3GnCYMp3YOJtZie5ZwUc3mJu51zYvba29vJGzzYD3WfNgOsfbbu/8YgZ4B08pW3ZU1ebdn53NSwcbXactyw27+kzN5hptBffc/xrtXjA7jx++f6PTaCW7zJjOeW7zE6h1ejZZmcdn2V60IXrzKyo7GnmfW7xmNctWANX/Nasa+dSM4trxh/MwLxSsPI+WPkHcxR4aD1c83cT9O1pbY6WvB5TXjxZx8rbZKZ5vv8b890B1z4EVQfg4Wnm8hypZ5ty1a2vmMkFW16AH+ZBdILZkTxxuWkDwODzzTkj51wNkXazA3v+ZjM77dZXu/9/CyilNmitJx63XALc2FRQxY0Pf8L0URk8ePM4VFd70lrDgxPNH9CP9pjrrFjd4ltN7+bSX8GF/6/zxy+53fTkHHEmpN1p5g/jb2NNr6NsJ4y6wfxxgKndPnax+ZDfsbFtYHXVX01wRyeaHvW819peo3QHbHsZpv2g6/OpqwpMWPTF/HZPtdmuyCgzuA6mBv7AWBh7iwmN1Q+ZUogzztzv95spoq0zlXrbOz8zRwMANz0P51x56sdvfdGUms69xvx/f/1NE6YAL95mlo2ebXrOycOgYK3pxRdvhnG3mvMwWqdfbl5sesVgZr6kDDeD3+kjzc+8N01Qqghz9HN4C/gDs5rSRpijyl1vm6OH9h2hqgLTEch/x4zDDLsMXv1vE7azHjSBf+ATM8srYbDpKGx4yoyztE6Tzb4QZv7JDFa3aqozj1v9INQWm3MtbnyqrbOy6TlTNgFzDsj035ntfvSL5sJ5U74Dm543bZn5J7Mtax414e9ONzsUT7Vpz1X3ByYgdJ8EeBf8Y8Vu/vzOTv5843mnVUrZ+MajRBzezJjb/tH14D+TFW+BZT83RwInKjMcq7bEnO4/7Qem7NHq00fMgKaKDNSm29Xst75o/vjbX7+9oRLuP9f0mm56zpSSrObNu8wlENzpZnvnvd5/r91YZWat+JpNZ+JEveT2Wjzm/W6shAHj4Vvvt+3s6svN0cKB1dDQOkNLmV56YjZsfQFGXg/XPwZ5b5iSU/Y0s+NwuE/8elUHTRmp6qDZiQ2+wKz7kwdNyQRg+u/h/Ns7Ps/vh4/+agantd+0Yd4bZvvqK0yo+prMWElLAwyfbspOUTHm6G3to2aG06TbzFnL+1eZnZHXY8L9wv9nSpLt/3a1NudPVOyFr73aVkp6/DLzPn/rfdNxi8+C296DiAjT485/1/TY85cB2hxVXX0/PSUB3gU+v+aWf37K1kPVvHr7VM5Kj+30OY+v2stv3zRT5u6fM4brx2f1dTPPTO3LDa1aGs3sidyLO9YNT+Wtu82Mnu+u7ruTa/pSay/c19yz8k137V1pyiWdfbFJq7d/agY/b/wXjLr++Pu1Nmcdl+WZoyJ34MKjn/zd7OSzJpsB66xJZjZPd44stDZlhvoKGPOVkz9u70rYGDhvofUMaTBnaf/7FhPCUxd07GmD6Ris+J0JVu03M4JyLjQ7oEGTOm9b+891a687+0KzI/jm8o5jEq2OHDBnZo+6wRxp9pAEeBcVVTVy5QOrqGlsYfrIDL5+QTYDE6P5vKiGHcW1RNki+MLQJEYPjOfv7+/mgeX5zByVQUVdM58X1/DWggsZlNTPp0yfyZrrIdLR9ZOutDZ/ZFYM71ZLf2QOnW9fB6lnBbs1p1ZfbmZcTJ5/+u/5uidMeWPgeFPjbS0VnamqD5mB+q4cVZ5MiwfuP8fsJMfcYqan9gMJ8NNQXN3Iok8O8Pzag1Q3tl3GVam26aFRtgiavX5mT8jiD9eP5nCNh5kLV3F2Riz/nj8FW2QEfr/GpzX2yJPP1tx5uJbn1hxg3OBErhiZbk7nF9bWXG96ha2zQEJZSWCGSV/U9M9Uy38D6/4Jt6/t3jkX3SAB3g2NzT7e2FKEx+tn5IA4zsmIpaHZx9p9lazZW0FWYgy3TcshIsIcYr362SHuXLyJL48ZQLPXPK7J6+c3s0Zxw4SOpRWfX/P4qr38ddkuvH4/fg2uqEimj8rglsmDmTAk8Wg93dPiY93+SnJT3QxIiO7z7W72+imra2JgP7yWEJbj95nLFLQ/w7qPSYD3A601dy7exJJNRQxKiuYLOckcrGxg7b5KvjJxEPfOGkmNp4XVeyp45tMDrNt/hOkj0/nttaPZW1bHK58d4s0txdQ2eRmRGcfsiVnkFdeydFsxtR4vEQouPTeduVOGEBMVyfZD1eworqXZ5yfWaSPWaSMzPppzMmI5KyOWOOfxtbfqhhZqPC1kxjuxBY4MvD4/Byob+OxgFct3lLAqv5y6Ji9XjEjnZ1edy5Dk7vWuvD4/H+wqw+WwMSk7iciIzgd4m71+qhtbSHFH9XhAuLK+mYRo+9EdbH8oq21iS2EVFfXNXH5uOomuk8zn7yM1nhZ8Pt3vryv6lgR4P/H6/FQ2NJMW6zx6e+F7+Ty4YjexThu1HjN1KskVxc+vOpfrxg3sEFT1TV5e3XSIp1cfIO9w7dFe+YyRGWwqqGLxugIq6tsuipTsiiLGEUmtx0utx4vP3/b/meKOIi3WSVqcA69Ps6ukltJacyZhZIRiYEI0MVGR7C2vp9lrTvFOj3NwyTnppLijeOKjfXh9mlvPH8L4wYlkxDuJj7aTX1LL5sJq8g7X4IqykR7nJD3OQVZiDEOSY0iLc7B0SzFPfLyPgsrGo+28fEQ6LofNjCccriE+2s7147K4YcJAIiMUz605yPNrD1Je10xMVCSDk2LISXGRm+pmeLqb4WmxDE93dyhJldc1sae0jgEJ0QxMiEYD7+eV8uRH+1i9t4L0OAfTR2YwfWQGw9PdJLscR3ckfr+mocWHKyryuJ1FY7MPpcylFjpzqKqRpz7ex9KthzlU1XYdlajICGaMyuArkwbxhZykozvMVi0+/ynLa53xtPjYXlTDpoIqthRWsbWwmr3l9QAMSopm7KBExmTFM3ZQAiMHxBMdZeFxhVOobmxha2E16XEOslNcXXpPi6oaKattwuvX+LU+OlYZoaCpxU+Nx0uNp4XqhhYq6ps5Ut+Mx+sjJiqSaLuN9DgHV4zMICelrXPT0OzlYGUDqW4HSa6ed0DakwAPspU7S3lp4yFGDohjam4KIwbEnbJHqrVmT1kdAxNiOvzhNXl9rMgrwx6pGDkgnvQ4x9EPitaaomoPecU15B2upfBIA6U1TZTWNqEUDE+L5ax0N3HRdgoqGzhY2UBdk5fhaW7OzohjRGYc52bGHl1faY2HP72zkxc3FB7XPnukYlhaLE0tPg7XeGhoPv5aDxOGJPKtC4eiteatbYd5P68Ur98feK1YDlQ08MmeisAfjsKvNZecncYFw1I4dKSRAxX17C2v50BFPa37JYctgpED4hiYGMPWwir2VzQcfT2HLYJYp43yumYGxDu5YUIWu0pq+WBXGZ4Ws4OKUGbn2eLT1Hpa8Guzo5s6LIVpw1Kobmxhxc5S1u07AgomDklk6rAUMuKcFBxpoPBII40tPpJiokhyRbG3vJ6lW4sBuPScNCZlJ3FeVjwxUTZe2ljIyxsLqfF4iXPauHB4KuOHJLK7tJYNB46QX1pHQrSdIckuBiXF4PP7qfV4qW/yMizNzQW5KZyfm0xiTBTNPv/RwF69p4LVe8rZXlSDN/DGpMc5OC8rgTFZ8dgiI9hcUMWmgiqKq82JNJERiuFpbs4N/B/npLhRgE9rmrx+Sqo9FFd7qKxvIjoqErfDRny0nYGJ0QxOimFgQgzNXj9Vjc1U1jezt6yeXSW17Cmrw68hJioSpz2SFHcUmfHRZMY7SY11kOxykOSOoqHJy77yevZX1FPf5MPliMTlsBHntJPsjiLFbabp7SmtY3dpHfsq6imqaqSoykNlfTMRCuyRETjtkQxMiGZwcgwJ0XbWHzjClsKqo58Pe6RiaIqbqcNSuHxEOpOyE4lQipJaD7tL6/hodzkr8krZVVLX6d9s+896kisKpz2ShmYfjc0+6ppMR+ycjFjGDU5g26EaPi+uOdqBctgiGJgQTUa8k4x4J5nxTq4fn0Vu6kmmWHZCAlx0W3VDC0XVjRyu8VBZ10xumptzM2Nx2Np2LDWeFgoqGyioNCE3bnACE4Z0HO33+vwopTrsuAoqG3jls0M0e/18ZdKgE87gafL62F/eQN7hGrYUVrOlsIqiKg8jB8QxYUgiZ2XEcrjaw57SOkprmwI97vSjPd7GZh+f7q2g8EgDZbVNlNU1ExWpiIu2ExNlI+9wDR/vLqe8zhzZDE9zc9HZqWgNH+0uJ+9w7dG2pMc5iImyUVnfTHVjC26HjZsnD+LrU3NOOGbgafGxIq+UFTtLWbmzjNLaJuKcNsYPSWTUgHiONDRzoKKBgiMN2CIUsU47TnsEO4prOwygt2ePVIwdlMCk7CTGDkpgzKAE0uNOPOe7tMbD5sB7tvVQNTuKayipOcH1XDAhnOJ20Njio87jpbHl1BdgSoixMzzNHBE1tvhoaPJRXtfU4QixuzLinGQlRjMgIZpkdxRamyOWxmYfhUcaOVBZT0VdM6Oz4rlweCqTshMpr2tiV0kd24tq+HRvBc1eU1r0+vTRbbFFKCbnJHHJOWnkpLiIjDCfR4VCY3ri9sgI4qPtxDptxMfYiXXYjutNF1U18ta2wyzdWszOw7WMGhjHxCFJnJURS0VdE0VVjRyqauRwtYfD1R5Kapt4+huTuWBYN77+EQlwIU7J79fsKq3F7bCRldhxJ1JW20Stp4UBCdEdSiotPtOr72oZRGtNSU0TabGOTuvyPr9mR3ENa/ZV4mnxERUZQZQtgpwUFxOzE3s0W6myvpnCIw0oFEqZGVXpcU7inB2DytPi41BVIwcrGjhU1YjTHklCtJ2EGDuDk2NIdTtOWCbwtPgoqfGYMK8zPXanPZLsFBfZyTHEOu00tviob/JS3dhCRV0zFfVN+Pya3FQ3OSkuXI7Ot09rfdIyRX2Tlw93lfFhfhnRdhs5qS6GprgYnRV/wrGhvtbaM+/KONCJSIALIYRFnSzA5XKyQghhURLgQghhURLgQghhURLgQghhURLgQghhURLgQghhURLgQghhURLgQghhUf16Io9Sqgw40M2npwDlnT4q9ITjdofjNkN4bnc4bjOc/nYP0Vof97X2/RrgPaGUWn+iM5FCXThudzhuM4TndofjNkPvbbeUUIQQwqIkwIUQwqKsFOCPBbsBQRKO2x2O2wzhud3huM3QS9ttmRq4EEKIjqzUAxdCCNGOBLgQQliUJQJcKTVDKbVTKbVbKXV3sNvTF5RSg5RSK5RSO5RS25VSCwLLk5RS7yql8gM/E4Pd1t6mlIpUSn2mlHojcDsctjlBKfWiUiov8H9+fqhvt1LqB4HP9jal1PNKKWcobrNS6kmlVKlSalu7ZSfdTqXUPYFs26mUmn46r3XGB7hSKhL4BzATGAHcrJQaEdxW9Qkv8EOt9bnAFOD2wHbeDSzXWg8Hlgduh5oFwI52t8Nhm/8GvK21PgcYg9n+kN1updRA4PvARK31KCASuInQ3OangBnHLDvhdgb+xm8CRgae81Ag87rkjA9wYDKwW2u9V2vdDPwbmBXkNvU6rXWx1npj4PdazB/0QMy2Lgo8bBFwbVAa2EeUUlnAVcDj7RaH+jbHAV8EngDQWjdrrasI8e0GbEC0UsoGxABFhOA2a60/BCqPWXyy7ZwF/Ftr3aS13gfsxmRel1ghwAcCBe1uFwaWhSylVDYwDlgDpGuti8GEPJAWxKb1hYXAjwF/u2Whvs1DgTLgX4HS0eNKKRchvN1a60PAX4CDQDFQrbVeRghv8zFOtp09yjcrBPiJvsY5ZOc+KqXcwEvAnVrrmmC3py8ppa4GSrXWG4Ldln5mA8YDD2utxwH1hEbp4KQCNd9ZQA4wAHAppeYGt1VnhB7lmxUCvBAY1O52FubQK+QopeyY8H5Wa/1yYHGJUiozcH8mUBqs9vWBqcA1Sqn9mNLYJUqpZwjtbQbzmS7UWq8J3H4RE+ihvN2XAfu01mVa6xbgZeACQnub2zvZdvYo36wQ4OuA4UqpHKVUFKbg/1qQ29TrlFIKUxPdobW+v91drwHzAr/PA5b0d9v6itb6Hq11ltY6G/P/+r7Wei4hvM0AWuvDQIFS6uzAokuBzwnt7T4ITFFKxQQ+65dixnlCeZvbO9l2vgbcpJRyKKVygOHA2i6vVWt9xv8DrgR2AXuAnwW7PX20jdMwh05bgE2Bf1cCyZhR6/zAz6Rgt7WPtv8i4I3A7yG/zcBYYH3g//tVIDHUtxu4F8gDtgFPA45Q3GbgeUydvwXTw77tVNsJ/CyQbTuBmafzWnIqvRBCWJQVSihCCCFOQAJcCCEsSgJcCCEsSgJcCCEsSgJcCCEsSgJcCCEsSgJcCCEs6v8DHbH5fSjTP04AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_forecast_error = pd.DataFrame(\n",
    "        columns=['h', 'mae', 'rmse', 'mape', 'descriptions'])\n",
    "history = model.fit(train_X, train_y, epochs=100, batch_size=100, validation_data=(test_X, test_y), verbose=0, shuffle=False)\n",
    "# # plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(793, 1)"
      ]
     },
     "execution_count": 1058,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict(test_X)\n",
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU_scale_RFE_Lasso_50,6.463209960764323,8.001760147475094,0.11101400859625515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [6.463209960764323],\n",
       " 'rmse': [8.001760147475094],\n",
       " 'mape': [0.11101400859625515],\n",
       " 'r2': [0.3181114933660384],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 1059,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_y=pred_y[:,0,0,0]\n",
    "inverted_pred_y = y_scaler.inverse_transform(pred_y.reshape(test_y.shape))\n",
    "inverted_test_y = y_scaler.inverse_transform(test_y)\n",
    "# original_test_price=df_original_price.to_numpy()[train_size:]\n",
    "# inverted_pred_y=pred_y\n",
    "# inverted_test_y=test_y\n",
    "forecast_error=evaluate_series(inverted_test_y, inverted_pred_y, h)\n",
    "print(f\"GRU_{feature_name},{forecast_error['mae'][0]},{forecast_error['rmse'][0]},{forecast_error['mape'][0]}\")\n",
    "forecast_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': 1, 'mae': [3.3330300012422542], 'rmse': [4.488167371111368], 'mape': [0.05918497706973728], 'r2': [0.7854737703270569], 'descriptions': ''}\n",
      "\n",
      "Forecast Error (1-fold cross-validation)\n",
      "X: (1982, 30)\n",
      "y: (1982, 1)\n",
      "h= 1\n",
      "Model: Functional\n",
      "MAE = 3.333030 +/- nan\n",
      "RMSE = 4.488167 +/- nan\n",
      "MAPE = 0.059185 +/- nan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(forecast_error)\n",
    "df_forecast_error = df_forecast_error.append(\n",
    "    pd.DataFrame(forecast_error), ignore_index=True)\n",
    "mae = df_forecast_error[\"mae\"]\n",
    "rmse = df_forecast_error[\"rmse\"]\n",
    "mape = df_forecast_error[\"mape\"]\n",
    "k = 1\n",
    "msg = f\"\"\"\n",
    "Forecast Error ({k}-fold cross-validation)\n",
    "X: {X.shape}\n",
    "y: {y.shape}\n",
    "h= {h}\n",
    "Model: {model.__class__.__name__}\n",
    "MAE = {mae.mean():.6f} +/- {mae.std():.3f}\n",
    "RMSE = {rmse.mean():.6f} +/- {rmse.std():.3f}\n",
    "MAPE = {mape.mean():.6f} +/- {mape.std():.3f}\n",
    "\"\"\"\n",
    "print(msg)\n",
    "logging.info(msg)\n",
    "evaluation_result = {\n",
    "'h': h,\n",
    "'mae': [mae.mean()],\n",
    "'rmse': [rmse.mean()],\n",
    "'mape': [mape.mean()],\n",
    "'descriptions': [msg]\n",
    "}\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_series(inverted_test_y, inverted_pred_y, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import gzip\n",
    "\n",
    "# with gzip.GzipFile('./trained_models/model.pgz', 'w') as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gzip.open('./trained_models/model.pgz', 'r') as f:\n",
    "#     trained_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_y = trained_model.predict([test_emb, test_X])\n",
    "# inverted_pred_y = y_scaler.inverse_transform(pred_y.reshape(-1, 1))\n",
    "# inverted_test_y = y_scaler.inverse_transform(test_y)  # should be same as testXy\n",
    "# inverted_pred_y=inverted_pred_y.ravel()+df_original_price.Price.to_numpy()[train_size:]\n",
    "# inverted_test_y=inverted_test_y.ravel()+df_original_price.Price.to_numpy()[train_size:]\n",
    "# forecast_error = evaluate_series(inverted_test_y, inverted_pred_y, h)\n",
    "# forecast_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export architecture as json \n",
    "# model.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare models\n",
    "\n",
    "ts_size=X.shape[-1]\n",
    "\n",
    "ts_inputs = Input(shape=(1,ts_size))\n",
    "ts_model = Bidirectional(GRU(300,dropout=0.1,return_sequences=False))(ts_inputs)\n",
    "# ts_model=Reshape((1,1,1,30))(ts_inputs)\n",
    "# ts_model=ConvLSTM2D(1000,(1,1))(ts_model)\n",
    "ts_model= Dropout(0.2)(ts_model)\n",
    "# ts_model=Reshape((10,100))(ts_model)\n",
    "# ts_model = Bidirectional(GRU(200,dropout=0.1,return_sequences=True))(ts_model)\n",
    "# ts_model = Bidirectional(GRU(100,dropout=0.1,return_sequences=False))(ts_model)\n",
    "ts_model = Dense(1)(ts_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size=X.shape[-1]\n",
    "emb_inputs = Input(shape=(10,emb_size))\n",
    "emb_model = Reshape((1,emb_size,10))(emb_inputs)\n",
    "emb_model = Conv2D(300,(1, 50), activation=\"relu\",padding='valid')(emb_model)\n",
    "# emb_model = MaxPooling2D(pool_size=(1, 20))(emb_model)\n",
    "emb_model = Dropout(0.2)(emb_model)\n",
    "# emb_model = Conv2D(100,(1, 50), activation=\"relu\",padding='valid')(emb_model)\n",
    "# emb_model = MaxPooling2D(pool_size=(1, 10))(emb_model)\n",
    "# emb_model = Dropout(0.2)(emb_model)\n",
    "# emb_model = Conv2D(1,(1, 20), activation=\"relu\")(emb_model)\n",
    "emb_model = Reshape((1,-1))(emb_model)\n",
    "emb_model = GRU(300,dropout=0.2)(emb_model)\n",
    "emb_model = Dropout(0.2)(emb_model)\n",
    "emb_model = Dense(1)(emb_model)\n",
    "model = Model(inputs=emb_inputs, outputs=emb_model)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:        \n",
    "    cv = get_TS_cv()\n",
    "    df_forecast_error = pd.DataFrame(\n",
    "        columns=['h', 'mae', 'rmse', 'mape', 'descriptions'])\n",
    "    for train_idx, test_idx in cv.split(y):\n",
    "        # split train/test set for emb,X,y      \n",
    "        train_X = X[train_idx,:]\n",
    "        test_X = X[test_idx,:]\n",
    "        train_y = y[train_idx]\n",
    "        test_y = y[test_idx]\n",
    "\n",
    "        # normalize features\n",
    "        # X_scaler = MinMaxScaler(feature_range=(0.1, 1))\n",
    "        # X_scaler.fit(train_X)\n",
    "        y_scaler = MinMaxScaler(feature_range=(0.1, 1))\n",
    "        y_scaler.fit(train_y)\n",
    "\n",
    "        # train_X=X_scaler.transform(train_X)\n",
    "        # test_X=X_scaler.transform(test_X)\n",
    "        train_y=y_scaler.transform(train_y)\n",
    "        test_y=y_scaler.transform(test_y)\n",
    "\n",
    "        # reshape to 3D for RNN/LSTM/GRU       \n",
    "        # train_X=train_X.reshape(train_X.shape[0],1,train_X.shape[-1])\n",
    "        # test_X=test_X.reshape(test_X.shape[0],1,test_X.shape[-1])\n",
    "        print(f\"train_X: {train_X.shape} test_X:{test_X.shape}\")\n",
    "        print(f\"train_y: {train_y.shape} test_y:{test_y.shape}\")\n",
    "        # model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "        # model.compile(loss='mse', optimizer='adam')\n",
    "        model = Model(inputs=emb_inputs, outputs=emb_model)\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        history = model.fit(train_X, train_y, epochs=80, batch_size=100, validation_data=(test_X, test_y), verbose=0, shuffle=False)\n",
    "        \n",
    "        # plot history\n",
    "        pyplot.plot(history.history['loss'], label='train')\n",
    "        pyplot.plot(history.history['val_loss'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n",
    "        pred_y = model.predict(test_X)\n",
    "        print(f\"pred_y: {pred_y.shape}\")\n",
    "\n",
    "        # scale\n",
    "        inverted_pred_y = y_scaler.inverse_transform(pred_y.reshape(test_y.shape))\n",
    "        inverted_test_y = y_scaler.inverse_transform(test_y)\n",
    "        # original_test_price=df_original_price.to_numpy()[test_idx]\n",
    "        # inverted_pred_y=inverted_pred_y+original_test_price\n",
    "        # inverted_test_y=inverted_test_y+original_test_price\n",
    "\n",
    "\n",
    "\n",
    "        # inverted_pred_y = y_scaler.inverse_transform(pred_y.reshape(-1, 1))\n",
    "        # inverted_pred_y=inverted_pred_y.ravel()+df_original_price.Price1.to_numpy()[test_idx]\n",
    "        # inverted_test_y = y_scaler.inverse_transform(test_y)  # should be same as testXy\n",
    "        # inverted_test_y=inverted_test_y.ravel()+df_original_price.Price1.to_numpy()[test_idx]\n",
    "        \n",
    "        # no scale\n",
    "        # inverted_pred_y=pred_y.ravel()+df_original_price.Price.to_numpy()[test_idx]\n",
    "        # inverted_test_y=test_y.ravel()+df_original_price.Price.to_numpy()[test_idx]\n",
    "        \n",
    "        forecast_error = evaluate_series(inverted_test_y, inverted_pred_y, h)\n",
    "        # forecast_error = evaluate_series(test_y.reshape(-1, 1), pred_y.reshape(-1, 1), h)\n",
    "        print(forecast_error)\n",
    "        df_forecast_error = df_forecast_error.append(\n",
    "            pd.DataFrame(forecast_error), ignore_index=True)\n",
    "    mae = df_forecast_error[\"mae\"]\n",
    "    rmse = df_forecast_error[\"rmse\"]\n",
    "    mape = df_forecast_error[\"mape\"]\n",
    "    k = cv.get_n_splits()\n",
    "    msg = f\"\"\"\n",
    "    Forecast Error ({k}-fold cross-validation)\n",
    "    X: {X.shape}\n",
    "    y: {y.shape}\n",
    "    h= {h}\n",
    "    Model: {model.__class__.__name__}\n",
    "    MAE = {mae.mean():.6f} +/- {mae.std():.3f}\n",
    "    RMSE = {rmse.mean():.6f} +/- {rmse.std():.3f}\n",
    "    MAPE = {mape.mean():.6f} +/- {mape.std():.3f}\n",
    "    \"\"\"\n",
    "    print(msg)\n",
    "    logging.info(msg)\n",
    "    evaluation_result = {\n",
    "        'h': h,\n",
    "        'mae': [mae.mean()],\n",
    "        'rmse': [rmse.mean()],\n",
    "        'mape': [mape.mean()],\n",
    "        'descriptions': [msg]\n",
    "    }    \n",
    "except Exception as e:\n",
    "    logging.exception(\"EXCEPTION: %s\", e, exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log=\"BiGRU;\"\n",
    "FE_log=\"CLC1(h=1); 1stDIFF(xy)+ RFE(Ridge,60); scale y;\"\n",
    "msg=f\"{model_log} {FE_log} mse;\"\n",
    "evaluation_result[\"descriptions\"]=msg\n",
    "df_result = pd.DataFrame(evaluation_result)\n",
    "df_result[\"time\"] = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "df_result = df_result[['time', 'descriptions', 'h', 'mae', 'rmse', 'mape']]\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result.to_csv(f\"{HOME}/results/experiment_results.csv\",mode=\"a+\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f3e05a59671f1eb5b3f5f0e003aaa5a39f5d3316373e39c3606e56079185283"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('OPP-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
