{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, MaxPool3D, Bidirectional, ConvLSTM2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import pymongo\n",
    "import random\n",
    "import string\n",
    "import fasttext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "HOME = os.environ['LIMA_HOME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> list:\n",
    "    \"\"\"\n",
    "    clean text with tokenization; stemming; removing stop word, punctuation, number, and empty string.\n",
    "\n",
    "    Args:\n",
    "        text (str): text\n",
    "\n",
    "    Returns:\n",
    "        list: cleaned text as list of tokenized str\n",
    "    \"\"\"\n",
    "\n",
    "    # to list of token\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    # stemming and convert to lower case if not proper noun: punctuation and stop word seem to help POS tagging, remove them after stemming\n",
    "    word_tag = pos_tag(text)\n",
    "    porter = PorterStemmer()\n",
    "    text = [\n",
    "        porter.stem(each[0])\n",
    "        if each[1] != \"NNP\" and each[1] != \"NNPS\" else each[0]\n",
    "        for each in word_tag\n",
    "    ]\n",
    "\n",
    "    # remove stop word: it seems stemming skip stop word; OK to remove stop word after stemming;\n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    text = [each for each in text if not each in stop_word]\n",
    "\n",
    "    # remove punctuation\n",
    "    text = [\n",
    "        each.translate(str.maketrans('', '', string.punctuation))\n",
    "        for each in text\n",
    "    ]\n",
    "    # text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", text) # if using re\n",
    "\n",
    "    # convert number to <NUM>\n",
    "    text = [\"<NUM>\" if each.isdigit() else each for each in text]\n",
    "\n",
    "    # remove empty string\n",
    "    text = [each for each in text if each != \"\"]\n",
    "\n",
    "    return text\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = data.copy()\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "        \n",
    "\t\tnames += [f'{data.columns[j]}(t-{i})' for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [f'{data.columns[j]}(t)' for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [f'{data.columns[j]}(t+{i})' for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def get_TS_cv(k=5, test_size=None):\n",
    "    \"\"\"\n",
    "    ML models do not need to care about forecast horizon when splitting training and test set. Forecast horizon should be handled by feature preparation ([X_t-1,X_t-2...]). Actually repeated K-fold can also be used, but stick to TS split to align with TS_evaluate().\n",
    "    \"\"\"\n",
    "    return TimeSeriesSplit(\n",
    "        n_splits=k,\n",
    "        gap=0,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "\n",
    "def evaluate_series(y_true, y_pred, horizon):\n",
    "    \"\"\"\n",
    "    Some models (like ARIMA) may not support cross_validate(), compare the forecasting result directly\n",
    "    Args:\n",
    "        y_true: y of test set\n",
    "        y_pred: y of prediction\n",
    "        horizon: forecast horizon\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: single row DF with 3 metrics wrt horizon\n",
    "    \"\"\"\n",
    "    # RMSE\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2=r2_score(y_true, y_pred)\n",
    "    forecast_error = {\n",
    "        'h': horizon,\n",
    "        'mae': [mae],\n",
    "        'rmse': [rmse],\n",
    "        'mape': [mape],\n",
    "        'r2':[r2],\n",
    "        'descriptions': \"\"\n",
    "    }\n",
    "    return forecast_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "h = 1\n",
    "past = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient\n",
    "client= InfluxDBClient.from_config_file(f\"{HOME}/dev/DB/influxdb_config.ini\")\n",
    "query_api = client.query_api()\n",
    "df_WTI = query_api.query_data_frame(\"\"\"\n",
    "from(bucket: \"dummy\")\n",
    "  |> range(start: 2011-04-01, stop: 2019-04-01)\n",
    "  |> filter(fn: (r) => r[\"_measurement\"] == \"WTI\") \n",
    "  |> filter(fn: (r) => r[\"type\"] == \"closing_price\") \n",
    "  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "  |> drop(columns: [\"_start\", \"_stop\"])\n",
    "\"\"\")\n",
    "df_WTI=df_WTI[[\"_time\",\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]]\n",
    "df_WTI.columns=[\"Date\",\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]\n",
    "df_WTI.set_index(\"Date\",inplace=True)\n",
    "df_WTI.index=df_WTI.index.map(lambda each: each.date())\n",
    "df_WTI.index=pd.to_datetime(df_WTI.index)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=[each.month for each in df_WTI.index]\n",
    "day=[each.day for each in df_WTI.index]\n",
    "day_in_week=[each.weekday() for each in df_WTI.index]\n",
    "df_dt=pd.DataFrame()\n",
    "df_dt[\"month\"]=month\n",
    "df_dt[\"day\"]=day\n",
    "df_dt[\"day_in_week\"]=day_in_week\n",
    "df_dt.index=df_WTI.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.load_model(f\"{HOME}/data/big/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment=pd.read_pickle(\"df_sentiment_2type.pkl\")\n",
    "df_sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic=pd.read_pickle(\"df_topic_2type.pkl\")\n",
    "df_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geoidx=pd.read_pickle(\"df_geoidx_2type.pkl\")\n",
    "df_geoidx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy = pd.concat([df_sentiment,df_topic, df_geoidx,df_WTI], axis=1, join=\"inner\")\n",
    "print(df_Xy.shape)\n",
    "df_Xy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shifted = series_to_supervised(df_Xy.dropna(), past, h)\n",
    "# remove current day features for forecast\n",
    "for each in df_shifted.columns[:-1]:\n",
    "    if \"(t)\" in each:\n",
    "        df_shifted.drop(each, axis=1, inplace=True)\n",
    "# add time feature without shift \n",
    "df_shifted=pd.concat([df_dt,df_shifted],axis=1).dropna()\n",
    "raw_X = df_shifted.to_numpy()[:, :-1]\n",
    "y =  df_shifted.to_numpy()[:, -1].reshape(-1, 1) \n",
    "# y = df_Xy[df_Xy.index.isin(df_selected.index)].to_numpy()[:, -1].reshape(-1, 1)\n",
    "# y=df_WTI[df_WTI.index.isin(df_selected.index)][\"CLC1\"].to_numpy().reshape(-1, 1)\n",
    "f\"{raw_X.shape}   |{y.shape} | \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression,RFE,RFECV,SelectFromModel,SequentialFeatureSelector,chi2,SelectKBest,f_regression,VarianceThreshold,r_regression\n",
    "from sklearn.linear_model import Ridge,Lasso,LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor\n",
    "from sklearn.svm import LinearSVR,SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = Lasso(random_state=42)\n",
    "# estimator = Ridge(random_state=42)\n",
    "estimator = LinearRegression( )\n",
    "# selector = RFECV(estimator,cv=get_TS_cv(),step=1)\n",
    "# selector = RFE(estimator,n_features_to_select=20,step=1)\n",
    "selector = SequentialFeatureSelector(estimator,n_features_to_select=20,cv=get_TS_cv())\n",
    "# selector=SelectFromModel(estimator,max_features=20)\n",
    "scaled_raw_X=MinMaxScaler().fit_transform(raw_X)\n",
    "selector = selector.fit(raw_X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_X[:, selector.get_support()]\n",
    "print(f\"{X.shape} | {y.shape}\")\n",
    "df_shifted.columns[:-1][selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, ARDRegression, SGDRegressor, ElasticNet, Lars, Lasso, GammaRegressor, TweedieRegressor, PoissonRegressor, Ridge, BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor\n",
    "from keras.layers import Reshape,MaxPooling2D,Bidirectional,ConvLSTM2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN \n",
    "from keras.layers import Conv2D,Conv3D,Conv1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.keras.backend.clear_session()\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=X.shape[0]\n",
    "train_size=int(length*0.7)\n",
    "step_size=1\n",
    "\n",
    "train_X=X[:train_size]\n",
    "train_y=y[:train_size,:]\n",
    "\n",
    "test_X=X[train_size:]\n",
    "test_y=y[train_size:,:]\n",
    "\n",
    "# X_scaler = MinMaxScaler()\n",
    "# X_scaler.fit(train_X)\n",
    "# train_X=X_scaler.transform(train_X)\n",
    "# test_X=X_scaler.transform(test_X)\n",
    "\n",
    "# y_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "# y_scaler.fit(train_y)\n",
    "# train_y=y_scaler.transform(train_y)\n",
    "# test_y=y_scaler.transform(test_y)\n",
    "\n",
    "# train_X=train_X.reshape(train_X.shape[0],step_size,train_X.shape[-1])\n",
    "# test_X=test_X.reshape(test_X.shape[0],step_size,test_X.shape[-1])\n",
    "print(f\"train_X: {train_X.shape}\\t   \\t test_X:{test_X.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\\t   test_y:{test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_X=X_scaler.transform(X)\n",
    "# scaled_y=y_scaler.transform(y)\n",
    "# lin_model=Lasso(random_state=42)\n",
    "# lin_model=Ridge(random_state=42)\n",
    "# lin_model=LinearSVR(random_state=42)\n",
    "# lin_model= SVR()\n",
    "lin_model=LinearRegression()\n",
    "lin_model.fit(train_X,train_y.ravel())\n",
    "linear_y=lin_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_series(y,linear_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linear_y=y-linear_y.reshape(y.shape)\n",
    "non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shifted.index[-245:-199]\n",
    "# non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "plt.plot(df_shifted.index[-250:-230],non_linear_y[-250:-230],'ob') \n",
    "# plt.plot(non_linear_y[290:295],'ob') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_shifted.index[-245:-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "stl = STL(non_linear_y,10)\n",
    "res = stl.fit()\n",
    "non_linear_trend=res.trend.reshape(-1,1)\n",
    "non_linear_season=res.seasonal.reshape(-1,1)\n",
    "non_linear_residual=res.resid.reshape(-1,1)\n",
    "# decomposed_y=np.array([res.trend,res.seasonal,res.resid]).transpose()\n",
    "# non_linear_y=decomposed_y\n",
    "# fig = res.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationary test before diff\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "for i in range(X.shape[1]):\n",
    "    test_result=adfuller(X[:,i])\n",
    "    if test_result[1]>0.05:\n",
    "        print(f\"X[{i}]: {test_result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result=adfuller(non_linear_trend.ravel())\n",
    "if test_result[1]>0.05:\n",
    "    print(f\"non_linear_trend: {test_result[1]}\")\n",
    "test_result=adfuller(non_linear_season.ravel())\n",
    "if test_result[1]>0.05:\n",
    "    print(f\"non_linear_season: {test_result[1]}\")\n",
    "test_result=adfuller(non_linear_residual.ravel())\n",
    "if test_result[1]>0.05:\n",
    "    print(f\"non_linear_residual: {test_result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.diff(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all stationary\n",
    "non_linear_y=np.concatenate([non_linear_trend,non_linear_season,non_linear_residual],axis=1)\n",
    "\n",
    "# else\n",
    "# original_non_linear_y=non_linear_y[1:]\n",
    "# diff_non_linear_y=np.diff(non_linear_y,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y[1:]\n",
    "linear_y=linear_y[1:]\n",
    "non_linear_y=non_linear_y[1:]\n",
    "f\"{X.shape}|{y.shape}|{linear_y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationary test after diff\n",
    "for i in range(X.shape[1]):\n",
    "    test_result=adfuller(X[:,i])\n",
    "    if test_result[1]>0.05:\n",
    "        print(f\"X[{i}]: {test_result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=X[:train_size]\n",
    "test_X=X[train_size:]\n",
    "X_scaler = MinMaxScaler()\n",
    "X_scaler.fit(train_X)\n",
    "train_X=X_scaler.transform(train_X)\n",
    "test_X=X_scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_non_linear_y=non_linear_y[:train_size,:]\n",
    "test_non_linear_y=non_linear_y[train_size:,:]\n",
    "print(f\"{train_X.shape}|{train_non_linear_y.shape}\")\n",
    "print(f\"{test_X.shape}|{test_non_linear_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linear_y_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "non_linear_y_scaler.fit(train_non_linear_y)\n",
    "train_non_linear_y=non_linear_y_scaler.transform(train_non_linear_y)\n",
    "test_non_linear_y=non_linear_y_scaler.transform(test_non_linear_y)\n",
    "f\"{train_non_linear_y.shape}|{test_non_linear_y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_out=train_non_linear_y.shape[-1]\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_inputs = Input(shape=(train_X.shape[-1],))\n",
    "# ts_model=Reshape((1,20,1))(ts_inputs)\n",
    "# # ts_model=Reshape((step_size,train_X.shape[-1]))(ts_inputs)\n",
    "# # ts_model=Reshape((1,1,1,20))(ts_model)\n",
    "# # ts_model=ConvLSTM2D(1,(1,1),return_sequences=False)(ts_model)\n",
    "# # ts_model= Dropout(0.2)(ts_model)\n",
    "# # ts_model=ConvLSTM2D(50,(1,3),return_sequences=False)(ts_model)\n",
    "\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# # ts_model=Conv2D(1,(1,3) )(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# # ts_model=Conv2D(1,(1,3) )(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# ts_model=Dense(num_out)(ts_model)\n",
    "# # ts_model=Conv2D(num_out,(1,1))(ts_model)\n",
    "# ts_model=Reshape((num_out,20))(ts_model)\n",
    "# # ts_model=Bidirectional(GRU(50,dropout=0.2 ,return_sequences=True))(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# # ts_model=Bidirectional(LSTM(50,dropout=0.2 ,return_sequences=True))(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# ts_model=Bidirectional(LSTM(50,dropout=0.2 ,return_sequences=False),merge_mode='mul')(ts_model)\n",
    "# ts_model= Dropout(0.4)(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# # ts_model=Bidirectional(LSTM(500,dropout=0.1  ,return_sequences=True))(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# # ts_model=Bidirectional(LSTM(300,dropout=0.1  ,return_sequences=True))(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# # # ts_model=Bidirectional(GRU(300,dropout=0.2 ,return_sequences=True))(ts_model)\n",
    "# # ts_model=Bidirectional(LSTM(100,dropout=0.1  ,return_sequences=False))(ts_model)\n",
    "\n",
    "# # ts_model =Dense(50)(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "# # ts_model =Dense(50)(ts_model)\n",
    "# # ts_model= Dropout(0.4)(ts_model)\n",
    "\n",
    "# ts_model= Flatten()(ts_model)\n",
    "# ts_model =Dense(num_out)(ts_model)\n",
    "# ts_model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "# # ts_model.compile(loss='mae', optimizer=Adam())\n",
    "# # ts_model.compile(loss='log_cosh', optimizer=Adam(0.0002))\n",
    "# ts_model.summary()\n",
    "# model=ts_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 1, 20, 1)          0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1, 20, 3)          6         \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 3, 20)             0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 100)              21600     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,909\n",
      "Trainable params: 21,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 3)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_inputs = Input(shape=(train_X.shape[-1],))\n",
    "ts_model=Reshape((1,20,1))(ts_inputs)\n",
    "ts_model=Dense(num_out)(ts_model)\n",
    "ts_model=Reshape((num_out,20))(ts_model)\n",
    "# ts_model=Bidirectional(LSTM(50,dropout=0.2 ,return_sequences=False),merge_mode='mul')(ts_model)\n",
    "ts_model= Bidirectional(GRU(50,dropout=0.2 ,return_sequences=False))(ts_model)\n",
    "ts_model= Dropout(0.4)(ts_model)\n",
    "ts_model= Flatten()(ts_model)\n",
    "ts_model =Dense(num_out)(ts_model)\n",
    "ts_model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "ts_model.summary()\n",
    "ts_model.compile(loss='log_cosh', optimizer=Adam(0.0002))\n",
    "ts_model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 56.5319 - val_loss: 57.9543\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 56.3547 - val_loss: 57.7766\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 56.1746 - val_loss: 57.5952\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 55.9780 - val_loss: 57.3941\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 55.7710 - val_loss: 57.1705\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 55.5441 - val_loss: 56.9105\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 55.2447 - val_loss: 56.6040\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 54.9025 - val_loss: 56.2367\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 54.5079 - val_loss: 55.7921\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 54.0262 - val_loss: 55.2502\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 53.4173 - val_loss: 54.5907\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 52.7440 - val_loss: 53.7952\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 51.8556 - val_loss: 52.8433\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 50.7513 - val_loss: 51.7067\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 49.5743 - val_loss: 50.3850\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 48.1634 - val_loss: 48.8596\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 46.5956 - val_loss: 47.1102\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 44.6136 - val_loss: 45.0985\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 42.5639 - val_loss: 42.8372\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 40.2401 - val_loss: 40.2907\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 37.8248 - val_loss: 37.4851\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 34.9553 - val_loss: 34.4162\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 31.6351 - val_loss: 31.0847\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 28.7176 - val_loss: 27.6896\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 26.3158 - val_loss: 24.4587\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 24.0292 - val_loss: 21.5691\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 21.7931 - val_loss: 19.0720\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 20.3156 - val_loss: 17.0108\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 19.5629 - val_loss: 15.3372\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 18.9969 - val_loss: 14.0769\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 18.4526 - val_loss: 13.0330\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 17.8533 - val_loss: 12.1707\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 17.4836 - val_loss: 11.5630\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 17.4943 - val_loss: 11.0789\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.5631 - val_loss: 10.6430\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.9920 - val_loss: 10.1852\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.5339 - val_loss: 9.7908\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.6056 - val_loss: 9.5506\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.3027 - val_loss: 9.3091\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.6330 - val_loss: 9.0963\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.3901 - val_loss: 9.0134\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.4300 - val_loss: 8.9622\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.3214 - val_loss: 8.9618\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.3115 - val_loss: 8.8697\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.3827 - val_loss: 8.7948\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 16.3125 - val_loss: 8.7565\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6262 - val_loss: 8.7423\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6107 - val_loss: 8.6487\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.7109 - val_loss: 8.6528\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.9689 - val_loss: 8.7073\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6024 - val_loss: 8.6560\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.7894 - val_loss: 8.5559\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6978 - val_loss: 8.5480\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.8134 - val_loss: 8.4258\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.9228 - val_loss: 8.3563\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.7655 - val_loss: 8.2241\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.4449 - val_loss: 8.2469\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.9138 - val_loss: 8.2698\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.8609 - val_loss: 8.5118\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.9032 - val_loss: 8.2816\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.4964 - val_loss: 8.4368\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.5630 - val_loss: 8.4751\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.5823 - val_loss: 8.4155\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.8225 - val_loss: 8.2897\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6299 - val_loss: 8.1971\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6670 - val_loss: 8.2155\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.3337 - val_loss: 8.2040\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.8407 - val_loss: 8.4079\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.9802 - val_loss: 8.2501\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.3909 - val_loss: 8.2073\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6868 - val_loss: 8.2031\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1362 - val_loss: 8.2860\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.3467 - val_loss: 8.2878\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6774 - val_loss: 8.3301\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.4443 - val_loss: 8.2046\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9631 - val_loss: 8.1562\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8938 - val_loss: 8.0342\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.2818 - val_loss: 8.0995\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.6670 - val_loss: 8.1056\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1974 - val_loss: 8.0615\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.3967 - val_loss: 8.0924\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.0542 - val_loss: 8.2084\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.3004 - val_loss: 8.2970\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9147 - val_loss: 8.2984\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1818 - val_loss: 8.2756\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.4242 - val_loss: 8.0459\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.2975 - val_loss: 8.1010\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1103 - val_loss: 7.9502\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.2950 - val_loss: 8.2651\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.0407 - val_loss: 8.3167\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1118 - val_loss: 8.2194\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.0820 - val_loss: 8.2768\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1095 - val_loss: 8.3109\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.3236 - val_loss: 8.2419\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9229 - val_loss: 8.2207\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.0244 - val_loss: 8.1645\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.2664 - val_loss: 8.2050\n",
      "Epoch 98/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9794 - val_loss: 8.0821\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1827 - val_loss: 8.0013\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8769 - val_loss: 7.9046\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1836 - val_loss: 8.0486\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.2404 - val_loss: 8.1032\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8733 - val_loss: 8.1329\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9616 - val_loss: 7.9651\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6146 - val_loss: 7.9584\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5839 - val_loss: 8.1304\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.0452 - val_loss: 8.0118\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7061 - val_loss: 7.8705\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.0347 - val_loss: 7.9814\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9259 - val_loss: 8.0322\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6267 - val_loss: 7.9311\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6727 - val_loss: 7.8402\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.2849 - val_loss: 7.9132\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9387 - val_loss: 8.0374\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5944 - val_loss: 7.9204\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9299 - val_loss: 7.8449\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1205 - val_loss: 7.9104\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9577 - val_loss: 7.8404\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7223 - val_loss: 7.9120\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.1682 - val_loss: 7.7591\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7842 - val_loss: 7.7872\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5455 - val_loss: 7.8218\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7068 - val_loss: 7.9086\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7079 - val_loss: 8.1128\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3043 - val_loss: 7.9663\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4536 - val_loss: 8.0136\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7389 - val_loss: 7.9493\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3640 - val_loss: 7.7669\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9463 - val_loss: 7.8178\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 14.5783 - val_loss: 7.9407\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9054 - val_loss: 8.0495\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.0289 - val_loss: 8.0401\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6628 - val_loss: 7.9700\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8504 - val_loss: 8.0032\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 14.8800 - val_loss: 7.9706\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5950 - val_loss: 7.9978\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7648 - val_loss: 7.9086\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6534 - val_loss: 7.7829\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6590 - val_loss: 7.8188\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3915 - val_loss: 7.9307\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4283 - val_loss: 7.9177\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 15.0165 - val_loss: 7.9312\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7126 - val_loss: 7.7744\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6668 - val_loss: 7.7187\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8450 - val_loss: 7.9934\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7161 - val_loss: 8.0127\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7998 - val_loss: 7.8774\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4371 - val_loss: 7.7564\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8053 - val_loss: 7.9598\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5609 - val_loss: 7.8231\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4419 - val_loss: 7.7529\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5124 - val_loss: 7.8027\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8518 - val_loss: 7.6999\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4429 - val_loss: 7.6162\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4073 - val_loss: 7.7655\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8252 - val_loss: 7.8884\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3907 - val_loss: 7.9646\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4197 - val_loss: 7.7026\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9307 - val_loss: 7.7499\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6310 - val_loss: 7.7511\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.2452 - val_loss: 7.7714\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.8085 - val_loss: 7.8791\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4555 - val_loss: 7.8587\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3187 - val_loss: 7.6975\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.1819 - val_loss: 7.7891\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.9143 - val_loss: 7.7020\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.2344 - val_loss: 7.7268\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3636 - val_loss: 7.6666\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4688 - val_loss: 7.7074\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.1884 - val_loss: 7.9449\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4770 - val_loss: 7.8454\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.1706 - val_loss: 7.8208\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3193 - val_loss: 7.7194\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5246 - val_loss: 7.8073\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5761 - val_loss: 7.7832\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6768 - val_loss: 7.8434\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.7330 - val_loss: 7.8588\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.5039 - val_loss: 7.8539\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3336 - val_loss: 7.8222\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3942 - val_loss: 7.8760\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4047 - val_loss: 7.8064\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4365 - val_loss: 7.7554\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6686 - val_loss: 7.7545\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6756 - val_loss: 7.6460\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.0683 - val_loss: 7.6768\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6993 - val_loss: 7.7487\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.2432 - val_loss: 7.9175\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.0731 - val_loss: 7.8496\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.4706 - val_loss: 7.6624\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3398 - val_loss: 7.7552\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.1982 - val_loss: 7.8546\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.2853 - val_loss: 7.8621\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.2728 - val_loss: 7.9126\n",
      "Epoch 194/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.1422 - val_loss: 7.8332\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.3627 - val_loss: 7.8565\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.2893 - val_loss: 7.8725\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.6465 - val_loss: 7.7752\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 14.0279 - val_loss: 7.7416\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.0099 - val_loss: 7.6427\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 14.2936 - val_loss: 7.9135\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_forecast_error = pd.DataFrame(\n",
    "        columns=['h', 'mae', 'rmse', 'mape', 'descriptions'])\n",
    "history = ts_model.fit(train_X, train_non_linear_y, epochs=200, batch_size=40, validation_data=(test_X, test_non_linear_y), verbose=1, shuffle=False)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602, 3)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_non_linear_y = ts_model.predict(test_X)\n",
    "pred_non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_pred_non_linear_y = non_linear_y_scaler.inverse_transform(pred_non_linear_y.reshape(test_non_linear_y.shape))\n",
    "inverted_test_non_linear_y = non_linear_y_scaler.inverse_transform(test_non_linear_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_original_non_linear_y=original_non_linear_y[train_size:]\n",
    "# test_original_non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_pred_non_linear_y=inverted_pred_non_linear_y+test_original_non_linear_y\n",
    "# inverted_test_non_linear_y=inverted_test_non_linear_y+test_original_non_linear_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_pred_non_linear_y=pd.DataFrame(inverted_pred_non_linear_y).apply(lambda x:x.sum(),axis=1).to_numpy().reshape(-1,1)\n",
    "inverted_test_non_linear_y=pd.DataFrame(inverted_test_non_linear_y).apply(lambda x:x.sum(),axis=1).to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602, 1)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_pred_non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_linear_y=linear_y[train_size:].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_pred_y=inverted_pred_non_linear_y+test_linear_y\n",
    "inverted_test_y=inverted_test_non_linear_y+test_linear_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[-1]\n",
    "# inverted_pred_non_linear_y[-1]\n",
    "# test_linear_y[-1]\n",
    "# inverted_pred_y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [1.1495819971628591],\n",
       " 'rmse': [1.3641015664690221],\n",
       " 'mape': [0.020309281716039133],\n",
       " 'r2': [0.9742912952570667],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_series(y[train_size:], inverted_pred_y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [0.0],\n",
       " 'rmse': [0.0],\n",
       " 'mape': [0.0],\n",
       " 'r2': [1.0],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_series(y[train_size:], inverted_test_y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [0.008566784557765154],\n",
       " 'rmse': [0.011356980904778144],\n",
       " 'mape': [0.025872486658909972],\n",
       " 'r2': [0.9862877154026026],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_scaler=MinMaxScaler().fit(train_y)\n",
    "normalized_pred_y=normalize_scaler.transform(inverted_pred_y)\n",
    "normalized_inverted_test_y=normalize_scaler.transform(inverted_test_y)\n",
    "normalized_test_y=normalize_scaler.transform(y[train_size:])\n",
    "evaluate_series(normalized_test_y, normalized_pred_y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [0.0],\n",
       " 'rmse': [0.0],\n",
       " 'mape': [0.0],\n",
       " 'r2': [1.0],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_series(normalized_test_y, normalized_inverted_test_y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f3e05a59671f1eb5b3f5f0e003aaa5a39f5d3316373e39c3606e56079185283"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('OPP-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
