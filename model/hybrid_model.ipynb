{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 20:37:48.720183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-27 20:37:48.742499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-27 20:37:48.742672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, MaxPool3D, Bidirectional, ConvLSTM2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import pymongo\n",
    "import random\n",
    "import string\n",
    "import fasttext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "HOME = os.environ['LIMA_HOME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> list:\n",
    "    \"\"\"\n",
    "    clean text with tokenization; stemming; removing stop word, punctuation, number, and empty string.\n",
    "\n",
    "    Args:\n",
    "        text (str): text\n",
    "\n",
    "    Returns:\n",
    "        list: cleaned text as list of tokenized str\n",
    "    \"\"\"\n",
    "\n",
    "    # to list of token\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    # stemming and convert to lower case if not proper noun: punctuation and stop word seem to help POS tagging, remove them after stemming\n",
    "    word_tag = pos_tag(text)\n",
    "    porter = PorterStemmer()\n",
    "    text = [\n",
    "        porter.stem(each[0])\n",
    "        if each[1] != \"NNP\" and each[1] != \"NNPS\" else each[0]\n",
    "        for each in word_tag\n",
    "    ]\n",
    "\n",
    "    # remove stop word: it seems stemming skip stop word; OK to remove stop word after stemming;\n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    text = [each for each in text if not each in stop_word]\n",
    "\n",
    "    # remove punctuation\n",
    "    text = [\n",
    "        each.translate(str.maketrans('', '', string.punctuation))\n",
    "        for each in text\n",
    "    ]\n",
    "    # text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", text) # if using re\n",
    "\n",
    "    # convert number to <NUM>\n",
    "    text = [\"<NUM>\" if each.isdigit() else each for each in text]\n",
    "\n",
    "    # remove empty string\n",
    "    text = [each for each in text if each != \"\"]\n",
    "\n",
    "    return text\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = data.copy()\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "        \n",
    "\t\tnames += [f'{data.columns[j]}(t-{i})' for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [f'{data.columns[j]}(t)' for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [f'{data.columns[j]}(t+{i})' for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def get_TS_cv(k=5, test_size=None):\n",
    "    \"\"\"\n",
    "    ML models do not need to care about forecast horizon when splitting training and test set. Forecast horizon should be handled by feature preparation ([X_t-1,X_t-2...]). Actually repeated K-fold can also be used, but stick to TS split to align with TS_evaluate().\n",
    "    \"\"\"\n",
    "    return TimeSeriesSplit(\n",
    "        n_splits=k,\n",
    "        gap=0,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "\n",
    "def evaluate_series(y_true, y_pred, horizon):\n",
    "    \"\"\"\n",
    "    Some models (like ARIMA) may not support cross_validate(), compare the forecasting result directly\n",
    "    Args:\n",
    "        y_true: y of test set\n",
    "        y_pred: y of prediction\n",
    "        horizon: forecast horizon\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: single row DF with 3 metrics wrt horizon\n",
    "    \"\"\"\n",
    "    # RMSE\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2=r2_score(y_true, y_pred)\n",
    "    forecast_error = {\n",
    "        'h': horizon,\n",
    "        'mae': [mae],\n",
    "        'rmse': [rmse],\n",
    "        'mape': [mape],\n",
    "        'r2':[r2],\n",
    "        'descriptions': \"\"\n",
    "    }\n",
    "    return forecast_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "h = 1\n",
    "past = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient\n",
    "client= InfluxDBClient.from_config_file(f\"{HOME}/dev/DB/influxdb_config.ini\")\n",
    "query_api = client.query_api()\n",
    "df_WTI = query_api.query_data_frame(\"\"\"\n",
    "from(bucket: \"dummy\")\n",
    "  |> range(start: 2011-04-01, stop: 2019-04-01)\n",
    "  |> filter(fn: (r) => r[\"_measurement\"] == \"WTI\") \n",
    "  |> filter(fn: (r) => r[\"type\"] == \"closing_price\") \n",
    "  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "  |> drop(columns: [\"_start\", \"_stop\"])\n",
    "\"\"\")\n",
    "df_WTI=df_WTI[[\"_time\",\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]]\n",
    "df_WTI.columns=[\"Date\",\"CLC4\",\"CLC3\",\"CLC2\",\"CLC1\"]\n",
    "df_WTI.set_index(\"Date\",inplace=True)\n",
    "df_WTI.index=df_WTI.index.map(lambda each: each.date())\n",
    "df_WTI.index=pd.to_datetime(df_WTI.index)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=[each.month for each in df_WTI.index]\n",
    "day=[each.day for each in df_WTI.index]\n",
    "day_in_week=[each.weekday() for each in df_WTI.index]\n",
    "df_dt=pd.DataFrame()\n",
    "df_dt[\"month\"]=month\n",
    "df_dt[\"day\"]=day\n",
    "df_dt[\"day_in_week\"]=day_in_week\n",
    "df_dt.index=df_WTI.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLC4</th>\n",
       "      <th>CLC3</th>\n",
       "      <th>CLC2</th>\n",
       "      <th>CLC1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-04-01</th>\n",
       "      <td>109.17</td>\n",
       "      <td>108.94</td>\n",
       "      <td>108.50</td>\n",
       "      <td>107.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-04</th>\n",
       "      <td>109.83</td>\n",
       "      <td>109.53</td>\n",
       "      <td>109.05</td>\n",
       "      <td>108.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-05</th>\n",
       "      <td>109.91</td>\n",
       "      <td>109.56</td>\n",
       "      <td>108.99</td>\n",
       "      <td>108.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-06</th>\n",
       "      <td>110.39</td>\n",
       "      <td>110.05</td>\n",
       "      <td>109.48</td>\n",
       "      <td>108.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-07</th>\n",
       "      <td>111.66</td>\n",
       "      <td>111.37</td>\n",
       "      <td>110.88</td>\n",
       "      <td>110.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>59.55</td>\n",
       "      <td>59.33</td>\n",
       "      <td>59.08</td>\n",
       "      <td>58.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>60.34</td>\n",
       "      <td>60.23</td>\n",
       "      <td>60.10</td>\n",
       "      <td>59.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>59.91</td>\n",
       "      <td>59.77</td>\n",
       "      <td>59.61</td>\n",
       "      <td>59.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>59.76</td>\n",
       "      <td>59.63</td>\n",
       "      <td>59.48</td>\n",
       "      <td>59.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>60.49</td>\n",
       "      <td>60.40</td>\n",
       "      <td>60.28</td>\n",
       "      <td>60.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2018 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              CLC4    CLC3    CLC2    CLC1\n",
       "Date                                      \n",
       "2011-04-01  109.17  108.94  108.50  107.94\n",
       "2011-04-04  109.83  109.53  109.05  108.47\n",
       "2011-04-05  109.91  109.56  108.99  108.34\n",
       "2011-04-06  110.39  110.05  109.48  108.83\n",
       "2011-04-07  111.66  111.37  110.88  110.30\n",
       "...            ...     ...     ...     ...\n",
       "2019-03-25   59.55   59.33   59.08   58.82\n",
       "2019-03-26   60.34   60.23   60.10   59.94\n",
       "2019-03-27   59.91   59.77   59.61   59.41\n",
       "2019-03-28   59.76   59.63   59.48   59.30\n",
       "2019-03-29   60.49   60.40   60.28   60.14\n",
       "\n",
       "[2018 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_WTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = fasttext.load_model(f\"{HOME}/data/big/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2014, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment=pd.read_pickle(\"df_sentiment_2type.pkl\")\n",
    "df_sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2014, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic=pd.read_pickle(\"df_topic_2type.pkl\")\n",
    "df_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2014, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geoidx=pd.read_pickle(\"df_geoidx_2type.pkl\")\n",
    "df_geoidx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2014, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Combined_Sentiment</th>\n",
       "      <th>Decay_Polarity</th>\n",
       "      <th>Decay_Subjectivity</th>\n",
       "      <th>Decay_Combined_Sentiment</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>...</th>\n",
       "      <th>Terrorist_Threats</th>\n",
       "      <th>War_Acts</th>\n",
       "      <th>Terrorist_Acts</th>\n",
       "      <th>Decay_Geopolitical_Threats</th>\n",
       "      <th>Decay_Nuclear_Threats</th>\n",
       "      <th>Decay_War_Threats</th>\n",
       "      <th>Decay_Terrorist_Threats</th>\n",
       "      <th>Decay_War_Acts</th>\n",
       "      <th>Decay_Terrorist_Acts</th>\n",
       "      <th>CLC1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-04-01</th>\n",
       "      <td>0.085833</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.11566</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.11566</td>\n",
       "      <td>0.155508</td>\n",
       "      <td>0.119983</td>\n",
       "      <td>0.070445</td>\n",
       "      <td>0.500355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.35214</td>\n",
       "      <td>0.359889</td>\n",
       "      <td>0.32632</td>\n",
       "      <td>0.257284</td>\n",
       "      <td>0.242575</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.35214</td>\n",
       "      <td>0.359889</td>\n",
       "      <td>107.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Polarity  Subjectivity  Combined_Sentiment  Decay_Polarity  \\\n",
       "Date                                                                     \n",
       "2011-04-01  0.085833        0.3475             0.11566        0.085833   \n",
       "\n",
       "            Decay_Subjectivity  Decay_Combined_Sentiment    Topic1    Topic2  \\\n",
       "Date                                                                           \n",
       "2011-04-01              0.3475                   0.11566  0.155508  0.119983   \n",
       "\n",
       "              Topic3    Topic4  ...  Terrorist_Threats  War_Acts  \\\n",
       "Date                            ...                                \n",
       "2011-04-01  0.070445  0.500355  ...           0.221318   0.35214   \n",
       "\n",
       "            Terrorist_Acts   Decay_Geopolitical_Threats  \\\n",
       "Date                                                      \n",
       "2011-04-01         0.359889                     0.32632   \n",
       "\n",
       "            Decay_Nuclear_Threats  Decay_War_Threats  Decay_Terrorist_Threats  \\\n",
       "Date                                                                            \n",
       "2011-04-01               0.257284           0.242575                 0.221318   \n",
       "\n",
       "            Decay_War_Acts  Decay_Terrorist_Acts     CLC1  \n",
       "Date                                                       \n",
       "2011-04-01         0.35214               0.359889  107.94  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xy = pd.concat([df_sentiment,df_topic, df_geoidx,df_WTI[\"CLC1\"]], axis=1, join=\"inner\")\n",
    "print(df_Xy.shape)\n",
    "df_Xy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(2009, 148)   |(2009, 1) | '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shifted = series_to_supervised(df_Xy.dropna(), past, h)\n",
    "# remove current day features for forecast\n",
    "for each in df_shifted.columns[:-1]:\n",
    "    if \"(t)\" in each:\n",
    "        df_shifted.drop(each, axis=1, inplace=True)\n",
    "# add time feature without shift \n",
    "df_shifted=pd.concat([df_dt,df_shifted],axis=1).dropna()\n",
    "raw_X = df_shifted.to_numpy()[:, :-1]\n",
    "y =  df_shifted.to_numpy()[:, -1].reshape(-1, 1) \n",
    "# y = df_Xy[df_Xy.index.isin(df_selected.index)].to_numpy()[:, -1].reshape(-1, 1)\n",
    "# y=df_WTI[df_WTI.index.isin(df_selected.index)][\"CLC1\"].to_numpy().reshape(-1, 1)\n",
    "f\"{raw_X.shape}   |{y.shape} | \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression,RFE,RFECV,SelectFromModel,SequentialFeatureSelector,chi2,SelectKBest,f_regression,VarianceThreshold,r_regression\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor\n",
    "from sklearn.svm import LinearSVR,SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Lasso(random_state=42)\n",
    "selector = RFE(estimator,n_features_to_select=30,step=1)\n",
    "scaled_raw_X=MinMaxScaler().fit_transform(raw_X)\n",
    "selector = selector.fit(scaled_raw_X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2009, 30) | (2009, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Decay_Topic2(t-5)', 'Decay_Topic3(t-5)', 'Decay_Topic4(t-5)',\n",
       "       'Decay_Topic5(t-5)', 'Geopolitical_Threats(t-5)',\n",
       "       'Nuclear_Threats(t-5)', 'War_Threats(t-5)', 'Terrorist_Threats(t-5)',\n",
       "       'War_Acts(t-5)', 'Terrorist_Acts (t-5)',\n",
       "       'Decay_Geopolitical_Threats(t-5)', 'Decay_Nuclear_Threats(t-5)',\n",
       "       'Decay_War_Threats(t-5)', 'Decay_Terrorist_Threats(t-5)',\n",
       "       'Decay_Topic2(t-1)', 'Decay_Topic4(t-1)', 'Decay_Topic5(t-1)',\n",
       "       'Geopolitical_Threats(t-1)', 'Nuclear_Threats(t-1)', 'War_Threats(t-1)',\n",
       "       'Terrorist_Threats(t-1)', 'War_Acts(t-1)', 'Terrorist_Acts (t-1)',\n",
       "       'Decay_Geopolitical_Threats(t-1)', 'Decay_Nuclear_Threats(t-1)',\n",
       "       'Decay_War_Threats(t-1)', 'Decay_Terrorist_Threats(t-1)',\n",
       "       'Decay_War_Acts(t-1)', 'Decay_Terrorist_Acts (t-1)', 'CLC1(t-1)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = raw_X[:, selector.get_support()]\n",
    "print(f\"{X.shape} | {y.shape}\")\n",
    "df_shifted.columns[:-1][selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, ARDRegression, SGDRegressor, ElasticNet, Lars, Lasso, GammaRegressor, TweedieRegressor, PoissonRegressor, Ridge, BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor\n",
    "from keras.layers import Reshape,MaxPooling2D,Bidirectional,ConvLSTM2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN \n",
    "from keras.layers import Conv2D,Conv3D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.keras.backend.clear_session()\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X: (1406, 30)\t   \t test_X:(603, 30)\n",
      "train_y: (1406, 1)\t   test_y:(603, 1)\n"
     ]
    }
   ],
   "source": [
    "length=X.shape[0]\n",
    "train_size=int(length*0.7)\n",
    "step_size=1\n",
    "\n",
    "train_X=X[:train_size]\n",
    "train_y=y[:train_size,:]\n",
    "\n",
    "test_X=X[train_size:]\n",
    "test_y=y[train_size:,:]\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X_scaler.fit(train_X)\n",
    "train_X=X_scaler.transform(train_X)\n",
    "test_X=X_scaler.transform(test_X)\n",
    "\n",
    "# train_X=train_X.reshape(train_X.shape[0],step_size,train_X.shape[-1])\n",
    "# test_X=test_X.reshape(test_X.shape[0],step_size,test_X.shape[-1])\n",
    "print(f\"train_X: {train_X.shape}\\t   \\t test_X:{test_X.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\\t   test_y:{test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X=np.concatenate([train_X,test_X])\n",
    "lin_model=LinearRegression()\n",
    "lin_model.fit(train_X,train_y.ravel())\n",
    "linear_y=lin_model.predict(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [1.0024480675820517],\n",
       " 'rmse': [1.3216394888907417],\n",
       " 'mape': [0.015384922471402796],\n",
       " 'r2': [0.9968102216157455],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_series(y,linear_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2009, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_y=y-linear_y.reshape(y.shape)\n",
    "non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "plt.plot(non_linear_y[250:300],'ob') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(2009, 1)|(1406, 1)|(603, 1)'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_non_linear_y=non_linear_y[:train_size,:]\n",
    "test_non_linear_y=non_linear_y[train_size:,:]\n",
    "f\"{non_linear_y.shape}|{train_non_linear_y.shape}|{test_non_linear_y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(1406, 1)|(603, 1)'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_y_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "non_linear_y_scaler.fit(train_non_linear_y)\n",
    "train_non_linear_y=non_linear_y_scaler.transform(train_non_linear_y)\n",
    "test_non_linear_y=non_linear_y_scaler.transform(test_non_linear_y)\n",
    "f\"{train_non_linear_y.shape}|{test_non_linear_y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "ts_inputs = Input(shape=(train_X.shape[-1],))\n",
    "ts_model=Reshape((step_size,train_X.shape[-1]))(ts_inputs)\n",
    "ts_model=LSTM(50,dropout=0.8 ,return_sequences=True)(ts_model)\n",
    "ts_model= Dropout(0.8)(ts_model)\n",
    "ts_model=LSTM(50,dropout=0.8 ,return_sequences=True)(ts_model)\n",
    "ts_model= Dropout(0.8)(ts_model)\n",
    "ts_model=LSTM(50,dropout=0.8 ,return_sequences=True)(ts_model)\n",
    "ts_model= Dropout(0.8)(ts_model)\n",
    "ts_model=LSTM(50,dropout=0.8 ,return_sequences=False)(ts_model)\n",
    "ts_model= Dropout(0.8)(ts_model)\n",
    "# ts_model= Dropout(0.4)(ts_model)\n",
    "# ts_model=Bidirectional(GRU(300,dropout=0.2 ,return_sequences=True))(ts_model)\n",
    "# ts_model= Dropout(0.4)(ts_model)\n",
    "# ts_model=Bidirectional(GRU(300,dropout=0.2 ,return_sequences=True))(ts_model)\n",
    "# ts_model= Dropout(0.4)(ts_model)\n",
    "# ts_model=Bidirectional(LSTM(300,dropout=0.4 ,return_sequences=False))(ts_model)\n",
    "ts_model =Dense(1)(ts_model)\n",
    "ts_model = Model(inputs=ts_inputs, outputs=ts_model)\n",
    "# ts_model.compile(loss='mae', optimizer=Adam())\n",
    "ts_model.compile(loss='log_cosh', optimizer=Adam(0.0002))\n",
    "# ts_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "36/36 [==============================] - 4s 31ms/step - loss: 56.8001 - val_loss: 59.0689\n",
      "Epoch 2/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.7782 - val_loss: 59.0475\n",
      "Epoch 3/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.7573 - val_loss: 59.0254\n",
      "Epoch 4/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.7307 - val_loss: 59.0026\n",
      "Epoch 5/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.7121 - val_loss: 58.9786\n",
      "Epoch 6/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.6813 - val_loss: 58.9528\n",
      "Epoch 7/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.6578 - val_loss: 58.9248\n",
      "Epoch 8/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.6254 - val_loss: 58.8946\n",
      "Epoch 9/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.5933 - val_loss: 58.8616\n",
      "Epoch 10/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.5565 - val_loss: 58.8252\n",
      "Epoch 11/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.5244 - val_loss: 58.7852\n",
      "Epoch 12/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.4804 - val_loss: 58.7421\n",
      "Epoch 13/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.4221 - val_loss: 58.6938\n",
      "Epoch 14/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.3844 - val_loss: 58.6412\n",
      "Epoch 15/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.3337 - val_loss: 58.5836\n",
      "Epoch 16/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.2668 - val_loss: 58.5206\n",
      "Epoch 17/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.1970 - val_loss: 58.4502\n",
      "Epoch 18/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.1220 - val_loss: 58.3734\n",
      "Epoch 19/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 56.0485 - val_loss: 58.2906\n",
      "Epoch 20/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 55.9555 - val_loss: 58.1990\n",
      "Epoch 21/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 55.8680 - val_loss: 58.1003\n",
      "Epoch 22/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 55.7668 - val_loss: 57.9914\n",
      "Epoch 23/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 55.6513 - val_loss: 57.8728\n",
      "Epoch 24/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 55.5220 - val_loss: 57.7412\n",
      "Epoch 25/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 55.4059 - val_loss: 57.5978\n",
      "Epoch 26/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 55.2756 - val_loss: 57.4427\n",
      "Epoch 27/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 55.0960 - val_loss: 57.2688\n",
      "Epoch 28/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 54.9121 - val_loss: 57.0770\n",
      "Epoch 29/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 54.7328 - val_loss: 56.8657\n",
      "Epoch 30/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 54.4811 - val_loss: 56.6314\n",
      "Epoch 31/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 54.3279 - val_loss: 56.3792\n",
      "Epoch 32/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 54.0437 - val_loss: 56.0995\n",
      "Epoch 33/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 53.8228 - val_loss: 55.7950\n",
      "Epoch 34/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 53.4661 - val_loss: 55.4536\n",
      "Epoch 35/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 53.1181 - val_loss: 55.0855\n",
      "Epoch 36/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 52.8227 - val_loss: 54.6889\n",
      "Epoch 37/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 52.4959 - val_loss: 54.2571\n",
      "Epoch 38/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 52.1576 - val_loss: 53.8007\n",
      "Epoch 39/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 51.7895 - val_loss: 53.3105\n",
      "Epoch 40/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 51.3537 - val_loss: 52.7882\n",
      "Epoch 41/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 50.6570 - val_loss: 52.2078\n",
      "Epoch 42/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 50.3258 - val_loss: 51.6043\n",
      "Epoch 43/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 49.8308 - val_loss: 50.9815\n",
      "Epoch 44/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 49.2059 - val_loss: 50.3154\n",
      "Epoch 45/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 48.8040 - val_loss: 49.6423\n",
      "Epoch 46/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 47.9413 - val_loss: 48.9106\n",
      "Epoch 47/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 47.4598 - val_loss: 48.1671\n",
      "Epoch 48/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 46.8863 - val_loss: 47.4022\n",
      "Epoch 49/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 46.4629 - val_loss: 46.6392\n",
      "Epoch 50/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 45.7403 - val_loss: 45.8572\n",
      "Epoch 51/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 45.1092 - val_loss: 45.0626\n",
      "Epoch 52/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 44.5637 - val_loss: 44.2685\n",
      "Epoch 53/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 43.8309 - val_loss: 43.4605\n",
      "Epoch 54/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 43.1215 - val_loss: 42.6502\n",
      "Epoch 55/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 42.4243 - val_loss: 41.8496\n",
      "Epoch 56/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 41.9594 - val_loss: 41.0496\n",
      "Epoch 57/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 41.0071 - val_loss: 40.2566\n",
      "Epoch 58/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 40.4776 - val_loss: 39.4743\n",
      "Epoch 59/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 39.7048 - val_loss: 38.6918\n",
      "Epoch 60/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 39.5164 - val_loss: 37.9387\n",
      "Epoch 61/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 38.8666 - val_loss: 37.2029\n",
      "Epoch 62/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 37.8369 - val_loss: 36.4561\n",
      "Epoch 63/300\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 37.8019 - val_loss: 35.7513\n",
      "Epoch 64/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 36.7425 - val_loss: 35.0495\n",
      "Epoch 65/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 36.2348 - val_loss: 34.3610\n",
      "Epoch 66/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 35.8862 - val_loss: 33.6942\n",
      "Epoch 67/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 35.1413 - val_loss: 33.0466\n",
      "Epoch 68/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 34.5042 - val_loss: 32.4072\n",
      "Epoch 69/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 34.0677 - val_loss: 31.7820\n",
      "Epoch 70/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 33.7130 - val_loss: 31.1801\n",
      "Epoch 71/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 33.2100 - val_loss: 30.5806\n",
      "Epoch 72/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 32.6167 - val_loss: 30.0020\n",
      "Epoch 73/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 31.9639 - val_loss: 29.4375\n",
      "Epoch 74/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 31.7801 - val_loss: 28.8901\n",
      "Epoch 75/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 31.4645 - val_loss: 28.3501\n",
      "Epoch 76/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 30.6449 - val_loss: 27.8287\n",
      "Epoch 77/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 30.1560 - val_loss: 27.3125\n",
      "Epoch 78/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 29.5286 - val_loss: 26.7992\n",
      "Epoch 79/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 29.6323 - val_loss: 26.3119\n",
      "Epoch 80/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 28.9696 - val_loss: 25.8480\n",
      "Epoch 81/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 28.4615 - val_loss: 25.3819\n",
      "Epoch 82/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 28.2934 - val_loss: 24.9275\n",
      "Epoch 83/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 27.8122 - val_loss: 24.4852\n",
      "Epoch 84/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 27.2617 - val_loss: 24.0544\n",
      "Epoch 85/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 27.0692 - val_loss: 23.6376\n",
      "Epoch 86/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 26.3079 - val_loss: 23.2119\n",
      "Epoch 87/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 26.1007 - val_loss: 22.8045\n",
      "Epoch 88/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 26.1597 - val_loss: 22.4175\n",
      "Epoch 89/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 25.7862 - val_loss: 22.0437\n",
      "Epoch 90/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 25.4705 - val_loss: 21.6765\n",
      "Epoch 91/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 24.8443 - val_loss: 21.3044\n",
      "Epoch 92/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 25.1885 - val_loss: 20.9344\n",
      "Epoch 93/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 24.0865 - val_loss: 20.5775\n",
      "Epoch 94/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 24.4238 - val_loss: 20.2431\n",
      "Epoch 95/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 24.0131 - val_loss: 19.9152\n",
      "Epoch 96/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 24.4197 - val_loss: 19.5968\n",
      "Epoch 97/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 23.6108 - val_loss: 19.2760\n",
      "Epoch 98/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 23.3665 - val_loss: 18.9660\n",
      "Epoch 99/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 23.3802 - val_loss: 18.6678\n",
      "Epoch 100/300\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 23.2738 - val_loss: 18.3586\n",
      "Epoch 101/300\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.1913 - val_loss: 18.0612\n",
      "Epoch 102/300\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.3829 - val_loss: 17.7707\n",
      "Epoch 103/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 21.9704 - val_loss: 17.4860\n",
      "Epoch 104/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 21.1729 - val_loss: 17.2124\n",
      "Epoch 105/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 21.5807 - val_loss: 16.9539\n",
      "Epoch 106/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 21.1256 - val_loss: 16.6953\n",
      "Epoch 107/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.6224 - val_loss: 16.4307\n",
      "Epoch 108/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 21.1515 - val_loss: 16.1796\n",
      "Epoch 109/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 21.0737 - val_loss: 15.9243\n",
      "Epoch 110/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.7157 - val_loss: 15.6783\n",
      "Epoch 111/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.3247 - val_loss: 15.4309\n",
      "Epoch 112/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.4866 - val_loss: 15.1906\n",
      "Epoch 113/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.1352 - val_loss: 14.9508\n",
      "Epoch 114/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.5132 - val_loss: 14.7152\n",
      "Epoch 115/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.4569 - val_loss: 14.4814\n",
      "Epoch 116/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.0790 - val_loss: 14.2625\n",
      "Epoch 117/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.0198 - val_loss: 14.0549\n",
      "Epoch 118/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.4122 - val_loss: 13.8465\n",
      "Epoch 119/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.2099 - val_loss: 13.6363\n",
      "Epoch 120/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9532 - val_loss: 13.4317\n",
      "Epoch 121/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.5627 - val_loss: 13.2276\n",
      "Epoch 122/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9826 - val_loss: 13.0361\n",
      "Epoch 123/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9512 - val_loss: 12.8301\n",
      "Epoch 124/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.4158 - val_loss: 12.6380\n",
      "Epoch 125/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.6947 - val_loss: 12.4508\n",
      "Epoch 126/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.6364 - val_loss: 12.2828\n",
      "Epoch 127/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.8080 - val_loss: 12.1094\n",
      "Epoch 128/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.5702 - val_loss: 11.9269\n",
      "Epoch 129/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.8850 - val_loss: 11.7576\n",
      "Epoch 130/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.0394 - val_loss: 11.5725\n",
      "Epoch 131/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.7055 - val_loss: 11.3957\n",
      "Epoch 132/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.6417 - val_loss: 11.2322\n",
      "Epoch 133/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.0154 - val_loss: 11.0711\n",
      "Epoch 134/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.8753 - val_loss: 10.9246\n",
      "Epoch 135/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.3575 - val_loss: 10.7609\n",
      "Epoch 136/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.5739 - val_loss: 10.6099\n",
      "Epoch 137/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4920 - val_loss: 10.4564\n",
      "Epoch 138/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.9965 - val_loss: 10.3027\n",
      "Epoch 139/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.5818 - val_loss: 10.1493\n",
      "Epoch 140/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0292 - val_loss: 10.0199\n",
      "Epoch 141/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.3254 - val_loss: 9.8901\n",
      "Epoch 142/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.2124 - val_loss: 9.7467\n",
      "Epoch 143/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.1748 - val_loss: 9.6219\n",
      "Epoch 144/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.2130 - val_loss: 9.5069\n",
      "Epoch 145/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.2836 - val_loss: 9.3869\n",
      "Epoch 146/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.3487 - val_loss: 9.2701\n",
      "Epoch 147/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.7572 - val_loss: 9.1693\n",
      "Epoch 148/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.3419 - val_loss: 9.0527\n",
      "Epoch 149/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.4329 - val_loss: 8.9437\n",
      "Epoch 150/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.2366 - val_loss: 8.8457\n",
      "Epoch 151/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.7087 - val_loss: 8.7618\n",
      "Epoch 152/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.6203 - val_loss: 8.6677\n",
      "Epoch 153/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.1801 - val_loss: 8.5697\n",
      "Epoch 154/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.5362 - val_loss: 8.4683\n",
      "Epoch 155/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.1303 - val_loss: 8.3652\n",
      "Epoch 156/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5135 - val_loss: 8.2831\n",
      "Epoch 157/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.8064 - val_loss: 8.2017\n",
      "Epoch 158/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.3321 - val_loss: 8.1162\n",
      "Epoch 159/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.8103 - val_loss: 8.0237\n",
      "Epoch 160/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5621 - val_loss: 7.9440\n",
      "Epoch 161/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.1899 - val_loss: 7.8665\n",
      "Epoch 162/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.9238 - val_loss: 7.7945\n",
      "Epoch 163/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.0875 - val_loss: 7.7263\n",
      "Epoch 164/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.8748 - val_loss: 7.6570\n",
      "Epoch 165/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.9836 - val_loss: 7.5828\n",
      "Epoch 166/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5240 - val_loss: 7.5097\n",
      "Epoch 167/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.8159 - val_loss: 7.4533\n",
      "Epoch 168/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.6085 - val_loss: 7.4015\n",
      "Epoch 169/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.7124 - val_loss: 7.3413\n",
      "Epoch 170/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.7325 - val_loss: 7.2845\n",
      "Epoch 171/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5294 - val_loss: 7.2305\n",
      "Epoch 172/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.6187 - val_loss: 7.1667\n",
      "Epoch 173/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4375 - val_loss: 7.0917\n",
      "Epoch 174/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2295 - val_loss: 7.0300\n",
      "Epoch 175/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4087 - val_loss: 6.9676\n",
      "Epoch 176/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4031 - val_loss: 6.9258\n",
      "Epoch 177/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.9345 - val_loss: 6.8668\n",
      "Epoch 178/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4151 - val_loss: 6.8133\n",
      "Epoch 179/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4385 - val_loss: 6.7581\n",
      "Epoch 180/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4330 - val_loss: 6.7092\n",
      "Epoch 181/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.1537 - val_loss: 6.6690\n",
      "Epoch 182/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8260 - val_loss: 6.6083\n",
      "Epoch 183/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7071 - val_loss: 6.5673\n",
      "Epoch 184/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2335 - val_loss: 6.5171\n",
      "Epoch 185/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4696 - val_loss: 6.4765\n",
      "Epoch 186/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4700 - val_loss: 6.4422\n",
      "Epoch 187/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2646 - val_loss: 6.4147\n",
      "Epoch 188/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5548 - val_loss: 6.3729\n",
      "Epoch 189/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4038 - val_loss: 6.3510\n",
      "Epoch 190/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2783 - val_loss: 6.3156\n",
      "Epoch 191/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0905 - val_loss: 6.2755\n",
      "Epoch 192/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6425 - val_loss: 6.2342\n",
      "Epoch 193/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8132 - val_loss: 6.1979\n",
      "Epoch 194/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0743 - val_loss: 6.1756\n",
      "Epoch 195/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.9166 - val_loss: 6.1551\n",
      "Epoch 196/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.9543 - val_loss: 6.1553\n",
      "Epoch 197/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4518 - val_loss: 6.1268\n",
      "Epoch 198/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8287 - val_loss: 6.1033\n",
      "Epoch 199/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7774 - val_loss: 6.0767\n",
      "Epoch 200/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8887 - val_loss: 6.0551\n",
      "Epoch 201/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7824 - val_loss: 6.0227\n",
      "Epoch 202/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.3241 - val_loss: 5.9888\n",
      "Epoch 203/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0027 - val_loss: 5.9765\n",
      "Epoch 204/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.1691 - val_loss: 5.9510\n",
      "Epoch 205/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0684 - val_loss: 5.9186\n",
      "Epoch 206/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.9894 - val_loss: 5.9024\n",
      "Epoch 207/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5711 - val_loss: 5.8900\n",
      "Epoch 208/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4646 - val_loss: 5.8653\n",
      "Epoch 209/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6708 - val_loss: 5.8424\n",
      "Epoch 210/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6633 - val_loss: 5.8198\n",
      "Epoch 211/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7850 - val_loss: 5.8089\n",
      "Epoch 212/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2762 - val_loss: 5.7839\n",
      "Epoch 213/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.9893 - val_loss: 5.7859\n",
      "Epoch 214/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4947 - val_loss: 5.7767\n",
      "Epoch 215/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7034 - val_loss: 5.7648\n",
      "Epoch 216/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.3434 - val_loss: 5.7485\n",
      "Epoch 217/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6923 - val_loss: 5.7150\n",
      "Epoch 218/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5073 - val_loss: 5.7029\n",
      "Epoch 219/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7121 - val_loss: 5.6844\n",
      "Epoch 220/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7462 - val_loss: 5.6622\n",
      "Epoch 221/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7607 - val_loss: 5.6605\n",
      "Epoch 222/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5947 - val_loss: 5.6541\n",
      "Epoch 223/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8392 - val_loss: 5.6446\n",
      "Epoch 224/300\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.9270 - val_loss: 5.6275\n",
      "Epoch 225/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2625 - val_loss: 5.6111\n",
      "Epoch 226/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7945 - val_loss: 5.6022\n",
      "Epoch 227/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4369 - val_loss: 5.6016\n",
      "Epoch 228/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7992 - val_loss: 5.5817\n",
      "Epoch 229/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1240 - val_loss: 5.5680\n",
      "Epoch 230/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7037 - val_loss: 5.5640\n",
      "Epoch 231/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6869 - val_loss: 5.5615\n",
      "Epoch 232/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.9884 - val_loss: 5.5625\n",
      "Epoch 233/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1178 - val_loss: 5.5486\n",
      "Epoch 234/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6234 - val_loss: 5.5398\n",
      "Epoch 235/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8824 - val_loss: 5.5177\n",
      "Epoch 236/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.2582 - val_loss: 5.4959\n",
      "Epoch 237/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6046 - val_loss: 5.4929\n",
      "Epoch 238/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5338 - val_loss: 5.4996\n",
      "Epoch 239/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7461 - val_loss: 5.5077\n",
      "Epoch 240/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2352 - val_loss: 5.5090\n",
      "Epoch 241/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5402 - val_loss: 5.4902\n",
      "Epoch 242/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8243 - val_loss: 5.4912\n",
      "Epoch 243/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6633 - val_loss: 5.4849\n",
      "Epoch 244/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8504 - val_loss: 5.4859\n",
      "Epoch 245/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4765 - val_loss: 5.4807\n",
      "Epoch 246/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6931 - val_loss: 5.4679\n",
      "Epoch 247/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0482 - val_loss: 5.4633\n",
      "Epoch 248/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.2428 - val_loss: 5.4534\n",
      "Epoch 249/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3338 - val_loss: 5.4653\n",
      "Epoch 250/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7242 - val_loss: 5.4553\n",
      "Epoch 251/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.8284 - val_loss: 5.4491\n",
      "Epoch 252/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.2986 - val_loss: 5.4374\n",
      "Epoch 253/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5690 - val_loss: 5.4338\n",
      "Epoch 254/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4586 - val_loss: 5.4297\n",
      "Epoch 255/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.0866 - val_loss: 5.4408\n",
      "Epoch 256/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6064 - val_loss: 5.4413\n",
      "Epoch 257/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5510 - val_loss: 5.4403\n",
      "Epoch 258/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3511 - val_loss: 5.4350\n",
      "Epoch 259/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6351 - val_loss: 5.4438\n",
      "Epoch 260/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5385 - val_loss: 5.4351\n",
      "Epoch 261/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8976 - val_loss: 5.4328\n",
      "Epoch 262/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6979 - val_loss: 5.4445\n",
      "Epoch 263/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5432 - val_loss: 5.4390\n",
      "Epoch 264/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5451 - val_loss: 5.4476\n",
      "Epoch 265/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7144 - val_loss: 5.4575\n",
      "Epoch 266/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6958 - val_loss: 5.4641\n",
      "Epoch 267/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3739 - val_loss: 5.4657\n",
      "Epoch 268/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.7865 - val_loss: 5.4550\n",
      "Epoch 269/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6788 - val_loss: 5.4436\n",
      "Epoch 270/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.9060 - val_loss: 5.4393\n",
      "Epoch 271/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.0210 - val_loss: 5.4383\n",
      "Epoch 272/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3414 - val_loss: 5.4311\n",
      "Epoch 273/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1535 - val_loss: 5.4340\n",
      "Epoch 274/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5424 - val_loss: 5.4339\n",
      "Epoch 275/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.2351 - val_loss: 5.4303\n",
      "Epoch 276/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.8710 - val_loss: 5.4319\n",
      "Epoch 277/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5905 - val_loss: 5.4331\n",
      "Epoch 278/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7261 - val_loss: 5.4322\n",
      "Epoch 279/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3233 - val_loss: 5.4425\n",
      "Epoch 280/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.0334 - val_loss: 5.4410\n",
      "Epoch 281/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1641 - val_loss: 5.4421\n",
      "Epoch 282/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4050 - val_loss: 5.4362\n",
      "Epoch 283/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.0907 - val_loss: 5.4329\n",
      "Epoch 284/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.9352 - val_loss: 5.4200\n",
      "Epoch 285/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7554 - val_loss: 5.4257\n",
      "Epoch 286/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1352 - val_loss: 5.4239\n",
      "Epoch 287/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5760 - val_loss: 5.4311\n",
      "Epoch 288/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5781 - val_loss: 5.4400\n",
      "Epoch 289/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3324 - val_loss: 5.4439\n",
      "Epoch 290/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.0904 - val_loss: 5.4608\n",
      "Epoch 291/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1672 - val_loss: 5.4594\n",
      "Epoch 292/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1528 - val_loss: 5.4672\n",
      "Epoch 293/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1494 - val_loss: 5.4747\n",
      "Epoch 294/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8137 - val_loss: 5.4768\n",
      "Epoch 295/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7619 - val_loss: 5.4647\n",
      "Epoch 296/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.9136 - val_loss: 5.4706\n",
      "Epoch 297/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.8467 - val_loss: 5.4788\n",
      "Epoch 298/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5737 - val_loss: 5.4789\n",
      "Epoch 299/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.9936 - val_loss: 5.4906\n",
      "Epoch 300/300\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1925 - val_loss: 5.5007\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_forecast_error = pd.DataFrame(\n",
    "        columns=['h', 'mae', 'rmse', 'mape', 'descriptions'])\n",
    "history = ts_model.fit(train_X, train_non_linear_y, epochs=300, batch_size=40, validation_data=(test_X, test_non_linear_y), verbose=1, shuffle=False)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603, 1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_non_linear_y = ts_model.predict(test_X)\n",
    "pred_non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [0.9465669357972669],\n",
       " 'rmse': [1.1664253154095656],\n",
       " 'mape': [0.01656308383543575],\n",
       " 'r2': [0.9812303339118221],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_pred_non_linear_y = non_linear_y_scaler.inverse_transform(pred_non_linear_y.reshape(test_non_linear_y.shape))\n",
    "inverted_test_non_linear_y = non_linear_y_scaler.inverse_transform(test_non_linear_y)\n",
    "test_linear_y=linear_y[train_size:].reshape(-1,1)\n",
    "inverted_pred_non_linear_y=inverted_pred_non_linear_y+test_linear_y\n",
    "inverted_test_non_linear_y=inverted_test_non_linear_y+test_linear_y\n",
    "\n",
    "evaluate_series(test_y, inverted_pred_non_linear_y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [0.0],\n",
       " 'rmse': [0.0],\n",
       " 'mape': [0.0],\n",
       " 'r2': [1.0],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_series(test_y, inverted_test_non_linear_y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f3e05a59671f1eb5b3f5f0e003aaa5a39f5d3316373e39c3606e56079185283"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('OPP-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
