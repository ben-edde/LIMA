{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from tensorflow import keras\n",
    "\n",
    "from Dataset import Dataset\n",
    "from FeatureEngineeringService import FeatureEngineeringService\n",
    "from FeatureEngineeringStrategy import *\n",
    "\n",
    "HOME = os.environ['LIMA_HOME']\n",
    "# set random seed\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value) \n",
    "h = 1\n",
    "past = 10\n",
    "training_ratio=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "def evaluate_series(y_true, y_pred, horizon):\n",
    "    \"\"\"\n",
    "    Some models (like ARIMA) may not support cross_validate(), compare the forecasting result directly\n",
    "    Args:\n",
    "        y_true: y of test set\n",
    "        y_pred: y of prediction\n",
    "        horizon: forecast horizon\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: single row DF with 3 metrics wrt horizon\n",
    "    \"\"\"\n",
    "    # RMSE\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2=r2_score(y_true, y_pred)\n",
    "    forecast_error = {\n",
    "        'h': horizon,\n",
    "        'mae': [mae],\n",
    "        'rmse': [rmse],\n",
    "        'mape': [mape],\n",
    "        'r2':[r2],\n",
    "        'descriptions': \"\"\n",
    "    }\n",
    "    return forecast_error\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "\n",
    "        names += [f'{data.columns[j]}(t-{i})' for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [f'{data.columns[j]}(t)' for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [f'{data.columns[j]}(t+{i})' for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "news_feature_helper = FeatureProviderFactory.get_provider(\"news\")\n",
    "price_feature_helper = FeatureProviderFactory.get_provider(\n",
    "            \"price\") \n",
    "df_news_feature = news_feature_helper.get_feature(\n",
    "        mode=\"build\")\n",
    "df_price_feature, df_dt = price_feature_helper.get_feature(\n",
    "        mode=\"build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy = pd.concat([df_news_feature, df_price_feature],axis=1,                    join=\"inner\")\n",
    "# joblib.dump(df_Xy,\"tmp/df_Xy.joblib\")\n",
    "# df_Xy=joblib.load(\"tmp/df_Xy.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy=df_Xy[df_Xy.index>=\"2011-04-01\"]\n",
    "df_Xy=df_Xy[df_Xy.index<=\"2019-04-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Combined_Sentiment</th>\n",
       "      <th>Decay_Polarity</th>\n",
       "      <th>Decay_Subjectivity</th>\n",
       "      <th>Decay_Combined_Sentiment</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>...</th>\n",
       "      <th>Decay_Geopolitical_Threats</th>\n",
       "      <th>Decay_Nuclear_Threats</th>\n",
       "      <th>Decay_War_Threats</th>\n",
       "      <th>Decay_Terrorist_Threats</th>\n",
       "      <th>Decay_War_Acts</th>\n",
       "      <th>Decay_Terrorist_Acts</th>\n",
       "      <th>CLC4</th>\n",
       "      <th>CLC3</th>\n",
       "      <th>CLC2</th>\n",
       "      <th>CLC1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-04-01</th>\n",
       "      <td>0.085833</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.115660</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>0.446071</td>\n",
       "      <td>0.084436</td>\n",
       "      <td>0.133230</td>\n",
       "      <td>0.266711</td>\n",
       "      <td>0.196648</td>\n",
       "      <td>0.389213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923041</td>\n",
       "      <td>0.764067</td>\n",
       "      <td>0.673147</td>\n",
       "      <td>0.667264</td>\n",
       "      <td>1.035576</td>\n",
       "      <td>1.062797</td>\n",
       "      <td>109.17</td>\n",
       "      <td>108.94</td>\n",
       "      <td>108.50</td>\n",
       "      <td>107.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-04</th>\n",
       "      <td>0.068889</td>\n",
       "      <td>0.342222</td>\n",
       "      <td>0.092464</td>\n",
       "      <td>0.176389</td>\n",
       "      <td>0.959008</td>\n",
       "      <td>0.246140</td>\n",
       "      <td>0.143312</td>\n",
       "      <td>0.197903</td>\n",
       "      <td>0.314226</td>\n",
       "      <td>0.305502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781256</td>\n",
       "      <td>0.665039</td>\n",
       "      <td>0.583094</td>\n",
       "      <td>0.594434</td>\n",
       "      <td>0.938713</td>\n",
       "      <td>0.969097</td>\n",
       "      <td>109.83</td>\n",
       "      <td>109.53</td>\n",
       "      <td>109.05</td>\n",
       "      <td>108.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-05</th>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.299536</td>\n",
       "      <td>0.031424</td>\n",
       "      <td>0.155625</td>\n",
       "      <td>1.015314</td>\n",
       "      <td>0.214869</td>\n",
       "      <td>0.177093</td>\n",
       "      <td>0.206282</td>\n",
       "      <td>0.197064</td>\n",
       "      <td>0.390251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782248</td>\n",
       "      <td>0.729914</td>\n",
       "      <td>0.577842</td>\n",
       "      <td>0.582328</td>\n",
       "      <td>0.920887</td>\n",
       "      <td>0.995813</td>\n",
       "      <td>109.91</td>\n",
       "      <td>109.56</td>\n",
       "      <td>108.99</td>\n",
       "      <td>108.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Polarity  Subjectivity  Combined_Sentiment  Decay_Polarity  \\\n",
       "Date                                                                     \n",
       "2011-04-01  0.085833      0.347500            0.115660        0.060119   \n",
       "2011-04-04  0.068889      0.342222            0.092464        0.176389   \n",
       "2011-04-05  0.024181      0.299536            0.031424        0.155625   \n",
       "\n",
       "            Decay_Subjectivity  Decay_Combined_Sentiment    Topic1    Topic2  \\\n",
       "Date                                                                           \n",
       "2011-04-01            0.446071                  0.084436  0.133230  0.266711   \n",
       "2011-04-04            0.959008                  0.246140  0.143312  0.197903   \n",
       "2011-04-05            1.015314                  0.214869  0.177093  0.206282   \n",
       "\n",
       "              Topic3    Topic4  ...  Decay_Geopolitical_Threats  \\\n",
       "Date                            ...                               \n",
       "2011-04-01  0.196648  0.389213  ...                    0.923041   \n",
       "2011-04-04  0.314226  0.305502  ...                    0.781256   \n",
       "2011-04-05  0.197064  0.390251  ...                    0.782248   \n",
       "\n",
       "            Decay_Nuclear_Threats  Decay_War_Threats  Decay_Terrorist_Threats  \\\n",
       "Date                                                                            \n",
       "2011-04-01               0.764067           0.673147                 0.667264   \n",
       "2011-04-04               0.665039           0.583094                 0.594434   \n",
       "2011-04-05               0.729914           0.577842                 0.582328   \n",
       "\n",
       "            Decay_War_Acts  Decay_Terrorist_Acts    CLC4    CLC3    CLC2  \\\n",
       "Date                                                                       \n",
       "2011-04-01        1.035576              1.062797  109.17  108.94  108.50   \n",
       "2011-04-04        0.938713              0.969097  109.83  109.53  109.05   \n",
       "2011-04-05        0.920887              0.995813  109.91  109.56  108.99   \n",
       "\n",
       "              CLC1  \n",
       "Date                \n",
       "2011-04-01  107.94  \n",
       "2011-04-04  108.47  \n",
       "2011-04-05  108.34  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Combined_Sentiment</th>\n",
       "      <th>Decay_Polarity</th>\n",
       "      <th>Decay_Subjectivity</th>\n",
       "      <th>Decay_Combined_Sentiment</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>...</th>\n",
       "      <th>Decay_Geopolitical_Threats</th>\n",
       "      <th>Decay_Nuclear_Threats</th>\n",
       "      <th>Decay_War_Threats</th>\n",
       "      <th>Decay_Terrorist_Threats</th>\n",
       "      <th>Decay_War_Acts</th>\n",
       "      <th>Decay_Terrorist_Acts</th>\n",
       "      <th>CLC4</th>\n",
       "      <th>CLC3</th>\n",
       "      <th>CLC2</th>\n",
       "      <th>CLC1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>-0.019507</td>\n",
       "      <td>0.339305</td>\n",
       "      <td>-0.026125</td>\n",
       "      <td>-0.133724</td>\n",
       "      <td>0.908372</td>\n",
       "      <td>-0.182488</td>\n",
       "      <td>0.129218</td>\n",
       "      <td>0.175803</td>\n",
       "      <td>0.219396</td>\n",
       "      <td>0.109485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797849</td>\n",
       "      <td>0.773509</td>\n",
       "      <td>0.586512</td>\n",
       "      <td>0.664073</td>\n",
       "      <td>0.966407</td>\n",
       "      <td>1.005316</td>\n",
       "      <td>59.76</td>\n",
       "      <td>59.63</td>\n",
       "      <td>59.48</td>\n",
       "      <td>59.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>0.208462</td>\n",
       "      <td>0.309487</td>\n",
       "      <td>0.272978</td>\n",
       "      <td>0.100826</td>\n",
       "      <td>0.965868</td>\n",
       "      <td>0.128440</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.199383</td>\n",
       "      <td>0.299585</td>\n",
       "      <td>0.078013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825752</td>\n",
       "      <td>0.785170</td>\n",
       "      <td>0.592541</td>\n",
       "      <td>0.715096</td>\n",
       "      <td>0.963197</td>\n",
       "      <td>1.042693</td>\n",
       "      <td>60.49</td>\n",
       "      <td>60.40</td>\n",
       "      <td>60.28</td>\n",
       "      <td>60.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>-0.071296</td>\n",
       "      <td>0.215741</td>\n",
       "      <td>-0.086678</td>\n",
       "      <td>-0.226358</td>\n",
       "      <td>1.236487</td>\n",
       "      <td>-0.584150</td>\n",
       "      <td>0.094111</td>\n",
       "      <td>0.115631</td>\n",
       "      <td>0.175034</td>\n",
       "      <td>0.093650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924013</td>\n",
       "      <td>0.883445</td>\n",
       "      <td>0.636587</td>\n",
       "      <td>0.727725</td>\n",
       "      <td>1.065803</td>\n",
       "      <td>1.191592</td>\n",
       "      <td>61.83</td>\n",
       "      <td>61.80</td>\n",
       "      <td>61.71</td>\n",
       "      <td>61.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Polarity  Subjectivity  Combined_Sentiment  Decay_Polarity  \\\n",
       "Date                                                                     \n",
       "2019-03-28 -0.019507      0.339305           -0.026125       -0.133724   \n",
       "2019-03-29  0.208462      0.309487            0.272978        0.100826   \n",
       "2019-04-01 -0.071296      0.215741           -0.086678       -0.226358   \n",
       "\n",
       "            Decay_Subjectivity  Decay_Combined_Sentiment    Topic1    Topic2  \\\n",
       "Date                                                                           \n",
       "2019-03-28            0.908372                 -0.182488  0.129218  0.175803   \n",
       "2019-03-29            0.965868                  0.128440  0.114900  0.199383   \n",
       "2019-04-01            1.236487                 -0.584150  0.094111  0.115631   \n",
       "\n",
       "              Topic3    Topic4  ...  Decay_Geopolitical_Threats  \\\n",
       "Date                            ...                               \n",
       "2019-03-28  0.219396  0.109485  ...                    0.797849   \n",
       "2019-03-29  0.299585  0.078013  ...                    0.825752   \n",
       "2019-04-01  0.175034  0.093650  ...                    0.924013   \n",
       "\n",
       "            Decay_Nuclear_Threats  Decay_War_Threats  Decay_Terrorist_Threats  \\\n",
       "Date                                                                            \n",
       "2019-03-28               0.773509           0.586512                 0.664073   \n",
       "2019-03-29               0.785170           0.592541                 0.715096   \n",
       "2019-04-01               0.883445           0.636587                 0.727725   \n",
       "\n",
       "            Decay_War_Acts  Decay_Terrorist_Acts   CLC4   CLC3   CLC2   CLC1  \n",
       "Date                                                                          \n",
       "2019-03-28        0.966407              1.005316  59.76  59.63  59.48  59.30  \n",
       "2019-03-29        0.963197              1.042693  60.49  60.40  60.28  60.14  \n",
       "2019-04-01        1.065803              1.191592  61.83  61.80  61.71  61.59  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xy.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shifted = series_to_supervised(df_Xy.dropna(), past, h)\n",
    "# remove current day features for forecast\n",
    "for each in df_shifted.columns[:-1]:\n",
    "    if \"(t)\" in each:\n",
    "        df_shifted.drop(each, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity(t-10)</th>\n",
       "      <th>Subjectivity(t-10)</th>\n",
       "      <th>Combined_Sentiment(t-10)</th>\n",
       "      <th>Decay_Polarity(t-10)</th>\n",
       "      <th>Decay_Subjectivity(t-10)</th>\n",
       "      <th>Decay_Combined_Sentiment(t-10)</th>\n",
       "      <th>Topic1(t-10)</th>\n",
       "      <th>Topic2(t-10)</th>\n",
       "      <th>Topic3(t-10)</th>\n",
       "      <th>Topic4(t-10)</th>\n",
       "      <th>...</th>\n",
       "      <th>Decay_Nuclear_Threats(t-1)</th>\n",
       "      <th>Decay_War_Threats(t-1)</th>\n",
       "      <th>Decay_Terrorist_Threats(t-1)</th>\n",
       "      <th>Decay_War_Acts(t-1)</th>\n",
       "      <th>Decay_Terrorist_Acts(t-1)</th>\n",
       "      <th>CLC4(t-1)</th>\n",
       "      <th>CLC3(t-1)</th>\n",
       "      <th>CLC2(t-1)</th>\n",
       "      <th>CLC1(t-1)</th>\n",
       "      <th>CLC1(t)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-04-15</th>\n",
       "      <td>0.085833</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.115660</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>0.446071</td>\n",
       "      <td>0.084436</td>\n",
       "      <td>0.133230</td>\n",
       "      <td>0.266711</td>\n",
       "      <td>0.196648</td>\n",
       "      <td>0.389213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766220</td>\n",
       "      <td>0.718494</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.926449</td>\n",
       "      <td>0.938652</td>\n",
       "      <td>109.49</td>\n",
       "      <td>109.19</td>\n",
       "      <td>108.70</td>\n",
       "      <td>108.11</td>\n",
       "      <td>109.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-18</th>\n",
       "      <td>0.068889</td>\n",
       "      <td>0.342222</td>\n",
       "      <td>0.092464</td>\n",
       "      <td>0.176389</td>\n",
       "      <td>0.959008</td>\n",
       "      <td>0.246140</td>\n",
       "      <td>0.143312</td>\n",
       "      <td>0.197903</td>\n",
       "      <td>0.314226</td>\n",
       "      <td>0.305502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746939</td>\n",
       "      <td>0.705949</td>\n",
       "      <td>0.588953</td>\n",
       "      <td>0.937933</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>110.96</td>\n",
       "      <td>110.68</td>\n",
       "      <td>110.22</td>\n",
       "      <td>109.66</td>\n",
       "      <td>107.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-19</th>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.299536</td>\n",
       "      <td>0.031424</td>\n",
       "      <td>0.155625</td>\n",
       "      <td>1.015314</td>\n",
       "      <td>0.214869</td>\n",
       "      <td>0.177093</td>\n",
       "      <td>0.206282</td>\n",
       "      <td>0.197064</td>\n",
       "      <td>0.390251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678688</td>\n",
       "      <td>0.567106</td>\n",
       "      <td>0.593173</td>\n",
       "      <td>0.847213</td>\n",
       "      <td>0.898380</td>\n",
       "      <td>108.42</td>\n",
       "      <td>108.15</td>\n",
       "      <td>107.69</td>\n",
       "      <td>107.12</td>\n",
       "      <td>108.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Polarity(t-10)  Subjectivity(t-10)  Combined_Sentiment(t-10)  \\\n",
       "Date                                                                       \n",
       "2011-04-15        0.085833            0.347500                  0.115660   \n",
       "2011-04-18        0.068889            0.342222                  0.092464   \n",
       "2011-04-19        0.024181            0.299536                  0.031424   \n",
       "\n",
       "            Decay_Polarity(t-10)  Decay_Subjectivity(t-10)  \\\n",
       "Date                                                         \n",
       "2011-04-15              0.060119                  0.446071   \n",
       "2011-04-18              0.176389                  0.959008   \n",
       "2011-04-19              0.155625                  1.015314   \n",
       "\n",
       "            Decay_Combined_Sentiment(t-10)  Topic1(t-10)  Topic2(t-10)  \\\n",
       "Date                                                                     \n",
       "2011-04-15                        0.084436      0.133230      0.266711   \n",
       "2011-04-18                        0.246140      0.143312      0.197903   \n",
       "2011-04-19                        0.214869      0.177093      0.206282   \n",
       "\n",
       "            Topic3(t-10)  Topic4(t-10)  ...  Decay_Nuclear_Threats(t-1)  \\\n",
       "Date                                    ...                               \n",
       "2011-04-15      0.196648      0.389213  ...                    0.766220   \n",
       "2011-04-18      0.314226      0.305502  ...                    0.746939   \n",
       "2011-04-19      0.197064      0.390251  ...                    0.678688   \n",
       "\n",
       "            Decay_War_Threats(t-1)  Decay_Terrorist_Threats(t-1)  \\\n",
       "Date                                                               \n",
       "2011-04-15                0.718494                      0.608394   \n",
       "2011-04-18                0.705949                      0.588953   \n",
       "2011-04-19                0.567106                      0.593173   \n",
       "\n",
       "            Decay_War_Acts(t-1)  Decay_Terrorist_Acts(t-1)  CLC4(t-1)  \\\n",
       "Date                                                                    \n",
       "2011-04-15             0.926449                   0.938652     109.49   \n",
       "2011-04-18             0.937933                   0.900554     110.96   \n",
       "2011-04-19             0.847213                   0.898380     108.42   \n",
       "\n",
       "            CLC3(t-1)  CLC2(t-1)  CLC1(t-1)  CLC1(t)  \n",
       "Date                                                  \n",
       "2011-04-15     109.19     108.70     108.11   109.66  \n",
       "2011-04-18     110.68     110.22     109.66   107.12  \n",
       "2011-04-19     108.15     107.69     107.12   108.15  \n",
       "\n",
       "[3 rows x 321 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shifted.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time feature without shift\n",
    "df_shifted = pd.concat([df_dt, df_shifted], axis=1).dropna()\n",
    "raw_X = df_shifted.to_numpy()[:, :-1]\n",
    "y = df_shifted.to_numpy()[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_in_week</th>\n",
       "      <th>Polarity(t-10)</th>\n",
       "      <th>Subjectivity(t-10)</th>\n",
       "      <th>Combined_Sentiment(t-10)</th>\n",
       "      <th>Decay_Polarity(t-10)</th>\n",
       "      <th>Decay_Subjectivity(t-10)</th>\n",
       "      <th>Decay_Combined_Sentiment(t-10)</th>\n",
       "      <th>Topic1(t-10)</th>\n",
       "      <th>...</th>\n",
       "      <th>Decay_Nuclear_Threats(t-1)</th>\n",
       "      <th>Decay_War_Threats(t-1)</th>\n",
       "      <th>Decay_Terrorist_Threats(t-1)</th>\n",
       "      <th>Decay_War_Acts(t-1)</th>\n",
       "      <th>Decay_Terrorist_Acts(t-1)</th>\n",
       "      <th>CLC4(t-1)</th>\n",
       "      <th>CLC3(t-1)</th>\n",
       "      <th>CLC2(t-1)</th>\n",
       "      <th>CLC1(t-1)</th>\n",
       "      <th>CLC1(t)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.031101</td>\n",
       "      <td>0.151511</td>\n",
       "      <td>-0.035813</td>\n",
       "      <td>-0.183627</td>\n",
       "      <td>0.581536</td>\n",
       "      <td>-0.225676</td>\n",
       "      <td>0.079573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757268</td>\n",
       "      <td>0.566983</td>\n",
       "      <td>0.661205</td>\n",
       "      <td>0.947744</td>\n",
       "      <td>0.972369</td>\n",
       "      <td>59.91</td>\n",
       "      <td>59.77</td>\n",
       "      <td>59.61</td>\n",
       "      <td>59.41</td>\n",
       "      <td>59.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.161859</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>-0.096372</td>\n",
       "      <td>0.524129</td>\n",
       "      <td>-0.115866</td>\n",
       "      <td>0.156320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773509</td>\n",
       "      <td>0.586512</td>\n",
       "      <td>0.664073</td>\n",
       "      <td>0.966407</td>\n",
       "      <td>1.005316</td>\n",
       "      <td>59.76</td>\n",
       "      <td>59.63</td>\n",
       "      <td>59.48</td>\n",
       "      <td>59.30</td>\n",
       "      <td>60.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039366</td>\n",
       "      <td>0.253747</td>\n",
       "      <td>0.049355</td>\n",
       "      <td>-0.003102</td>\n",
       "      <td>0.454577</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.103892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785170</td>\n",
       "      <td>0.592541</td>\n",
       "      <td>0.715096</td>\n",
       "      <td>0.963197</td>\n",
       "      <td>1.042693</td>\n",
       "      <td>60.49</td>\n",
       "      <td>60.40</td>\n",
       "      <td>60.28</td>\n",
       "      <td>60.14</td>\n",
       "      <td>61.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            month  day  day_in_week  Polarity(t-10)  Subjectivity(t-10)  \\\n",
       "Date                                                                      \n",
       "2019-03-28      3   28            3       -0.031101            0.151511   \n",
       "2019-03-29      3   29            4        0.011859            0.161859   \n",
       "2019-04-01      4    1            0        0.039366            0.253747   \n",
       "\n",
       "            Combined_Sentiment(t-10)  Decay_Polarity(t-10)  \\\n",
       "Date                                                         \n",
       "2019-03-28                 -0.035813             -0.183627   \n",
       "2019-03-29                  0.013778             -0.096372   \n",
       "2019-04-01                  0.049355             -0.003102   \n",
       "\n",
       "            Decay_Subjectivity(t-10)  Decay_Combined_Sentiment(t-10)  \\\n",
       "Date                                                                   \n",
       "2019-03-28                  0.581536                       -0.225676   \n",
       "2019-03-29                  0.524129                       -0.115866   \n",
       "2019-04-01                  0.454577                        0.001292   \n",
       "\n",
       "            Topic1(t-10)  ...  Decay_Nuclear_Threats(t-1)  \\\n",
       "Date                      ...                               \n",
       "2019-03-28      0.079573  ...                    0.757268   \n",
       "2019-03-29      0.156320  ...                    0.773509   \n",
       "2019-04-01      0.103892  ...                    0.785170   \n",
       "\n",
       "            Decay_War_Threats(t-1)  Decay_Terrorist_Threats(t-1)  \\\n",
       "Date                                                               \n",
       "2019-03-28                0.566983                      0.661205   \n",
       "2019-03-29                0.586512                      0.664073   \n",
       "2019-04-01                0.592541                      0.715096   \n",
       "\n",
       "            Decay_War_Acts(t-1)  Decay_Terrorist_Acts(t-1)  CLC4(t-1)  \\\n",
       "Date                                                                    \n",
       "2019-03-28             0.947744                   0.972369      59.91   \n",
       "2019-03-29             0.966407                   1.005316      59.76   \n",
       "2019-04-01             0.963197                   1.042693      60.49   \n",
       "\n",
       "            CLC3(t-1)  CLC2(t-1)  CLC1(t-1)  CLC1(t)  \n",
       "Date                                                  \n",
       "2019-03-28      59.77      59.61      59.41    59.30  \n",
       "2019-03-29      59.63      59.48      59.30    60.14  \n",
       "2019-04-01      60.40      60.28      60.14    61.59  \n",
       "\n",
       "[3 rows x 324 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shifted.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression,RFE,RFECV,SelectFromModel,SequentialFeatureSelector,chi2,SelectKBest,f_regression,VarianceThreshold,r_regression\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "def get_TS_cv(k=5, test_size=None):\n",
    "    \"\"\"\n",
    "    ML models do not need to care about forecast horizon when splitting training and test set. Forecast horizon should be handled by feature preparation ([X_t-1,X_t-2...]). Actually repeated K-fold can also be used, but stick to TS split to align with TS_evaluate().\n",
    "    \"\"\"\n",
    "    return TimeSeriesSplit(\n",
    "        n_splits=k,\n",
    "        gap=0,\n",
    "        test_size=test_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LinearRegression()\n",
    "feature_selector = SequentialFeatureSelector(estimator,n_features_to_select=20,cv=get_TS_cv(),n_jobs=-1)\n",
    "feature_selector = feature_selector.fit(raw_X, y.ravel())\n",
    "X = raw_X[:, feature_selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Decay_Topic3(t-10)', 'Decay_Topic4(t-10)', 'CLC1(t-10)',\n",
       "       'Subjectivity(t-9)', 'Topic2(t-9)', 'Decay_Topic1(t-9)', 'CLC3(t-8)',\n",
       "       'Decay_Combined_Sentiment(t-6)', 'Geopolitical_Threats(t-6)',\n",
       "       'Topic4(t-5)', 'Polarity(t-3)', 'War_Acts(t-3)', 'Topic5(t-2)',\n",
       "       'Decay_Topic4(t-2)', 'Terrorist_Threats(t-2)', 'CLC1(t-2)',\n",
       "       'Terrorist_Threats(t-1)', 'Terrorist_Acts(t-1)',\n",
       "       'Decay_War_Threats(t-1)', 'CLC1(t-1)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shifted.columns[:-1][feature_selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratio=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(X=X, y=y, idx=df_shifted.index,train_ratio=training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "\n",
    "linear_model=LinearRegression()\n",
    "linear_model.fit(dataset.train_X, dataset.train_y)\n",
    "pred_linear_y = linear_model.predict(dataset.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>59.613654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>59.476711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>60.237310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "Date                 \n",
       "2019-03-28  59.613654\n",
       "2019-03-29  59.476711\n",
       "2019-04-01  60.237310"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_linear=pd.DataFrame(pred_linear_y ,index=dataset.idx)\n",
    "df_linear.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def stationary_test(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        test_result = adfuller(data[:, i])\n",
    "        if test_result[1] > 0.05:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def stl_decompose(data):\n",
    "    stl = STL(data, 10)\n",
    "    res = stl.fit()\n",
    "    non_linear_trend = res.trend.reshape(-1, 1)\n",
    "    non_linear_season = res.seasonal.reshape(-1, 1)\n",
    "    non_linear_residual = res.resid.reshape(-1, 1)\n",
    "    non_linear_y = np.concatenate(\n",
    "        [non_linear_season, non_linear_trend, non_linear_residual], axis=1)\n",
    "    return non_linear_y\n",
    "\n",
    "def differencing(data):\n",
    "    diffed_data = np.diff(data, axis=0)\n",
    "    return diffed_data\n",
    "\n",
    "def ensure_stationary(data):\n",
    "    diff_order = 0\n",
    "    feature = data.feature\n",
    "    label = data.label\n",
    "    while not stationary_test(feature):\n",
    "        feature = differencing(feature)\n",
    "        diff_order += 1\n",
    "    label = label[diff_order:]\n",
    "    return feature, label, diff_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2003, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_y = dataset.label - pred_linear_y.reshape(\n",
    "    dataset.label.shape)\n",
    "non_linear_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_data = Dataset(X=X,\n",
    "                            y=non_linear_y,\n",
    "                            idx=df_shifted.index,\n",
    "                            scaling=True,train_ratio=training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_data.label = stl_decompose(nonlinear_data.label)\n",
    "nonlinear_data.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NonLinear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import (GRU, LSTM, Bidirectional, Conv1D, Conv2D, Conv3D,\n",
    "                          ConvLSTM2D, Dense, Dropout, Flatten, Input,\n",
    "                          MaxPooling2D, Reshape, SimpleRNN)\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature=nonlinear_data.feature.shape[-1]\n",
    "num_label=nonlinear_data.label.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 14:02:26.449630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.474707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.474875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.475600: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-07 14:02:26.476181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.476326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.476453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.772341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.772520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.772651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 14:02:26.772762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7880 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 20, 1)             0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 20, 100)          15900     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 20, 100)          45600     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 20, 100)          45600     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 20, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 20, 100)          45600     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 100)              45600     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,603\n",
      "Trainable params: 198,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# set random seed \n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "input_layer = Input(shape=(num_feature,))\n",
    "bimodel_draft = Reshape((num_feature, 1))(input_layer)\n",
    "bimodel_draft =  Bidirectional( GRU(50, dropout=0.2, return_sequences=True)) (bimodel_draft)\n",
    "bimodel_draft = Dropout(0.4)(bimodel_draft)\n",
    "bimodel_draft =  Bidirectional( GRU(50, dropout=0.2, return_sequences=True)) (bimodel_draft)\n",
    "bimodel_draft = Dropout(0.4)(bimodel_draft)\n",
    "bimodel_draft =  Bidirectional( GRU(50, dropout=0.2, return_sequences=True)) (bimodel_draft)\n",
    "bimodel_draft = Dropout(0.4)(bimodel_draft)\n",
    "bimodel_draft =  Bidirectional( GRU(50, dropout=0.2, return_sequences=True)) (bimodel_draft)\n",
    "bimodel_draft = Dropout(0.4)(bimodel_draft)\n",
    "bimodel_draft =  Bidirectional( GRU(50, dropout=0.2, return_sequences=False)) (bimodel_draft)\n",
    "bimodel_draft = Dropout(0.4)(bimodel_draft)\n",
    "bimodel_draft = Flatten()(bimodel_draft)\n",
    "bimodel_draft = Dense(num_label)(bimodel_draft)\n",
    "bimodel= Model(inputs=input_layer, outputs=bimodel_draft)\n",
    "bimodel.compile(loss='log_cosh', optimizer=Adam(0.0002))\n",
    "bimodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 14:02:35.856147: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8204\n",
      "2022-04-07 14:02:36.483858: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 11s 54ms/step - loss: 47.8109 - val_loss: 45.4049\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 43.5357 - val_loss: 41.1189\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 40.8305 - val_loss: 39.5506\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 39.4415 - val_loss: 38.5147\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 38.4491 - val_loss: 37.5974\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 37.5193 - val_loss: 36.7317\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 36.6712 - val_loss: 35.8996\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 35.9021 - val_loss: 35.0933\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 35.1010 - val_loss: 34.2998\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 34.3004 - val_loss: 33.5207\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 33.5505 - val_loss: 32.7533\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 32.7266 - val_loss: 31.9954\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 32.0059 - val_loss: 31.2447\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 31.3740 - val_loss: 30.5063\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 30.5120 - val_loss: 29.7694\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 29.8015 - val_loss: 29.0409\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 29.0500 - val_loss: 28.3208\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 28.3699 - val_loss: 27.6058\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 27.6573 - val_loss: 26.8984\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 27.0745 - val_loss: 26.1980\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 26.3138 - val_loss: 25.5109\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 25.6703 - val_loss: 24.8313\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 25.0667 - val_loss: 24.1613\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 24.3651 - val_loss: 23.5002\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 23.7839 - val_loss: 22.8452\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 23.0176 - val_loss: 22.1972\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 22.5204 - val_loss: 21.5612\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 21.8076 - val_loss: 20.9337\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 21.2957 - val_loss: 20.3114\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 20.6476 - val_loss: 19.7021\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 20.0704 - val_loss: 19.0985\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 19.4866 - val_loss: 18.5034\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 19.0639 - val_loss: 17.9239\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 18.4948 - val_loss: 17.3532\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 17.8231 - val_loss: 16.7906\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 17.3221 - val_loss: 16.2368\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 16.8881 - val_loss: 15.6893\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 16.2903 - val_loss: 15.1570\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 15.9003 - val_loss: 14.6343\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 15.4389 - val_loss: 14.1205\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 14.9144 - val_loss: 13.6168\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 14.4162 - val_loss: 13.1251\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 14.1371 - val_loss: 12.6557\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 13.7782 - val_loss: 12.1972\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 13.1796 - val_loss: 11.7546\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 12.8595 - val_loss: 11.3204\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 12.5482 - val_loss: 10.9061\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 12.1859 - val_loss: 10.5052\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 11.8295 - val_loss: 10.1184\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 11.5518 - val_loss: 9.7473\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 11.2576 - val_loss: 9.3930\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 11.0588 - val_loss: 9.0498\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 10.6939 - val_loss: 8.7329\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 10.4670 - val_loss: 8.4276\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 10.3195 - val_loss: 8.1345\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 10.0070 - val_loss: 7.8660\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 9.9419 - val_loss: 7.6175\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 9.6560 - val_loss: 7.3837\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 9.3974 - val_loss: 7.1751\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 9.3398 - val_loss: 6.9844\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 9.2847 - val_loss: 6.7969\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 9.1641 - val_loss: 6.6315\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 9.1027 - val_loss: 6.4778\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.9541 - val_loss: 6.3373\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.7708 - val_loss: 6.2141\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.7782 - val_loss: 6.0986\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 8.7278 - val_loss: 5.9935\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.6328 - val_loss: 5.9054\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.5151 - val_loss: 5.8250\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.5367 - val_loss: 5.7472\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.4248 - val_loss: 5.6871\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.5013 - val_loss: 5.6265\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.4868 - val_loss: 5.5747\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.4471 - val_loss: 5.5265\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.4666 - val_loss: 5.4855\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2834 - val_loss: 5.4531\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.3425 - val_loss: 5.4206\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.3340 - val_loss: 5.3898\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2581 - val_loss: 5.3674\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2393 - val_loss: 5.3440\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.4039 - val_loss: 5.3249\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1788 - val_loss: 5.3108\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2463 - val_loss: 5.2991\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.3109 - val_loss: 5.2886\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2456 - val_loss: 5.2811\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2371 - val_loss: 5.2710\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2486 - val_loss: 5.2642\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.3074 - val_loss: 5.2583\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1602 - val_loss: 5.2515\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2872 - val_loss: 5.2466\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2541 - val_loss: 5.2440\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2048 - val_loss: 5.2383\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1767 - val_loss: 5.2368\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2549 - val_loss: 5.2345\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2593 - val_loss: 5.2312\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2763 - val_loss: 5.2288\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1291 - val_loss: 5.2287\n",
      "Epoch 98/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1945 - val_loss: 5.2278\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 8.1742 - val_loss: 5.2274\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2875 - val_loss: 5.2271\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2122 - val_loss: 5.2276\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1798 - val_loss: 5.2274\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 8.1567 - val_loss: 5.2280\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2174 - val_loss: 5.2271\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1049 - val_loss: 5.2267\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2849 - val_loss: 5.2254\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1822 - val_loss: 5.2259\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1704 - val_loss: 5.2265\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2514 - val_loss: 5.2277\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1050 - val_loss: 5.2256\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2812 - val_loss: 5.2255\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2356 - val_loss: 5.2226\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2059 - val_loss: 5.2208\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.3111 - val_loss: 5.2228\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1640 - val_loss: 5.2235\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1768 - val_loss: 5.2230\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2255 - val_loss: 5.2225\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2569 - val_loss: 5.2212\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1703 - val_loss: 5.2218\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1968 - val_loss: 5.2235\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.3049 - val_loss: 5.2226\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1653 - val_loss: 5.2253\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.3091 - val_loss: 5.2269\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2328 - val_loss: 5.2278\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1477 - val_loss: 5.2263\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1378 - val_loss: 5.2275\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2861 - val_loss: 5.2276\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1242 - val_loss: 5.2277\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1165 - val_loss: 5.2251\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2400 - val_loss: 5.2263\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1671 - val_loss: 5.2247\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2632 - val_loss: 5.2234\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2581 - val_loss: 5.2226\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1257 - val_loss: 5.2208\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2710 - val_loss: 5.2197\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2431 - val_loss: 5.2194\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2825 - val_loss: 5.2205\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2512 - val_loss: 5.2213\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.0884 - val_loss: 5.2236\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2595 - val_loss: 5.2239\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2318 - val_loss: 5.2233\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1820 - val_loss: 5.2248\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2659 - val_loss: 5.2251\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2111 - val_loss: 5.2240\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.0904 - val_loss: 5.2233\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1535 - val_loss: 5.2243\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1633 - val_loss: 5.2254\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.3073 - val_loss: 5.2231\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2253 - val_loss: 5.2250\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1924 - val_loss: 5.2235\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.3247 - val_loss: 5.2248\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1579 - val_loss: 5.2248\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2314 - val_loss: 5.2242\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2854 - val_loss: 5.2248\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2334 - val_loss: 5.2276\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.0768 - val_loss: 5.2256\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2922 - val_loss: 5.2215\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2515 - val_loss: 5.2203\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1140 - val_loss: 5.2209\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2989 - val_loss: 5.2220\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2631 - val_loss: 5.2217\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1567 - val_loss: 5.2213\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2050 - val_loss: 5.2222\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.3590 - val_loss: 5.2232\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1759 - val_loss: 5.2224\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2289 - val_loss: 5.2206\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2153 - val_loss: 5.2196\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.0801 - val_loss: 5.2203\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2146 - val_loss: 5.2207\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1916 - val_loss: 5.2209\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1594 - val_loss: 5.2213\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1678 - val_loss: 5.2211\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2020 - val_loss: 5.2240\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2506 - val_loss: 5.2243\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1119 - val_loss: 5.2250\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1894 - val_loss: 5.2258\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2292 - val_loss: 5.2217\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1753 - val_loss: 5.2217\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2154 - val_loss: 5.2203\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2281 - val_loss: 5.2262\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2182 - val_loss: 5.2266\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2105 - val_loss: 5.2221\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2990 - val_loss: 5.2203\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2181 - val_loss: 5.2205\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1836 - val_loss: 5.2212\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1550 - val_loss: 5.2207\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1188 - val_loss: 5.2226\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2747 - val_loss: 5.2227\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1823 - val_loss: 5.2224\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1312 - val_loss: 5.2180\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.2466 - val_loss: 5.2173\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1732 - val_loss: 5.2186\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.0851 - val_loss: 5.2204\n",
      "Epoch 194/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1743 - val_loss: 5.2189\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1731 - val_loss: 5.2201\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1855 - val_loss: 5.2178\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1700 - val_loss: 5.2183\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.1911 - val_loss: 5.2186\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2919 - val_loss: 5.2219\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.2595 - val_loss: 5.2204\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = bimodel.fit(nonlinear_data.train_X,\n",
    "                            nonlinear_data.train_y,\n",
    "                            validation_data=(nonlinear_data.test_X,\n",
    "                            nonlinear_data.test_y),\n",
    "                            epochs=200,\n",
    "                            batch_size=40,\n",
    "                            verbose=1,\n",
    "                            shuffle=False)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_error = pd.DataFrame(\n",
    "        columns=['seed', 'mae', 'rmse', 'mape', 'descriptions'])\n",
    "pred_nonliner_test_y=bimodel.predict(nonlinear_data.test_X)\n",
    "inverted_pred_nonlinear_test_y=nonlinear_data.label_scaler.inverse_transform(pred_nonliner_test_y)\n",
    "pred_combined_nonlinear_test_y = pd.DataFrame(inverted_pred_nonlinear_test_y).apply(lambda x: x.sum(), axis=1).to_numpy().reshape(-1, 1)\n",
    "df_pred_linear_y=df_linear[df_linear.index.isin(nonlinear_data.test_idx)]\n",
    "pred_linear_test_y = df_pred_linear_y.to_numpy()\n",
    "\n",
    "pred_final = pred_combined_nonlinear_test_y.ravel()   + pred_linear_test_y.ravel() \n",
    "test_real_y=y[-pred_final.shape[0]:]\n",
    "result=evaluate_series(test_real_y,pred_final,1)\n",
    "result['descriptions']=\"BiGRU\"\n",
    "result['seed']=seed_value\n",
    "df_forecast_error=df_forecast_error.append(result,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mape</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>h</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>[0.7408232246530382]</td>\n",
       "      <td>[0.9969050881869899]</td>\n",
       "      <td>[0.01318231451009793]</td>\n",
       "      <td>BiGRU</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.986208201430629]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  seed                   mae                  rmse                   mape  \\\n",
       "0   42  [0.7408232246530382]  [0.9969050881869899]  [0.01318231451009793]   \n",
       "\n",
       "  descriptions    h                   r2  \n",
       "0        BiGRU  1.0  [0.986208201430629]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_forecast_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_scaler=MinMaxScaler()# [0,1]\n",
    "new_label_scaler.fit(dataset.train_y)\n",
    "scaled_test_y=new_label_scaler.transform(dataset.test_y)\n",
    "scaled_pred_y=new_label_scaler.transform(pred_final.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [0.7408232246530382],\n",
       " 'rmse': [0.9969050881869899],\n",
       " 'mape': [0.01318231451009793],\n",
       " 'r2': [0.986208201430629],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_series(test_real_y,pred_final,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1,\n",
       " 'mae': [0.00844531719850705],\n",
       " 'rmse': [0.011364627088314976],\n",
       " 'mape': [0.025645882682705165],\n",
       " 'r2': [0.986208201430629],\n",
       " 'descriptions': ''}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_series(scaled_test_y,scaled_pred_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f3e05a59671f1eb5b3f5f0e003aaa5a39f5d3316373e39c3606e56079185283"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('OPP-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
